---
layout: blog
others: true
istop: true
title: "action recognition"
background-image: http://7568.github.io/images/2022-04-09-synthetic-humans-for-action-recognition-from-unseen-viewpoints.md/img.png
date:  2022-04-09
category: 视频动作识别
tags:
- video
- action recognition
- Synthetic data
---

[figure_1]:https://7568.github.io/images/2022-03-23-my-deep-compression-note/figure_1.png



[Synthetic Humans for Action Recognition from Unseen Viewpoints](https://arxiv.org/abs/1912.04070)

#摘要：
本文想通过利用合成的数据来提升视频人类动作的识别率。基于此想法，作者设计了一套合成数据的生成方法，生成了一个新的数据集 SURREACT，然后通过在
该数据集上进行训练，再分别在NTU RGB+D 和 UESTC 数据集上做微调，最后取得了目前动作识别最好准确率。NTU RGB+D 和 UESTC 数据集都是室内视频数据
集， 为了检验作者的方法，他们又在野外的视频数据集 Kinetics 上做了one-shot测试，即每一类只选择一个样本进行训练，然后取得了很好的效果。

Introduction：首先作者介绍通常大家都使用卷积神经网络CNN来对视频数据集UFC101进行动作识别训练和预测，但是作者提出卷积神经网络非常依赖于数据集的
大小，通常需要很大的数据集才能有好的效果，然后鉴于此就有很多工作 提出使用合成数据来增加数据量，例如使用光流估计，分割，身体和手势估计。在本文中研
究的是利用合成的数据来进行动作识别。

作者通过观察，发现对于现在流行的所有网络，对于同一个动作，如果训练和测试都使用同一个视角，能得到很好的结果，但是如果训练和测试使用不同的视角，这些
网络的性能就会大幅度减少。例如作者使用一个3D的卷积网络来对 NTU RGB+D 数据集进行训练， 当训练和测试都是正面视角的时候，最终能得到80%多的准确率，
但是如果我们的测试换成90度视角，这个时候准确率就只有40%了。这个结果激发了我们来从一个巧妙的视角研究视频动作识别。

在之前有一些对人体姿势预测进行了研究，并且取得了很好的成绩，通常他们的目的是动作捕获（MoCap），所以这些研究不适合于行为的预测，因为它们没有数据标记。

所以本文就提出了一个新的简单有效的方法来合成带有行为标签的数据。首先我们使用 HMMR 和 VIBE 等方法来动态的从单视角的 RGB 图像中得到 3D 的人，
这些 3D 的人是由一串 SMPL 的人体姿势的参数组成。
然后我们通过 SMPL 合成不同视角的带标签的训练数据。最后我们使用一个 3D 网络来对我们的数据进行训练，得到了非常好的效果。我们的效果主要有两个方面，
一是对于没见过的视角的行为识别，二是对于 one-shot 数据的训练识别。

----------------

#相关工作：
人类行为识别是一个成熟的研究领域，在 Kong et al 的一篇对其研究的综述论文中有详细的介绍。本文中我们只聚焦于相关工作中的合成数据，
交叉尺度动作识别，和简要的3D人类形态估计。

**合成人类数据**：在过去有很多研究都使用了合成数据来进行数据增强，但是他们都还没有将合成数据应用到人类行为识别上来，之前的一个重要的合成数据集 SURREAL ，
就是视频中人类 shape ， pose ， 和 motion 的数据集，而不是行为的识别。过去也有一些用姿势合成数据，和点轨迹合成数据来进行不同视角的行为识别，
但是利用合成的RGB图像来训练，来进行动作识别的研究还是一个比较新的领域，De Souza et al是最早的一批人来研究该领域。但是他们方法自动化程度不高。
有一些地方 需要手动处理，而且他们的可扩展性不是很好。

之前有一篇论文（*43*）他们与我们的工作最相近，他们是利用合成的数据来学习人类姿势模型，然后通过该模型得到行为视频的特征，最后将该特征进行行为预测的分类。
我们与该论文的最大区别就是我们合成的数据是直接用来行为识别，也就是说我们生成的数据是带有行为标签的。

**交叉视角的行为识别**：NTU RGB+D 是第一个大规模多视角的行为识别数据集。从而使得我们可以开始在该领域使用深度神经网络。最近又有人提出了一个新的数据集 
UESTC ，该数据集在实验室中 收集了大量动作的360全视角视频。在之前多视角行为数据集通常都是带有景深的RGB图像，一般大家的做法是通过RGB-D得到一个3D
的人体框架，然后基于该框架来进行多视角的研究， 也就是说他们这些研究在训练过程中使用的输入图像是 RGB-D 格式的或者是通过 RGB-D 得到的3D人体框架。
而我们提出的方法，训练的时候的输入输入的图像只是 RGB 格式的。

将RGB的特征转换成不同视角不像转换3D人体架构那样简单，之前有很多人对此进行了研究，我们与他们的不同是，我们不使用3D人体架构图，而是使用合成的数据
来进行数据增强，而且我们训练的时候并不假设训练的视频存在多视图。

**人体3d形态估计**：从单张图像中恢复出一个完整的人体体型网格已经有很多的研究，在本文中，我们使用 HMMR 和 VIBE 两种方法分别获得 SMPL 格式的人体参数。


**合成带有行为标签的人类数据**：首先我们通过 HMMR 或者 VIBE ，将一个视频数据中的人体以3D的形式提取出来，然后我们使用该3D数据来合成不同视角的数
据，最后我们使用一个3D网络来训练该合成的数据。

**训练一个3D网络** ：对于3D网络，我们使用3D ResNet-50，我们同时还使用了光流信息，我们使用一个 two-stack hourglass architecture
的网络来训练光流信息。通常一个视频段，里面包含的图像非常多，如果我们使用一个完整的clip，那么我们网络需要处理的数据会非常大，导致我们的网络训练起来
非常的慢。所以我们采用从一个视频clip抽取出16张图像来合成一个新的clip。

----------------


**SURREACT dataset**：通过 NTU RGB+D 和 UESTC 这两个标准的人类行为视频数据集来合成大量的多视角，多背景，多服饰，多身体形态的数据集。
**SURREAL dataset**：从多个人体动作视频数据集中合成的一个新的，多视角的人体动作视频数据集。






 
[Learning from Synthetic Humans](https://arxiv.org/pdf/1701.01370.pdf) 对于图像分割，其中有一个任务是通过输入一张图像，能够分割出人
的同时也要能分割出这个人在该图像中的景深信息。 对于这样一个任务，我们可用的数据集很少，于是本文就提出一个方法来人工合成一个数据集。这篇文章说我们使
用 Blender 从 CMU MoCap 中获得 SMPL 人体 参数信息，包括姿势和外形，然后通过改变人的衣服，环境，周围灯光，相机位置，最终合成大量的图像。

[Learning a Non-linear Knowledge Transfer Model for Cross-View Action Recognition](https://openaccess.thecvf.com/content_cvpr_2015/papers/Rahmani_Learning_a_Non-Linear_2015_CVPR_paper.pdf) 
本文提出一种方法来将视频中不同角度运动转换到一个最常用的经典角度， 从而提升视频中动作识别的准确率。而且这个方法是一种无监督学习的方式。本文的出发
点是希望这样做了之后，当我们在一个有限的数据集上训练一个网络，在测试的时候，如果我们碰到了一个动作，它的拍摄角度 是之前训练数据集里面没有的话，
那么该网络就对这种视频的动作识别准确率很差，但是如果我们有个方法，能够使得所有的测试集在进入到网络之前都全部转换成我们最常见的经典角度，这个时候我
们的网络 就可以很容易的识别该动作了。

(43) [Learning hu-man pose models from synthesized data for robust RGB-D action recognition]()：

(HMMR) [Learning 3D Human Dynamics from Video](https://arxiv.org/pdf/1812.01601.pdf): 