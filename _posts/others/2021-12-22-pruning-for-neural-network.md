---
layout: blog
others: true
istop: true
mathjax: true
title: "网络剪枝分析"
background-image: https://7568.github.io/images/2021-12-22-pruning-for-neural-network/img.png
date:  2021-12-22
category: 其他
tags:
- deep compression
- deep learning
---

[figure_1]:https://7568.github.io/images/2021-12-22-pruning-for-neural-network/figure_1.png

# 简介

[Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/abs/1506.02626) 是 [Song Han](https://songhan.mit.edu/) 在Stanford大学的时候发表的一篇关于网络如何减少参数的论文。

# Abstract

神经网络对计算和存储的要求都很高，使得他们一般很难不是到嵌入式系统中。而且，常规的网络在训练之前他的结构都是固定的，所以导致的结果就是训练的时候不能实时的优化结构。为了处理这一限制，我们提出了一种方法来
大幅度减少神经网络对存储和计算的需求，同时还不会影响网络的精确度。我们的方法使用三步来剪掉网络中多余的连接。首先，我们将网络训练，使得它能学习到哪些连接是重要的。第二步，我们剪掉那些不重要的连接。
最后，我们重新训练我们的网络来微调那些保存下来的连接。在ImageNet数据集上，我们的方法能将AlexNet的参数数量减少49倍，从61 million 减少到 6.7 million，而且还没有精度的损失。同样的实验用在VGG-16上，参数减少了13倍，
从138 million 较少到了 10.3 million，同样没有损失精度。

# Introduction

在计算机视觉，语音识别，自然语言处理中，神经网络已经无处不在。将卷积神经网络应用到计算机视觉中，已经发展了很长时间了。在1998年Lecun等人设计了一个用来识别手写字的网络模型LeNet-5，该模型的参数少于1M，
2012年的时候 Krizhevsky 设计了一个网络模型，赢得了当年的 ImageNet 数据集分类任务的比赛，该模型的参数只有60M，等。

虽然这些大规模的神经网络很强大，但是他们需要相当大的保存，缓存，和计算资源。对于嵌入式的手机而言，这些必须的资源是无法满足的。图1显示了在 45nm CMOS 处理器上的这些基本的算法在
耗能和存储上的消耗。从这些数据中我们可以看到，每一层上的能量的消耗主要在内存的访问上，范围从32 bits的 on-chip SRAM中耗能5pJ到64 bits的off-chip DRAM中耗能640pJ。
大型的网络并不适合在 on-chip 上存储，因此需要更昂贵的 DRAM 来存储。运行连接数有1 billion的一个网络，例如，在20Hz的频率下，只是对于 DRAM 就需要 (20Hz)(1G)(640pJ) = 12.8W 
的能耗，已经超出了普通手机的能量范围。我们对网络进行剪枝的目的就是为了减少运行大规模网络所需要的能量消耗，可以使他们能实时的运行在手机上。通过剪枝后的模型，使得整合了DNN的手机程序依然能够方便的存储和转换到手机上。
![figure_1]

为了达到这个目标，我们提出了一个手动给网络连接剪枝同时还保持原始精确度的方法。在初次阶段的训练之后，我们移除掉网络连接中所有权重小于指定阈值的连接。这个剪枝操作将密集的全连接层转换为稀疏的层。
第一阶段训练的网络拓扑 - 训练哪些连接是重要的，移除掉那些不重要的连接。然后我们重新训练稀疏网络，从而保留下来的网络，能够补偿那些被移走掉的网络。在剪枝和重新训练阶段，可能会重复进行，从而进一步
减少网络的复杂度。在效果方面，该训练过程训练权重以外还训练连接，这与哺乳动物的大脑很像。在儿童的前几个月大脑发育突触，然后就来将很少使用的连接慢慢的剪掉，最终形成了成人的价值观。

# Related Work

神经网络通常都是参数过剩的，对于深度学习模型更是这样。这就导致了运算和存储上的浪费。以下有几种方法来移除参数过剩：Vanhoucke 等人使用 8-bit 的固定点的激活函数来实现。Denton等人开发了一种
线性结构的网络模型，来发现参数的合适的低置估计，并保持与原始模型的误差在1%以内。Gong等人也提出了一种相同精度误差的方法，该方法通过向量量化来压缩深度转换。这些近似和量化的方法
与网络剪枝都不一样，并且他们还可以组合起来使用，从而达到更好的效果。

还有一些方法尝试通过使用全局平均池化操作来代替全连接层从而达到降低网络参数的作用。采用该方法的 Network in Network 和 GoogLenet 网络结构都在多个基准数据集上取得了 state-of-the-art 的效果。
然而在迁移学习中，例如，将 ImageNet 数据集上训练出来的特征，仅仅只是通过微调全连接层，就重新应用到新的任务中，使用该方法就不行了。Szegedy 在他的论文中指出了这个问题，并激励他们在网络的最开始的部分添加一个线性层，使得网络能够进行迁移学习。

