---
layout: blog
others: true
istop: true
mathjax: true
title: "网络优化"
background-image: https://7568.github.io/images/2022-03-16-deep-learning-with-limited-numerical-precision/img.png
date:  2022-03-16
category: 其他
tags:
- fp32 to fp16
- deep learning
---

[figure_1]:https://7568.github.io/images/2021-12-22-pruning-for-neural-network/figure_1.png
[figure_2]:https://7568.github.io/images/2021-12-22-pruning-for-neural-network/figure_2.png

# 简介

[Deep Learning with Limited Numerical Precision](https://arxiv.org/abs/1502.02551) 是 `IBM T. J. Watson Research Center` 发表的一篇关于网络如何提升神经网络训练速度的论文。

# Abstract

大规模的神经网络常常受限于可用的计算资源。我们在神经网络训练的时候，研究它的数据表示和计算在低精度时候的效果。在低精度固定点的计算环境中，我们观察到舍入策略在训练中对于决定网络的
走向起到了关键作用。我们的结果显示深度网络能够在使用随机舍入的时候，仅仅使用16字节的宽度的固定点来进行数字表示，也能很好的进行训练。

# Introduction

在很大程度上，深度学习技术的成功取决于底层硬件平台的快速执行能力，这些有监督的复杂网络使用大量的带标签数据。这些能力能够让我们快速的评估不同网络的结构，
和通过空间搜索来得到模型的参数。所以最近出现的人们感兴趣于的部署在大规模的对于训练神经网络而专门设计的大规模计算架构。例如CPU并行和GPU并行。

于此同时， 有一些研究指出神经网络的结构和学习算法天生对错误具有容忍性，于是使得他们与传统的那些工作不一样，他们需要高的动态范围的计算精度和数据表示。很高兴在之前的研究中
有人指出对于错误率的估计的统计分析中得出，在学习中使用高精度的计算不是必要的。而且，也有研究指出在训练的时候添加噪声能提高训练的准确度。除了部署使用异步版本的随机
梯度下降来减少网络的拥堵之外，当前的 SOTA 的大规模深度神经网络系统都没有充分的重视他们工作中的对错误的容忍性。这些系统通过整合为常规目的设计的硬件，而这些
硬件是为了迎合传统的工作的。这些往往会导致不必要的资源的消耗。

在本文中我们认为是否有可能使用算法层面上多错误的容忍性来释放底层硬件的限制。从而得到一个硬件软件互补的一个系统，从而能够显著的提升模型过的性能和资源利用率。
当我们使用低级的硬件组来执行估计，那些非决定性的计算，和这些由硬件生成的错误暴露到计算栈的算法层面，可能对开发该系统至关重要。而且低级的硬件中的
改变需要介绍到那些服务于程序模型的行为中来，以至于这些好处能够在应用层面被轻易的吸收，而不会明显的降低软件再开发的开销。

为了达到这种交叉层的联合设计，我们第一步要探究的是对神经网络使用低精度的固定点的算法，而在这种使用低精度的固定点的算法的时候，我们使用一个经过特别设计的精度舍弃方法。
使用固定点算法的动机有两个，第一，通常使用固定点单元会比浮点引擎的计算更加节约资源，而且也更快，而且硬件模型也更小，使得可以为其他的硬件腾出更多的地方。
第二，低精度的数据表示能减少存储空间，从而使得对于给定的内存，能够处理更大的模型。总的来说能大大提升数据层面的并行。

我们研究的关键就是神经网络能够使用低精度的固定点算法来进行训练，其中一个前提是进行固定点操作的时候使用随机精度舍去算法。我们将该方法在多个数据集上进行测试，都取得了不错的效果。

# Related Work

在实现神经网络的硬件中，数据表示和计算单元的精度在设计选择中起到了决定性作用。不意外的是，一个好的文章会聚焦于量化这种选择对网络性能的影响，但是现在大量的研究都聚焦于
网络前向环节的实现，假设网络是通过高精度计算而离线训练而来的。而且还有一些之前的研究聚焦于数据表示。之前有研究表明在大多数情况下，使用8 - 16 bit的对于反向传播是大大够用的。
还有研究标明，在网络的权重更新的时候，使用随机精度舍去会进一步减少基于梯度的技术的学习算法对精确度的要求。虽然之前对于神经网络的限制精度训练的研究取得了不错的效果，
但是我们通常认为神经网络通常的限制在经典的多层感知中的单个隐藏层或者是几个隐藏单元。而现在的SOTA的网络模型都超过了百万的参数，所以我们需要重新来评估计算精度受限制的神经网络的影响。

当前有一篇文章提出一个对于神经网络的硬件加速器，该加速器使用固定点计算单元，但是他们发现必须使用32-bit的固定点表示，在训练神经网络的时候才能收敛。与之相反的是，我们的研究标明，
其实在网络的训练中，在固定点计算中，使用随机精度舍去是可以仅仅使用16-bit的固定点的。据我们了解到，我们第一个提出在使用低精度固定点算法训练神经网络训练的时候使用随机精度舍弃。

# Limited Precision Arithmetic

神经网络的训练，标准的实现是在反向传播的时候使用 32-bit 的浮点型数据来表示真实数据。我们取而代之的是用固定点数来表示：[QI.QF],其中 QI 和 QF 分别表示一个数的整数部分和小数部分。
 一个数的整数部分的长度 (IL) 加上小数部分的长度 (FL) ，就得到了用来表示一个数的位数。IL + FL 表示一个数字的长度WL。在本文中我们使用 <IL, FL>来表示固定点。IL (FL) 分别表示固定点中整数的长度和小数的长度。
我们还用 $$\epsilon$$ 来表示给定的固定点格式的数据的最小正数。因此 <IL, FL> 固定点类型限制的精度为 FL 个 bit。能表达的数据的范围是 $$[-2^{IL - 1} , 2^{IL - 1} -2^{-FL} ]$$ , 
并定义$$\epsilon$$ = $$2^{-FL}$$

## Rounding Modes

我们使用精度舍弃模型将一个高精度的数字用低精度的形式来表达。给定一个数我们要将它转换成 <IL, FL> 格式的固定点类型，其中我们定义 $$\lfloor x \rfloor$$ 为这个数的最大整数，
再加上 $$\epsilon (=2^{-FL})$$ ，得到的结果会小于或者等于x。我们考虑了一下两种精度舍弃的方案：
. 就近舍弃原则
    $$
    Round(x,<IL, FL>) =\begin{cases}  \lfloor x \rfloor     & \quad \text{if } \lfloor x \rfloor \leq x \leq \lfloor x \rfloor + \frac{\epsilon}{2} \\
    \lfloor x \rfloor +\epsilon      & \quad \text{if } \lfloor x \rfloor + \frac{\epsilon}{2} < x \leq \lfloor x \rfloor + \epsilon \end{cases}
    $$
. 随机舍弃原则
    $$
    Round(x,<IL, FL>) =\begin{cases}  \lfloor x \rfloor     & \quad \text{w.p. } 1-\frac{x-\lfloor x \rfloor}{\epsilon} \\
    \lfloor x \rfloor +\epsilon      & \quad \text{w.p. } \frac{x-\lfloor x \rfloor}{\epsilon} \end{cases}
    $$

相比较于就近舍弃原则，随机舍弃原则是一个无偏的方案，而且期望也是0。

所以我们如果想使用<IL, FL> 的方式来表示一个数x，就可以表示成如下：
    $$
    Convert(x,<IL, FL>) = \begin{cases}  -2^{IL-1}     & \quad \text{if } x \leq -2^{IL-1} \\
    2^{IL-1} - 2^{-FL} & \quad \text{if } x \geq 2^{IL-1} - 2^{-FL}  \\
    Round(x,<IL, FL>) \quad \text{otherwise } \end{cases}
    $$

## Multiply and accumulate (MACC) operation





