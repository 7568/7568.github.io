---
layout: blog
others: true
istop: true
mathjax: true
title: "深度压缩论文学习"
background-image: https://7568.github.io/images/2021-12-14-deep-compression/img.png
date:  2021-12-14
category: 其他
tags:
- deep compression
- deep learning
---

[three-stage-compression-pipeline]:https://7568.github.io/images/2021-12-14-deep-compression/three-stage-compression-pipeline.png
[matrix-sparsity-relative-index]:https://7568.github.io/images/2021-12-14-deep-compression/matrix-sparsity-relative-index.png
[weight-sharing]:https://7568.github.io/images/2021-12-14-deep-compression/weight-sharing.png

# 简介

[DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING](https://arxiv.org/abs/1510.00149) 是 Song Han 在 2016 年发表在 cs.cv 上的一篇论文。
该论文主要讲述的是通过一种新方法使神经网络的参数大幅减少，同时网络的性能基本上不变，例如将 AlexNet 的参数从 240MB 减少到6.9M，这是十分惊人的效果。接下来我就通过论文和代码来分析一下论文。

# 摘要

神经网络通常需要很大的计算量和存储量，从而使得它很难在资源受限的嵌入式系统上部署。为了处理这种限制，我们提出了一种深度压缩的方法："deep compression"，该方法分成三个步骤来进行压缩，分别是剪枝、
量化处理、 Huffman 编码。这三个步骤共同作用，使得网络所需要的存储能缩小到的35倍或者49倍，而不影响网络的精度。我们方法的第一步就是通过学习来给网络剪枝，只留下重要的连接层。接下来，我们我们量化分析权重参数，使得网络能有
更多的权重共享。最后我们使用 Huffman 编码 。在经过了前两步之后，我们重新训练网络，来微调剩下的连接层，和量化之后的中心点。剪枝通常能将网络连接的层数减少9到13倍，然后量化分析能将每层连接的表示字节从32变到5。
在 ImageNet 数据集上，我们的方法将 AlexNet 所需要的存储从 240MB 减少到了 6.9MB，而且精度还没有损失。将 VGG-16 所需要的存储从 552MB 减少到了 11.3MB同样也没有损失精度。从而使得压缩之后的网络
能够适配到 SRAM 缓存的单片机上，而不仅仅是片外 DRAM 内存上。我们的方法同样能方便的使复杂的网络能够应用到那些大小和网络宽带受限的手机应用程序上。在一些标准的 CPU, GPU 和手机 GPU上，使用我们的方法压缩之后的网络，每一层都会有
3到4倍的提速，在计算效能上有3到7倍的提升。

压缩的整体结构如下图所示：

![three-stage-compression-pipeline]

# 44

首先我们通过正常的网络训练来学习网络中的连接，然后我们裁剪掉参数权重小的连接层：网路中所有参数权重小于指定的阈值的连接全部被裁剪掉。最后我们重新训练网络来学习到最终的网络权重参数，从而只
保留稀疏的连接层。接下来我们将剪枝后的稀疏结构的参数使用行压缩（CSR）或列压缩（CSC）的方法进行压缩并保存，该操作需要2a + n + 1个数据，其中a表示非零元素的个数，n表示行或者列。

为了进一步进行压缩，我们在对稀疏矩阵进行存储的时候使用相对位置索引的方式进行存储，而不是绝对位置的存储。在编码该相对位置中，我们使用8bits和5bits来分别编码 conv 层和 fc 层。
在我们编码的时候，当索引的相对位置大于我们指定的范围的时候，我们在中间插入0来解决该问题。例如在如下的图中：
![matrix-sparsity-relative-index]
该图表示我们需要存储的数据为一个一维的稀疏向量，向量长度为16，其中只有三个位置，分别在位置1、4、15，当我们需要存储该向量的时候，最简单的方式是定义一个长度为16的区域，在1、4、15，位置为3.4、0.9、1.7，其余
位置全为0，但是这样虽然简单，所需要的存储空间就会比较大。这时我们就可以使用一种相对位置编码的压缩方法，来压缩这个向量，从而保存的时候只需使用较少的存储空间。
具体的方法是我们找每个不为0的元素，再找到他们相对于前一个不为0的元素的相对位置，例如3.4这个数，相对于开始位置0而言他的相对位置为1，0.9这个数相对于3.4这个数而言他的位置为3，依次类推。
在图中，由于我们位置编码使用的是8bits，8bits只能表示0到7，包括7，表示不了大于7的数，在计算1.7这个数相对于0.9这个数的相对位置的时候，本来应该为11，但是超过了8bits所能表达的范围，所以这里作者就通过在相对位置刚好等于8的位置填充0，从而让1.7的相对位置是从填充0的位置开始计算的，于是乎1.7元素的相对位置就是3。
至于为什么用8bits，或者5bits，就是为了存储的时候能更节省存储空间。

# TRAINED QUANTIZATION

网络量化和权重共享通过减少用来表达每一层权重所需要的bits的数量来进一步对剪枝后的网络进行压缩。我们通过使用多个连接共享相同的权重，从而限制住需要保存的有效权重的数量，然后再微调这些共享权重。
下图是一个权重共享的示意图：
![weight-sharing]
假设我们网络中的某一层的权重参数为4x4的矩阵，如图中左上方的矩阵，图中左下方的矩阵为对应的梯度矩阵，我们的量化处理的过程为：首先我们将权重进行聚类，如图所示，将权重参数分成了4类，然后用每一类的均值来代替该类，
于是我们4x4的矩阵就被压缩成了4x1的矩阵。对于梯度矩阵，我们将梯度矩阵中与权重矩阵中位置相同的地方标为一类，然后再将每一类进行相加，得到一个4x1的梯度矩阵，于是我们的梯度矩阵也被压缩成了4x1的大小。
最终我们使用压缩后的（权重矩阵）-（lr*梯度矩阵），得到微调之后的权重参数矩阵。通过该方法我们将AlexNet中的channel为256的卷积核进行权重共享，将它量化到8-bits的格式中，将channel为32的全连接的参数进行权重共享，将它量化到5-bits的格式中，而且没有任何的精度损失。

在上面的聚类中，假如我们给定聚类的类别k，那么我们只需要$$log_2(k)$$个bits来对索引进行编码。通常来说对于有n个连接的网络，每一个连接我们都用b个bits来表示，我们将连接限制到只有k个共享的权重中，从而我们计算压缩率的公式为：

$$
r = \frac{n*b}{n*log_2(k) + k*b}
$$

例如在上面权重共享的图中，我们原始权重的大小为16，共享权重之后，大小为4。在原始权重中我们需要32 bits的大小来保存每个权重中的值，现在我们只有4个权重中的值需要32 bits的大小来保存，然后再用16个2 bits来保存聚类的索引，所以总的
压缩率为：$$\frac{16 * 32}{16*log_2(4) + 4 * 32}$$

# WEIGHT SHARING

当我们的原始网络训练好之后，我们使用K-means的聚类方法来确定网络的每一层参数的共享权重，所以所有的聚类在同一类的网络都共享相同的权重参数。权重不会在网络层之间共享。例如我们将大小为n的原始权重$$W = {w_1, w_2, ..., w_n} $$聚类成k类$$C = {c_1, c_2, ..., c_k}$$，
$$n \gg k$$，这个操作就像是最小化聚类内部的平方和（WCSS：within-cluster sum of squares），公式如下：

$$
\mathop{\arg\min}\limits_{C} \displaystyle\sum_{i=1}^k\sum_{w\in c_i}|w-c_i|^2
$$

2015年的时候 Chen 等人发表的方法[HashNet](https://arxiv.org/abs/1504.04788) ，其中也有关于权重共享的方法，它的具体做法是在网络未经过任何训练之前，通过hash的方法来决定权重的共享。我们的方法与该方法不同，
我们的做法是先让网络进行训练完成，然后再来决定权重的共享，所以我们的共享权重与原始网络中的权重是相近的。

# INITIALIZATION OF SHARED WEIGHTS

K-means中k个中心的初始化选择会影响聚类的质量进而影响网络预测的准确率。我们测试了三种初始化方法，分别是：随机的选择（Forgy方法）, 基于密度的选择（density-based）, 和线性初始化（linear initialization）。
在图4中我们展示了AlexNet中的第三层卷积的原始权重分布的累计分布函数CDF（cumulative distribution function），蓝线表示，概率密度函数PDF（probability density function），红线表示。在经过网络剪枝之后，
网络形成了双峰的分布。在图的底部展示的是三种不同的初始化方法所产生的关于权重的初始化中心点。在图中的例子中，我们使用的是$$k=13$$

随机的选择（Forgy方法）:假设我们要将我们的数据集聚类成k类，那么Forgy方法就是随机的选择k个点作为初始化中心点。如上图中的黄色的点。因为双峰分布有两个峰，所以Forgy方法倾向于将初始点聚焦在双峰的附近。

基于密度的选择（density-based）：首先将权重的CDF相对于y轴进行等分，然后找到等分点的水平线与CDF的交点，最后找到交点的垂直线与x轴的交点，将这些x轴上的点作为初始化点。如图中的蓝色的点所示。
该方法能让产生的初始化点集中在双峰附近，但是比Forgy方法分散一些。

线性初始化（linear initialization）：该方法首先找到权重中的最大值max和最小值min，然后在[min,max]之间进行等分，从而形成初始化中心点。该方法相对于前面的两个方法是最分散的。

我们认为大的权重的值会比小的权重的值更加重要，但是大的权重的值往往很少。因此对于 Forgy 初始化和 density-based 初始化，非常少的中心点会有绝对大的值，从而导致对于那些只有很少的大的值的权重的表示会很差，而Linear初始化就没有这种问题。
在实验部分我们比较了不同的初始化方法在在经过聚类和微调之后的精确度，结果显示使用Linear初始化效果最好。

# FEED-FORWARD AND BACK-PROPAGATION

在1维的K-means聚类中，中心点是权重共享的。于是在前向传播阶段和反向传播阶段查找权重表就有点不直接。每一层连接都保存有权重共享表中的索引，在反向传播阶段，通过计算和使用共享权重的梯度来更新共享权重，
该过程在图3中有展示。

我们使用$$\ell$$来表示损失，第$$i$$列，第$$j$$行的权重记为$$W_{ij}$$，$$W_{ij}$$所属的聚类的中心点的元素记为$$I_{ij}$$，该层中第$$k$$个聚类中心点记为$$C_k$$，
通过使用指示函数<span style='color:#07d015'>$$\unicode{x1D7D9}(.)_a$$</span>
