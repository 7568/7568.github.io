---
layout: blog
others: true
istop: true
mathjax: true
title: "深度压缩论文学习"
background-image: https://7568.github.io/images/2021-12-14-deep-compression/img.png
date:  2021-12-14
category: 其他
tags:
- deep compression
- deep learning
---

[three-stage-compression-pipeline]:https://7568.github.io/images/2021-12-14-deep-compression/three-stage-compression-pipeline.png
[matrix-sparsity-relative-index]:https://7568.github.io/images/2021-12-14-deep-compression/matrix-sparsity-relative-index.png
[weight-sharing]:https://7568.github.io/images/2021-12-14-deep-compression/weight-sharing.png

# 简介

[DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING](https://arxiv.org/abs/1510.00149) 是 Song Han 在 2016 年发表在 cs.cv 上的一篇论文。
该论文主要讲述的是通过一种新方法使神经网络的参数大幅减少，同时网络的性能基本上不变，例如将 AlexNet 的参数从 240MB 减少到6.9M，这是十分惊人的效果。接下来我就通过论文和代码来分析一下论文。

# 摘要

神经网络通常需要很大的计算量和存储量，从而使得它很难在资源受限的嵌入式系统上部署。为了处理这种限制，我们提出了一种深度压缩的方法："deep compression"，该方法分成三个步骤来进行压缩，分别是剪枝、
量化处理、 Huffman 编码。这三个步骤共同作用，使得网络所需要的存储能缩小到的35倍或者49倍，而不影响网络的精度。我们方法的第一步就是通过学习来给网络剪枝，只留下重要的连接层。接下来，我们我们量化分析权重参数，使得网络能有
更多的权重共享。最后我们使用 Huffman 编码 。在经过了前两步之后，我们重新训练网络，来微调剩下的连接层，和量化之后的中心点。剪枝通常能将网络连接的层数减少9到13倍，然后量化分析能将每层连接的表示字节从32变到5。
在 ImageNet 数据集上，我们的方法将 AlexNet 所需要的存储从 240MB 减少到了 6.9MB，而且精度还没有损失。将 VGG-16 所需要的存储从 552MB 减少到了 11.3MB同样也没有损失精度。从而使得压缩之后的网络
能够适配到 SRAM 缓存的单片机上，而不仅仅是片外 DRAM 内存上。我们的方法同样能方便的使复杂的网络能够应用到那些大小和网络宽带受限的手机应用程序上。在一些标准的 CPU, GPU 和手机 GPU上，使用我们的方法压缩之后的网络，每一层都会有
3到4倍的提速，在计算效能上有3到7倍的提升。

压缩的整体结构如下图所示：

![three-stage-compression-pipeline]

# 44

首先我们通过正常的网络训练来学习网络中的连接，然后我们裁剪掉参数权重小的连接层：网路中所有参数权重小于指定的阈值的连接全部被裁剪掉。最后我们重新训练网络来学习到最终的网络权重参数，从而只
保留稀疏的连接层。接下来我们将剪枝后的稀疏结构的参数使用行压缩（CSR）或列压缩（CSC）的方法进行压缩并保存，该操作需要2a + n + 1个数据，其中a表示非零元素的个数，n表示行或者列。

为了进一步进行压缩，我们在对稀疏矩阵进行存储的时候使用相对位置索引的方式进行存储，而不是绝对位置的存储。在编码该相对位置中，我们使用8bits和5bits来分别编码 conv 层和 fc 层。
在我们编码的时候，当索引的相对位置大于我们指定的范围的时候，我们在中间插入0来解决该问题。例如在如下的图中：
![matrix-sparsity-relative-index]
该图表示我们需要存储的数据为一个一维的稀疏向量，向量长度为16，其中只有三个位置，分别在位置1、4、15，当我们需要存储该向量的时候，最简单的方式是定义一个长度为16的区域，在1、4、15，位置为3.4、0.9、1.7，其余
位置全为0，但是这样虽然简单，所需要的存储空间就会比较大。这时我们就可以使用一种相对位置编码的压缩方法，来压缩这个向量，从而保存的时候只需使用较少的存储空间。
具体的方法是我们找每个不为0的元素，再找到他们相对于前一个不为0的元素的相对位置，例如3.4这个数，相对于开始位置0而言他的相对位置为1，0.9这个数相对于3.4这个数而言他的位置为3，依次类推。
在图中，由于我们位置编码使用的是8bits，8bits只能表示0到7，包括7，表示不了大于7的数，在计算1.7这个数相对于0.9这个数的相对位置的时候，本来应该为11，但是超过了8bits所能表达的范围，所以这里作者就通过在相对位置刚好等于8的位置填充0，从而让1.7的相对位置是从填充0的位置开始计算的，于是乎1.7元素的相对位置就是3。
至于为什么用8bits，或者5bits，就是为了存储的时候能更节省存储空间。

# TRAINED QUANTIZATION AND WEIGHT SHARING

网络量化和权重共享通过减少用来表达每一层权重所需要的bits的数量来进一步对剪枝后的网络进行压缩。我们通过使用多个连接共享相同的权重，从而限制住需要保存的有效权重的数量，然后再微调这些共享权重。
下图是一个权重共享的示意图：
![weight-sharing]
假设我们网络中的某一层的权重参数为4x4的矩阵，如图中左上方的矩阵，图中左下方的矩阵为对应的梯度矩阵，我们的量化处理的过程为：首先我们将权重进行聚类，如图所示，将权重参数分成了4类，然后用每一类的均值来代替该类，
于是我们4x4的矩阵就被压缩成了4x1的矩阵。对于梯度矩阵，我们将梯度矩阵中与权重矩阵中位置相同的地方标为一类，然后再将每一类进行相加，得到一个4x1的梯度矩阵，于是我们的梯度矩阵也被压缩成了4x1的大小。
最终我们使用压缩后的（权重矩阵）-（lr*梯度矩阵），得到微调之后的权重参数矩阵。通过该方法我们将AlexNet中的channel为256的卷积核进行权重共享，将它量化到8-bits的格式中，将channel为32的全连接的参数进行权重共享，将它量化到5-bits的格式中，而且没有任何的精度损失。

在上面的聚类中，假如我们给定聚类的类别k，那么我们只需要$$log_2(k)$$个bits来对索引进行编码。通常来说对于有n个连接的网络，每一个连接我们都用b个bits来表示，我们将连接限制到只有k个共享的权重中，从而我们计算压缩率的公式为：

$$
r = \frac{n*b}{n*log_2(k) + k*b}
$$

例如在上面权重共享的图中，我们原始权重的大小为16，共享权重之后，大小为4。在原始权重中我们需要32 bits的大小来保存每个权重中的值，现在我们只有4个权重中的值需要32 bits的大小来保存，然后再用16个2 bits来保存聚类的索引，所以总的
压缩率为：$$\frac{16 * 32}{16*log_2(4) + 4 * 32}$$

# WEIGHT SHARING

# 我f们使f用Kme们使f用K们使f用Kans的聚f类d方法来

# 例如在上面权重共享的图中，我们原始权重的大小 例如在上面权重共享的图中，我们原始权重的大小

# trained quantization and weight sharing

# trainedquantizationandweightsharing
