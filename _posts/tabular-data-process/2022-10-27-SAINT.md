---
layout: blog
tabular-data-process: true
mathjax: true
date:   2022-10-27
background-image: https://7568.github.io/images/2022-10-27-SAINT/img.png
title:  SAINT-Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training
category: tabular data 处理
tags:
- tabular data
- Self-supervised Learning
- Semi-supervised Learning
---

[TabTransformer_architecture]:https://7568.github.io/images/2022-10-27-SAINT/figure_1.png


# 简介

论文地址：[https://arxiv.org/pdf/2106.01342.pdf](https://arxiv.org/pdf/2106.01342.pdf)

代码地址：[https://github.com/somepago/saint](https://github.com/somepago/saint)


使用自监督和半监督来对表格型的数据进行神经网络的预训练

并且在论文TabTransformer上的基础上，设计了一个交叉注意力的网络

SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training

# 论文介绍

##TabTransformer

在介绍SAINT之前首先剪短的介绍一下[TabTransformer](https://arxiv.org/pdf/2012.06678.pdf)

TabTransformer是一个将transformer应用在tabular数据上的方法，TabTransformer总体结构如下：

![TabTransformer_architecture]

大概的步骤就是，先将tabular数据中的特征分成分类特征和连续值特征，如图中的左边为分类特征，右边为连续值特征。

然后将分类特征接入一个transformer模型中，得到的结果与连续值特征拼接起来，最后将拼接的结果接入一个多层全连接网络，得到输出。

在TabTransformer论文中作者还使用了一些自监督学习和半监督学习来对网络进行预训练，其主要思想来自[VIME](/2022/10/27/VIME.html)

关于在tabular训练的时候使用自监督学习和半监督学习，可以参考[VIME](/2022/10/27/VIME.html)

在TabTransformer中主要使用了两种预训练处理，分别是：

masked language modeling (MLM) 

随机遮住一些内容然后视图回复，从而来进行对模型预处理

the replaced token detection (RTD)

随机替换掉一些数据，然后用多个不共享的判别器来分别判断特征是否被替换，一个特征一个判别器，从而来对模型进行预处理。


## SAINT

1 针对TabTransformer中只是将类别特征放入transformer，然后将结果与连续值特征拼接起来，SAINT作者觉得这样做类别特征和
连续值特征之间一些相关的信息丢失。所以SAINT作者设计了一个网络，首先让分类特征数据和连续值数据分别映射到一个高维空间，然后
让他们同时经过transformer块，从而使得模型有更好的效果。

2 通常将数据放入transformer，注意力机制只会作用在特征之间，比如我们将一句话（"这是个很大的大学"）放入transformer中，那么transformer中的
自注意力机制就会去计算每个字与其他字之间的关系，然后再计算得到每个字的输出。在处理tabular数据的时候，每一条样本数据的不同特征就是相当于一个字，
然后transformer关注每个特征之间的关系，然后计算每个特征的输出，得到$$X(a)_1$$，然后将其当作下一个transformer块的输入。本文的作者提出在将
$$X(a)_1$$放入下一个transformer块之前，将$$X(a)_1$$转制一下，再放入一个transformer块中得到结果$$X(a)_2$$，也就是再将注意力机制关注到每个样本上，计算完成后
再将$$X(a)_2$$转制，放入下一个transformer块中。

SAINT具体结构如下图：



