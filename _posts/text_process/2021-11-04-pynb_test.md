---
layout: blog
text_process: true
background-image: http://7568.github.io/images/2021-11-03_2.png
category: 文本处理，机器翻译, 2021-11-04-pynb_test.md
title: 机器翻译 - Seq2Seq with Attention
tags:
- Seq2Seq
- Attention
- 文本处理
- 2021-11-04-pynb_test.md
---

<iframe   src="https://7568.github.io/htmls/2021-11-04-test_000.html" id="external-frame" style="width:100%;" onload="setIframeHeight(this)" frameborder="0" scrolling="no">
# 1 - Sequence to Sequence Learning with Neural Networks

In this series we'll be building a machine learning model to go from once sequence to another, using PyTorch and torchtext. This will be done on German to English translations, but the models can be applied to any problem that involves going from one sequence to another, such as summarization, i.e. going from a sequence to a shorter sequence in the same language.

In this first notebook, we'll start simple to understand the general concepts by implementing the model from the [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) paper. 

</iframe>