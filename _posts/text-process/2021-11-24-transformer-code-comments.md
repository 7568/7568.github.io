---layout: blogtext-process: truebackground-image: http://7568.github.io/images/2021-11-24-transformer-code-comments/img.pngcategory: æ–‡æœ¬å¤„ç†title: æœºå™¨ç¿»è¯‘ - Transformer ä»£ç åˆ†ætags:- transformer- self-attention- æ–‡æœ¬å¤„ç†---[aiayn]:http://7568.github.io/images/2021-11-24-transformer-code-comments/aiayn.png[ModalNet-19]:http://7568.github.io/images/2021-11-24-transformer-code-comments/ModalNet-19.png[ModalNet-20]:http://7568.github.io/images/2021-11-24-transformer-code-comments/ModalNet-20.png[ModalNet-21]:http://7568.github.io/images/2021-11-24-transformer-code-comments/ModalNet-21.png[display_attention]:http://7568.github.io/images/2021-11-24-transformer-code-comments/display_attention.png[display_attention-2]:http://7568.github.io/images/2021-11-24-transformer-code-comments/display_attention-2.png[display_attention-3]:http://7568.github.io/images/2021-11-24-transformer-code-comments/display_attention-3.png#å‰è¨€æœ¬æ–‡çš„å†…å®¹å¤§å¤šæ•°ç¿»è¯‘äº[bentrevett / pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb)ï¼Œåœ¨æœ¬æ–‡ä¸­æˆ‘ä»¬å®ç°äº†[Attention is All You Need ](https://arxiv.org/abs/1706.03762) æ–‡ç« ä¸­çš„transformeræ¨¡å‹ï¼Œåœ¨è¯¥æ¨¡å‹çš„åŸºç¡€ä¸Šæˆ‘ä»¬åšäº†è½»å¾®çš„æ”¹åŠ¨ã€‚æœ¬æ–‡ä¸»è¦æ˜¯ä»‹ç»ç”¨ä»£ç æ¥å®ç°transformerï¼Œä¸å¯¹transformeråšè¯¦ç»†çš„ä»‹ç»ã€‚æ›´å¤šå…³äºtransformerçš„ä¿¡æ¯ï¼Œå¤§å®¶å¯ä»¥å‚è€ƒè¿™ä¸‰ç¯‡æ–‡ç« ï¼ˆ[ğŸ’ ä¸€ ğŸ’](https://www.mihaileric.com/posts/transformers-attention-in-disguise )ï¼Œ[ğŸ’ äºŒ ğŸ’](https://jalammar.github.io/illustrated-transformer )ï¼Œ[ğŸ’ ä¸‰ ğŸ’](http://nlp.seas.harvard.edu/2018/04/03/attention.html )ï¼‰ ï¼Œä¹Ÿå¯ä»¥çœ‹æˆ‘çš„[ğŸ’ ä¸Šä¸€ç¯‡æ–‡ç«  ğŸ’](https://7568.github.io/2021/11/03/transformer.html )ã€‚ä¸‹é¢æ˜¯[ Attention is All You Need ](https://arxiv.org/abs/1706.03762) è®ºæ–‡çš„ä¸€ä¸ªæˆªå›¾ã€‚![aiayn]è¿™ä¸‹é¢æ˜¯ transformer çš„å¤§æ¦‚ç»“æ„![ModalNet-19]# ç®€ä»‹ä¸ä½¿ç”¨å·ç§¯ç½‘ç»œæ¥è¿›è¡Œ Sequence-to-Sequence è®¡ç®—ç±»ä¼¼ï¼Œtransformerä¹Ÿæ²¡æœ‰ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œã€‚è€Œä¸”å®ƒä¹Ÿæ²¡æœ‰ç”¨åˆ°ä»»ä½•çš„å·ç§¯å±‚ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯å®ƒå…¨éƒ¨ä½¿ç”¨çº¿æ€§å±‚ï¼Œæ³¨æ„åŠ›æœºåˆ¶å’Œå½’ä¸€åŒ–æ“ä½œã€‚æˆªè‡³2020å¹´1æœˆï¼ŒTransformeræ˜¯NPLé¢†åŸŸä¸­çš„ä¸»è¦ç½‘ç»œç»“æ„ï¼Œå¹¶ä¸”äººä»¬ä½¿ç”¨å®ƒåœ¨å¤šä¸ªä»»åŠ¡ä¸­éƒ½è¾¾åˆ°äº† state-of-the-art çš„ç»“æœï¼Œè€Œä¸”çœ‹èµ·æ¥å®ƒä¼šæ˜¯æœ€è¿‘æœªæ¥çš„ä¸€ä¸ªè¶‹åŠ¿ã€‚å…¶ä¸­æœ€å—æ¬¢è¿çš„Transformerå˜ä½“å½“å± [ğŸ’ BERT ğŸ’](https://arxiv.org/abs/1810.04805) (Bidirectional Encoder Representations from Transformers) ï¼Œé¢„è®­ç»ƒç‰ˆæœ¬çš„BERTè¢«å¹¿æ³›çš„åº”ç”¨åˆ°NPLæ¨¡å‹çš„ç½‘ç»œä¸­æ¥æ›¿æ¢embeddingå±‚ã€‚ç°åœ¨ç½‘ä¸Šæœ‰ä¸€ä¸ªæ¥å¤„ç†é¢„è®­ç»ƒtransformerçš„é€šç”¨åº“[ğŸ’ Transformers ğŸ’](https://huggingface.co/transformers/) ï¼Œåœ¨[ğŸ’ è¿™é‡Œ ğŸ’](https://huggingface.co/transformers/pretrained_models.html) å¯ä»¥æŸ¥çœ‹æ‰€æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚æœ¬æ–‡å…¶å®ä¸è®ºæ–‡[ Attention is All You Need ](https://arxiv.org/abs/1706.03762) ä¸­ä»‹ç»çš„transformeræœ‰ä¸€ç‚¹ç‚¹ä¸ä¸€æ ·ï¼Œä¸ºå¦‚ä¸‹å‡ ç‚¹ï¼š- è®ºæ–‡ä¸­ä½¿ç”¨çš„æ˜¯é™æ€çš„ä½ç½®ç¼–ç ï¼Œè€Œæœ¬æ–‡ä½¿ç”¨çš„æ˜¯å¯å­¦ä¹ çš„åŠ¨æ€çš„ä½ç½®ç¼–ç ã€‚- è®ºæ–‡ä¸­çš„ä¼˜åŒ–å‡½æ•°ä¸­çš„å­¦ä¹ ç‡ä½¿ç”¨çš„æ˜¯warm-up å’Œåˆ†é˜¶æ®µ cool-downçš„æ–¹æ³•ï¼Œè€Œæœ¬æ–‡ä½¿ç”¨çš„æ˜¯é™æ€çš„å­¦ä¹ ç‡ï¼Œè€Œä¸”æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æ ‡å‡†çš„Adamä¼˜åŒ–å™¨ã€‚- æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨label smoothingæ–¹æ³•ã€‚æˆ‘ä»¬çš„è¿™äº›æ”¹å˜ä½¿å¾—æˆ‘ä»¬çš„ä»£ç çš„æ›´åŠ æ¥è¿‘äºBERTå’Œå¤§å¤šæ•°Transformerå˜ç§çš„è®¾ç½®ã€‚# é¢„å¤„ç†æ•°æ®é¦–å…ˆæˆ‘ä»¬å¯¼å…¥å¿…è¦çš„åº“ï¼Œå’Œåšä¸€äº›å¸¸ç”¨çš„è®¾ç½®```pythonimport torchimport torch.nn as nnimport torch.optim as optimimport torchtextfrom torchtext.datasets import Multi30kfrom torchtext.data import Field, BucketIteratorimport matplotlib.pyplot as pltimport matplotlib.ticker as tickerimport spacyimport numpy as npimport randomimport mathimport timeSEED = 1234random.seed(SEED)np.random.seed(SEED)torch.manual_seed(SEED)torch.cuda.manual_seed(SEED)torch.backends.cudnn.deterministic = True```ç„¶åæˆ‘ä»¬åŠ è½½æ•°æ®é›†å’Œä¸€äº›é¢„å¤„ç†ï¼Œæˆ‘ä»¬è¿™æ¬¡è¿˜æ˜¯æ¥è¿›è¡Œå¾·è¯­äºè‹±è¯­çš„ç¿»è¯‘ä»»åŠ¡ï¼ˆå°†å¾·è¯­ç¿»è¯‘æˆè‹±è¯­ï¼‰ã€‚```pythonspacy_de = spacy.load('de_core_news_sm')spacy_en = spacy.load('en_core_web_sm')def tokenize_de(text):    """    Tokenizes German text from a string into a list of strings    """    return [tok.text for tok in spacy_de.tokenizer(text)]def tokenize_en(text):    """    Tokenizes English text from a string into a list of strings    """    return [tok.text for tok in spacy_en.tokenizer(text)]SRC = Field(tokenize = tokenize_de,             init_token = '<sos>',             eos_token = '<eos>',             lower = True,             batch_first = True)TRG = Field(tokenize = tokenize_en,             init_token = '<sos>',             eos_token = '<eos>',             lower = True,             batch_first = True)train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),                                                     fields = (SRC, TRG))SRC.build_vocab(train_data, min_freq = 2)TRG.build_vocab(train_data, min_freq = 2)device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')BATCH_SIZE = 128train_iterator, valid_iterator, test_iterator = BucketIterator.splits(    (train_data, valid_data, test_data),      batch_size = BATCH_SIZE,     device = device)```å…¶ä¸­`<sos>`å’Œ`<eos>`åˆ†åˆ«æŒ‡çš„æ˜¯å¼€å§‹å’Œç»“æŸç¬¦ã€‚æˆ‘ä»¬å°†æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†ï¼ˆtrain_dataï¼‰ï¼ŒéªŒè¯é›†ï¼ˆvalid_dataï¼‰ï¼Œå’Œæµ‹è¯•é›†ï¼ˆtest_dataï¼‰ã€‚å…¶ä¸­éªŒè¯é›†æ˜¯ç”¨æ¥åœ¨è®­ç»ƒçš„æ—¶å€™æŸ¥çœ‹æ¨¡å‹çš„æ•ˆæœçš„ï¼Œè€Œæµ‹è¯•é›†æ˜¯åœ¨è®­ç»ƒå®Œæˆä¹‹åæ¥æ£€éªŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›çš„ã€‚## æ¨¡å‹æ„å»ºä¸ä¹‹å‰ä»‹ç»çš„ç¿»è¯‘æ¨¡å‹ä¸€æ ·ï¼Œtransformeræ¨¡å‹ä¹Ÿæ˜¯encoder-decoderç»“æ„ï¼Œå…¶ä¸­encoderæ˜¯ç”¨æ¥å¯¹è¾“å…¥çš„å¥å­è¿›è¡Œç¼–ç å’Œæå–ç‰¹å¾ï¼Œè€Œdecoderæ˜¯å°†encoderçš„ç»“æœè¿›è¡Œè§£ç ï¼Œæœ€ç»ˆçš„åˆ°ç¿»è¯‘çš„ç»“æœã€‚### encoderä¸‹é¢æ˜¯encoderçš„ç»“æ„å›¾ï¼Œåœ¨æˆ‘çš„[ğŸ’ ä¸Šä¸€ç¯‡æ–‡ç«  ğŸ’](https://7568.github.io/2021/11/03/transformer.html ) æœ‰å¯¹encoderåšè¯¦ç»†çš„ä»‹ç»ã€‚![ModalNet-19]ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒencoderåŒ…æ‹¬input embeddingï¼ŒPositional Encodingï¼ŒMulti-Head Attentionï¼ŒAdd&Normï¼ŒFeed Forwardè¿™äº›ç»“æ„ï¼Œä»£ç å¦‚ä¸‹ï¼š```pythonclass Encoder(nn.Module):    def __init__(self,  input_dim,  hid_dim,  n_layers,  n_heads,  pf_dim, dropout,  device, max_length = 100):        super().__init__()        self.device = device                self.tok_embedding = nn.Embedding(input_dim, hid_dim)        self.pos_embedding = nn.Embedding(max_length, hid_dim)                self.layers = nn.ModuleList([EncoderLayer(hid_dim,  n_heads,   pf_dim, dropout, device)  for _ in range(n_layers)])                self.dropout = nn.Dropout(dropout)                self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)            def forward(self, src, src_mask):                #src = [batch size, src len]        #src_mask = [batch size, 1, 1, src len]                batch_size = src.shape[0]        src_len = src.shape[1]        # å…ˆç”Ÿæˆä¸€ä¸ªé¡ºåºçš„ä½ç½®ç¼–ç         pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)                #pos = [batch size, src len]        # è¿›è¡Œä½ç½®ç¼–ç         src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))                #src = [batch size, src len, hid dim]                for layer in self.layers:            src = layer(src, src_mask)                    #src = [batch size, src len, hid dim]                    return src```### EncoderLayeræ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹EncoderLayerçš„ä»£ç ï¼Œå…¶ä¸­åŒ…å«ä¸¤ä¸ªå½’ä¸€åŒ–å±‚ layer_normï¼Œä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›MultiHeadAttentionLayerå’Œä¸€ä¸ªä½ç½®ç¼–ç PositionwiseFeedforwardLayerã€‚å…¶ä¸­ layer_norm ç®€å•ç†è§£å°±æ˜¯å°†å•ä¸ªæ ·æœ¬è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä½¿å¾—å®ƒå‡å€¼ä¸º0æ–¹å·®ä¸º1ï¼Œä»è€Œèƒ½å¤Ÿè®©æ¨¡å‹æ›´å¿«çš„æ”¶æ•›ï¼Œåœ¨æˆ‘çš„[è¿™ç¯‡æ–‡ç« ](https://7568.github.io/2021/11/06/neural-network-architecture.html) ä¸­æœ‰å…³äº layer_norm çš„ä»‹ç»ã€‚```pythonclass EncoderLayer(nn.Module):    def __init__(self,  hid_dim,  n_heads,  pf_dim,  dropout,  device):        super().__init__()                self.self_attn_layer_norm = nn.LayerNorm(hid_dim)        self.ff_layer_norm = nn.LayerNorm(hid_dim)        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,                                                                      pf_dim,                                                                      dropout)        self.dropout = nn.Dropout(dropout)            def forward(self, src, src_mask):                #src = [batch size, src len, hid dim]        #src_mask = [batch size, 1, 1, src len]                         #self attention        _src, _ = self.self_attention(src, src, src, src_mask)                #dropout, residual connection and layer norm        src = self.self_attn_layer_norm(src + self.dropout(_src))                #src = [batch size, src len, hid dim]                #positionwise feedforward        _src = self.positionwise_feedforward(src)                #dropout, residual and layer norm        src = self.ff_layer_norm(src + self.dropout(_src))                #src = [batch size, src len, hid dim]                return src```### MultiHeadAttentionLayerå…¶ä¸­å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ç»“æ„å¦‚ä¸‹ï¼š![ModalNet-20]åœ¨æˆ‘çš„[ğŸ’ ä¸Šä¸€ç¯‡æ–‡ç«  ğŸ’](https://7568.github.io/2021/11/03/transformer.html ) æœ‰å¯¹å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶åšè¯¦ç»†çš„ä»‹ç»ã€‚å¤§æ¦‚å¯ä»¥ç†è§£ä¸ºä½¿ç”¨å¤šä¸ªä¸åŒçš„åˆå§‹åŒ–å‚æ•°ï¼Œç”Ÿæˆå¤šä¸ªä¸åŒçš„æ³¨æ„åŠ›çš„ç»“æœï¼Œç„¶åå°†ä»–ä»¬æ‹¼æ¥èµ·æ¥ï¼Œç„¶åå†ä¹˜ä»¥ä¸€ä¸ªå‚æ•° W å¾—åˆ°æœ€ç»ˆçš„ç»“æœï¼Œå½“ä½œä¸‹ä¸€ä¸ªå±‚çš„è¾“å…¥ã€‚ä»£ç å¦‚ä¸‹ï¼š```pythonclass MultiHeadAttentionLayer(nn.Module):    def __init__(self, hid_dim, n_heads, dropout, device):        super().__init__()                assert hid_dim % n_heads == 0                self.hid_dim = hid_dim        self.n_heads = n_heads        self.head_dim = hid_dim // n_heads                self.fc_q = nn.Linear(hid_dim, hid_dim)        self.fc_k = nn.Linear(hid_dim, hid_dim)        self.fc_v = nn.Linear(hid_dim, hid_dim)                self.fc_o = nn.Linear(hid_dim, hid_dim)                self.dropout = nn.Dropout(dropout)                self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)            def forward(self, query, key, value, mask = None):                batch_size = query.shape[0]                #query = [batch size, query len, hid dim]        #key = [batch size, key len, hid dim]        #value = [batch size, value len, hid dim]        # åœ¨self.fc_qï¼Œself.fc_kï¼Œself.fc_vä¸­å·²ç»åŒ…å«äº†å¤šå¤´               Q = self.fc_q(query)        K = self.fc_k(key)        V = self.fc_v(value)                #Q = [batch size, query len, hid dim]        #K = [batch size, key len, hid dim]        #V = [batch size, value len, hid dim]        # å°†å¤šå¤´å•ç‹¬å‡ºæ¥                Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)                #Q = [batch size, n heads, query len, head dim]        #K = [batch size, n heads, key len, head dim]        #V = [batch size, n heads, value len, head dim]                        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale                #energy = [batch size, n heads, query len, key len]                if mask is not None:            energy = energy.masked_fill(mask == 0, -1e10)                attention = torch.softmax(energy, dim = -1)                        #attention = [batch size, n heads, query len, key len]                        x = torch.matmul(self.dropout(attention), V)                #x = [batch size, n heads, query len, head dim]                x = x.permute(0, 2, 1, 3).contiguous()                #x = [batch size, query len, n heads, head dim]                x = x.view(batch_size, -1, self.hid_dim)                #x = [batch size, query len, hid dim]                x = self.fc_o(x)                #x = [batch size, query len, hid dim]                return x, attention```ä»ä»£ç ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒQï¼ŒKï¼ŒVéƒ½æ˜¯é€šè¿‡å°†è¾“å…¥æ”¾å…¥ä¸€ä¸ªå…¨è¿æ¥å±‚å¾—åˆ°çš„è¾“å‡ºï¼Œè€Œä¸”åœ¨Qï¼ŒKï¼ŒVçš„å¤„ç†ä¸­æ˜¯å°†å¤šå¤´æ”¾åœ¨ä¸€èµ·å¤„ç†çš„ï¼Œåé¢å†å°†ä»–ä»¬åˆ†å¼€ã€‚### PositionwiseFeedforwardLayeræ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹PositionwiseFeedforwardLayerçš„å®ç°ï¼Œå¯ä»¥çœ‹åˆ°è¯¥ä½ç½®ç¼–ç å°±æ˜¯ä¸¤ä¸ªå…¨è¿æ¥```pythonclass PositionwiseFeedforwardLayer(nn.Module):    def __init__(self, hid_dim, pf_dim, dropout):        super().__init__()                self.fc_1 = nn.Linear(hid_dim, pf_dim)        self.fc_2 = nn.Linear(pf_dim, hid_dim)                self.dropout = nn.Dropout(dropout)            def forward(self, x):                #x = [batch size, seq len, hid dim]                x = self.dropout(torch.relu(self.fc_1(x)))                #x = [batch size, seq len, pf dim]                x = self.fc_2(x)                #x = [batch size, seq len, hid dim]                return x```### Decoder decoderçš„ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œdecoderä¸encoderå…¶å®æ˜¯å¾ˆç›¸ä¼¼çš„ï¼Œåªæ˜¯åœ¨å…¶ä¸­çš„æ¯ä¸€ä¸ªdecoder blockä¸­ï¼Œå¤šåŠ äº†ä¸€ä¸ªMasked Multi-Head Attentionçš„æ®‹å·®ç½‘ç»œï¼Œè€Œä¸”å®ƒçš„Multi-Head Attentionæ¥æ”¶çš„å‚æ•°ä¸ä»…æœ‰ä»ä¸Šä¸€çº§ä¼ é€’è¿‡æ¥çš„ï¼Œè¿˜æœ‰ä»encoderä¸­ä¼ é€’è¿‡æ¥çš„ã€‚![ModalNet-21]decoderä»£ç å¦‚ä¸‹```pythonclass Decoder(nn.Module):    def __init__(self,  output_dim,  hid_dim,  n_layers, n_heads,  pf_dim,  dropout,  device, max_length = 100):        super().__init__()                self.device = device                self.tok_embedding = nn.Embedding(output_dim, hid_dim)        self.pos_embedding = nn.Embedding(max_length, hid_dim)                self.layers = nn.ModuleList([DecoderLayer(hid_dim,  n_heads,  pf_dim,  dropout,  device) for _ in range(n_layers)])                self.fc_out = nn.Linear(hid_dim, output_dim)                self.dropout = nn.Dropout(dropout)                self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)            def forward(self, trg, enc_src, trg_mask, src_mask):                #trg = [batch size, trg len]        #enc_src = [batch size, src len, hid dim]        #trg_mask = [batch size, 1, trg len, trg len]        #src_mask = [batch size, 1, 1, src len]                        batch_size = trg.shape[0]        trg_len = trg.shape[1]                pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)                                    #pos = [batch size, trg len]                    trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))                        #trg = [batch size, trg len, hid dim]                for layer in self.layers:            trg, attention = layer(trg, enc_src, trg_mask, src_mask)                #trg = [batch size, trg len, hid dim]        #attention = [batch size, n heads, trg len, src len]                output = self.fc_out(trg)                #output = [batch size, trg len, output dim]                    return output, attention```### Decoder Layerä¸‹é¢æ˜¯ Decoder Layer çš„ä»£ç ï¼ŒåŸºæœ¬ä¸Šè·Ÿ encoder layer ä¸€æ ·ï¼Œåªæ˜¯åšäº†ç¨å¾®çš„æ”¹åŠ¨ã€‚```pythonclass DecoderLayer(nn.Module):    def __init__(self,  hid_dim,  n_heads,  pf_dim,  dropout,  device):        super().__init__()                self.self_attn_layer_norm = nn.LayerNorm(hid_dim)        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)        self.ff_layer_norm = nn.LayerNorm(hid_dim)        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,  pf_dim,  dropout)        self.dropout = nn.Dropout(dropout)            def forward(self, trg, enc_src, trg_mask, src_mask):                #trg = [batch size, trg len, hid dim]        #enc_src = [batch size, src len, hid dim]        #trg_mask = [batch size, 1, trg len, trg len]        #src_mask = [batch size, 1, 1, src len]                #self attention        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)                #dropout, residual connection and layer norm        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))                    #trg = [batch size, trg len, hid dim]                    #encoder attention        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)                #dropout, residual connection and layer norm        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))                            #trg = [batch size, trg len, hid dim]                #positionwise feedforward        _trg = self.positionwise_feedforward(trg)                #dropout, residual and layer norm        trg = self.ff_layer_norm(trg + self.dropout(_trg))                #trg = [batch size, trg len, hid dim]        #attention = [batch size, n heads, trg len, src len]                return trg, attention```ä»£ç ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæœ‰ä¸¤ä¸ªmaskï¼Œåˆ†åˆ«æ˜¯trg_maskå’Œ src_maskï¼Œåœ¨æˆ‘çš„[ğŸ’ ä¸Šä¸€ç¯‡æ–‡ç«  ğŸ’](https://7568.github.io/2021/11/03/rnn-seq2seqModel.html#masking ) ä¸­åœ¨ä»‹ç»å¦‚ä½•æå‡å‡†ç¡®ç‡çš„æ—¶å€™æœ‰æåˆ°è¿‡ä¸€æ¬¡maskï¼Œè¯¥maskä¸»è¦æ˜¯åœ¨è¿›è¡Œattentionçš„æ—¶å€™ï¼Œè¿‡æ»¤æ‰padingçš„åœ°æ–¹ã€‚ä¸è¿‡å…¶ä¸­çš„maskä¸è¿™é‡Œçš„trg_maskå’Œ src_maskæœ‰ç‚¹ä¸ä¸€æ ·ï¼Œæ€æƒ³å·®ä¸å¤šï¼Œä½†æ˜¯å®ç°æ–¹å¼ä¸ä¸€æ ·ã€‚åœ¨Seq2Seqä»£ç ï¼Œé‡Œé¢æœ‰å…³äºmaskçš„å®ç°ã€‚### Seq2Seq```pythonclass Seq2Seq(nn.Module):    def __init__(self,  encoder, decoder,  src_pad_idx, trg_pad_idx, device):        super().__init__()                self.encoder = encoder        self.decoder = decoder        self.src_pad_idx = src_pad_idx        self.trg_pad_idx = trg_pad_idx        self.device = device            def make_src_mask(self, src):                #src = [batch size, src len]                src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)        #src_mask = [batch size, 1, 1, src len]        return src_mask        def make_trg_mask(self, trg):                #trg = [batch size, trg len]                trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)                #trg_pad_mask = [batch size, 1, 1, trg len]                trg_len = trg.shape[1]                trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()                #trg_sub_mask = [trg len, trg len]                    trg_mask = trg_pad_mask & trg_sub_mask                #trg_mask = [batch size, 1, trg len, trg len]                return trg_mask    def forward(self, src, trg):                #src = [batch size, src len]        #trg = [batch size, trg len]                        src_mask = self.make_src_mask(src)        trg_mask = self.make_trg_mask(trg)                #src_mask = [batch size, 1, 1, src len]        #trg_mask = [batch size, 1, trg len, trg len]                enc_src = self.encoder(src, src_mask)                #enc_src = [batch size, src len, hid dim]                        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)                #output = [batch size, trg len, output dim]        #attention = [batch size, n heads, trg len, src len]                return output, attention```ä»ä»£ç ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äº srcï¼Œå°±æ˜¯ç›´æ¥å°†é\<pad\>è®¾ç½®ä¸ºtrueï¼Œå°†\<pad\>è®¾ç½®ä¸ºfalseã€‚ è€Œå¯¹äºtrgï¼Œåˆ™ä¸ä¸€æ ·ã€‚å¯¹äºtrgï¼Œå…ˆé€šè¿‡å¥å­ä¸­å•è¯æ˜¯å¦ä¸ºé\<pad\>ï¼Œç”Ÿæˆä¸€ä¸ªmask1ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªå†…å®¹å…¨ä¸º1çš„ä¸‹ä¸‰è§’çŸ©é˜µmask2ï¼Œæœ€åå°†ä»–ä»¬è¿›è¡Œ"ä¸"è¿ç®—å¾—åˆ°æœ€åçš„trg-maskã€‚æ¯”å¦‚trgä¸º ["hello", "how", "are", "you", "?", \<pad\>, \<pad\>] é‚£ä¹ˆ mask1 å°±æ˜¯ $$\[ True,  True,  True,  True,  True, False, False \]$$ ï¼Œmask2å°±æ˜¯$$\begin{bmatrix}True, False, False, False, False, False, False \\True,  True, False, False, False, False, False \\True,  True, True,  False, False, False, False \\True,  True, True,  True,  False, False, False \\True,  True, True,  True,  True,  False, False \\True,  True, True,  True,  True,  True,  False \\True,  True, True,  True,  True,  True,  True \\ \end{bmatrix}$$ç„¶åå°†mask1å’Œmask2è¿›è¡Œ"ä¸"è¿ç®—ï¼Œå¾—åˆ°æœ€ç»ˆçš„trg-maskå¦‚ä¸‹$$\begin{bmatrix}True, False, False, False, False, False, False \\True,  True, False, False, False, False, False \\True,  True, True,  False, False, False, False \\True,  True, True,  True,  False, False, False \\True,  True, True,  True,  True,  False, False \\True,  True, True,  True,  True,  False, False \\True,  True, True,  True,  True,  False, False \\ \end{bmatrix}$$è‡³äºä¸ºä»€ä¹ˆè¦è¿™æ ·åšï¼Œæˆ‘æƒ³ä¸€éƒ¨åˆ†çš„ç†ç”±æ˜¯è¿™æ ·å¯ä»¥æ›´åŠ ç²¾ç»†åŒ–attentionï¼Œæœ‰çš„attentionå…³æ³¨ç¬¬ä¸€ä¸ªå•è¯ï¼Œæœ‰ä¸ªattentionå…³æ³¨å‰ä¸¤ä¸ªå•è¯ï¼Œç­‰ç­‰ï¼Œè¿™æ ·æ›´ç²¾ç»†åŒ–çš„attentionå¯èƒ½ä¼šæé«˜attentionçš„æ•ˆæœã€‚### Training the Seq2Seq Modelæ¥ä¸‹æ¥æˆ‘ä»¬å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œä»£ç å¦‚ä¸‹```pythonINPUT_DIM = len(SRC.vocab)OUTPUT_DIM = len(TRG.vocab)HID_DIM = 256ENC_LAYERS = 3DEC_LAYERS = 3ENC_HEADS = 8DEC_HEADS = 8ENC_PF_DIM = 512DEC_PF_DIM = 512ENC_DROPOUT = 0.1DEC_DROPOUT = 0.1enc = Encoder(INPUT_DIM,               HID_DIM,               ENC_LAYERS,               ENC_HEADS,               ENC_PF_DIM,               ENC_DROPOUT,               device)dec = Decoder(OUTPUT_DIM,               HID_DIM,               DEC_LAYERS,               DEC_HEADS,               DEC_PF_DIM,               DEC_DROPOUT,               device)SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)def count_parameters(model):    return sum(p.numel() for p in model.parameters() if p.requires_grad)print(f'The model has {count_parameters(model):,} trainable parameters')def initialize_weights(m):    if hasattr(m, 'weight') and m.weight.dim() > 1:        nn.init.xavier_uniform_(m.weight.data)model.apply(initialize_weights);LEARNING_RATE = 0.0005optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)def train(model, iterator, optimizer, criterion, clip):        model.train()        epoch_loss = 0        for i, batch in enumerate(iterator):                src = batch.src        trg = batch.trg                optimizer.zero_grad()                output, _ = model(src, trg[:,:-1])                        #output = [batch size, trg len - 1, output dim]        #trg = [batch size, trg len]                    output_dim = output.shape[-1]                    output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)                        #output = [batch size * trg len - 1, output dim]        #trg = [batch size * trg len - 1]                    loss = criterion(output, trg)                loss.backward()                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)                optimizer.step()                epoch_loss += loss.item()            return epoch_loss / len(iterator)def epoch_time(start_time, end_time):    elapsed_time = end_time - start_time    elapsed_mins = int(elapsed_time / 60)    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))    return elapsed_mins, elapsed_secsN_EPOCHS = 10CLIP = 1best_valid_loss = float('inf')for epoch in range(N_EPOCHS):        start_time = time.time()        train_loss = train(model, train_iterator, optimizer, criterion, CLIP)    valid_loss = evaluate(model, valid_iterator, criterion)        end_time = time.time()        epoch_mins, epoch_secs = epoch_time(start_time, end_time)        if valid_loss < best_valid_loss:        best_valid_loss = valid_loss        torch.save(model.state_dict(), 'tut6-model.pt')        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')```æ¥ä¸‹æ¥æˆ‘ä»¬è¿›è¡Œæ¨¡å‹è¯„ä¼°```pythondef evaluate(model, iterator, criterion):        model.eval()        epoch_loss = 0        with torch.no_grad():            for i, batch in enumerate(iterator):            src = batch.src            trg = batch.trg            output, _ = model(src, trg[:,:-1])                        #output = [batch size, trg len - 1, output dim]            #trg = [batch size, trg len]                        output_dim = output.shape[-1]                        output = output.contiguous().view(-1, output_dim)            trg = trg[:,1:].contiguous().view(-1)                        #output = [batch size * trg len - 1, output dim]            #trg = [batch size * trg len - 1]                        loss = criterion(output, trg)            epoch_loss += loss.item()            return epoch_loss / len(iterator)model.load_state_dict(torch.load('tut6-model.pt'))test_loss = evaluate(model, test_iterator, criterion)print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')```æ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥å®é™…è¿è¡Œçœ‹çœ‹ç¿»è¯‘è¿‡ç¨‹å’Œç¿»è¯‘ç»“æœï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬å®é™…ç¿»è¯‘æ—¶å€™çš„æµç¨‹- å°†è¾“å…¥çš„å­—ç¬¦ä¸²å¥å­è¿›è¡Œtokenizeæ“ä½œï¼Œä¸»è¦æ˜¯å°†å¥å­å˜æˆå•è¯æ•°ç»„- åŠ ä¸Šå¼€å§‹å’Œç»“æŸæ ‡å¿—\<sos\> and \<eos\>- å°†è¾“å…¥è¿›è¡Œæ•°å­—åŒ–ï¼Œå°†å­—ç¬¦è½¬æˆæ•°å­—- å°†è¾“å…¥ç»„è£…æˆå¸¦batchç»´åº¦çš„tensorï¼Œä¿æŒä¸è®­ç»ƒæ—¶æ•°æ®ç»´åº¦ä¸€è‡´- åˆ›å»ºè¾“å…¥æ•°æ®çš„mask- å°†è¾“å…¥æ•°æ®å’Œmaskæ”¾å…¥åˆ°encoderä¸­ï¼Œå¾—åˆ°ä¸€ä¸ªè¾“å‡º- åˆ›å»ºä¸€ä¸ªè¾“å‡ºçš„listï¼Œç¬¬ä¸€ä¸ªä½ç½®åˆå§‹åŒ–ä¸ºå¼€å§‹æ ‡å¿—ç¬¦<sos>- å¾ªç¯è¿è¡Œdecoderï¼Œç›´åˆ°åˆ°è¾¾è®¾ç½®çš„æœ€å¤§æ¬¡æ•°   - å°†å½“å‰è¾“å‡ºçš„listè½¬æ¢æˆtensorï¼Œå¹¶æ·»åŠ batchç»´åº¦ï¼Œç¬¬ä¸€æ¬¡çš„æ—¶å€™listä¸­åªæœ‰å¼€å§‹æ ‡å¿—   - åˆ›å»ºä¸€ä¸ªè¾“å‡ºç›®æ ‡çš„mask   - å°†å½“å‰çš„è¾“å‡ºlistï¼Œä¹‹å‰encoderçš„è¾“å‡ºï¼Œå’Œå½“å‰è¾“å‡ºçš„listå¯¹åº”çš„maskä¸€èµ·æ”¾è¿›decoderä¸­   - è·å¾—ä¸€ä¸ªè¾“å‡ºå•è¯çš„é¢„æµ‹   - å°†ä¸Šä¸€æ­¥è·å¾—çš„ä¸€ä¸ªå•è¯æ·»åŠ è¿›è¾“å‡ºlistä¸­   - å¦‚æœç¢°åˆ°ç»“æŸæ ‡å¿—ç¬¦ï¼Œå°±åœæ­¢å¾ªç¯- å°†è¾“å‡ºlistä¸­çš„æ•°æ®è½¬æ¢æˆtokens- å»æ‰å¼€å§‹ç¬¦å·ï¼Œå¾—åˆ°è¾“å‡ºå’Œattention```pythondef translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):        model.eval()            if isinstance(sentence, str):        nlp = spacy.load('de_core_news_sm')        tokens = [token.text.lower() for token in nlp(sentence)]    else:        tokens = [token.lower() for token in sentence]    tokens = [src_field.init_token] + tokens + [src_field.eos_token]            src_indexes = [src_field.vocab.stoi[token] for token in tokens]    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)        src_mask = model.make_src_mask(src_tensor)        with torch.no_grad():        enc_src = model.encoder(src_tensor, src_mask)    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]    for i in range(max_len):        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)        trg_mask = model.make_trg_mask(trg_tensor)                with torch.no_grad():            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)                pred_token = output.argmax(2)[:,-1].item()                trg_indexes.append(pred_token)        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:            break        trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]        return trg_tokens[1:], attentiondef display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):        assert n_rows * n_cols == n_heads        fig = plt.figure(figsize=(15,25))        for i in range(n_heads):                ax = fig.add_subplot(n_rows, n_cols, i+1)                _attention = attention.squeeze(0)[i].cpu().detach().numpy()        cax = ax.matshow(_attention, cmap='bone')        ax.tick_params(labelsize=12)        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'],                            rotation=45)        ax.set_yticklabels(['']+translation)        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))    plt.show()    plt.close()example_idx = 8src = vars(train_data.examples[example_idx])['src']trg = vars(train_data.examples[example_idx])['trg']print(f'src = {src}')print(f'trg = {trg}')translation, attention = translate_sentence(src, SRC, TRG, model, device)print(f'predicted trg = {translation}')display_attention(src, translation, attention)```![display_attention]æ¥ä¸‹æ¥æˆ‘ä»¬å†çœ‹çœ‹åœ¨éªŒè¯é›†ä¸Šçš„attentionçš„æ•ˆæœ```pythonexample_idx = 6src = vars(valid_data.examples[example_idx])['src']trg = vars(valid_data.examples[example_idx])['trg']print(f'src = {src}')print(f'trg = {trg}')translation, attention = translate_sentence(src, SRC, TRG, model, device)print(f'predicted trg = {translation}')display_attention(src, translation, attention)```![display_attention-2]æ¥ä¸‹æ¥æˆ‘ä»¬å†çœ‹çœ‹åœ¨æµ‹è¯•é›†ä¸Šçš„attentionçš„æ•ˆæœ```pythonexample_idx = 10src = vars(test_data.examples[example_idx])['src']trg = vars(test_data.examples[example_idx])['trg']print(f'src = {src}')print(f'trg = {trg}')translation, attention = translate_sentence(src, SRC, TRG, model, device)print(f'predicted trg = {translation}')display_attention(src, translation, attention)```![display_attention-3]æœ€åæˆ‘ä»¬é€šè¿‡è®¡ç®—BLUEå¾—åˆ†ï¼Œçœ‹çœ‹è¯¥æ¨¡å‹çš„æ•ˆæœ```pythonfrom torchtext.data.metrics import bleu_scoredef calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):        trgs = []    pred_trgs = []        for datum in data:                src = vars(datum)['src']        trg = vars(datum)['trg']                pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)                #cut off <eos> token        pred_trg = pred_trg[:-1]                pred_trgs.append(pred_trg)        trgs.append([trg])            return bleu_score(pred_trgs, trgs)bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)print(f'BLEU score = {bleu_score*100:.2f}')```æœ€ç»ˆç»“æœä¸º BLEU score = 36.52åˆ°æ­¤transformerçš„ä»£ç éƒ¨åˆ†å°±ä»‹ç»å®Œæ¯•ã€‚ğŸ‘‰ï¸ ğŸ‘‰ï¸ ğŸ‘‰ï¸ ç‚¹å‡»[ ğŸ’ ğŸ’ ğŸ’ å¯ä»¥ç›´æ¥ä¸‹è½½ Transformer æ¨¡å‹çš„ä»£ç ](https://7568.github.io/codes/text-process/2021-11-24-transformer-code-comments.py)ã€‚å°†ä»£ç ä¸­ `is_train = False` æ”¹æˆ `is_train = True` å°±å¯ä»¥è®­ç»ƒäº†ï¼Œæµ‹è¯•çš„æ—¶å€™å†æ”¹å›æ¥å³å¯ã€‚