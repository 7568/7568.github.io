---
layout: blog
text-process: true
background-image: http://7568.github.io/images/2021-11-03-seq2seqModel/img.png
category: æ–‡æœ¬å¤„ç†
title: æœºå™¨ç¿»è¯‘ - Seq2Seq with Attention
mathjax: true
tags:
- Seq2Seq
- Attention
- æ–‡æœ¬å¤„ç†
---

[2021-11-04_seq2seq_3]:http://7568.github.io/images/2021-11-03-seq2seqModel/2021-11-04_seq2seq_3.png
[input-batch]:http://7568.github.io/images/2021-11-03-seq2seqModel/input-batch.png
[padded-input-batch]:http://7568.github.io/images/2021-11-03-seq2seqModel/padded-input-batch.png
[input-numericalize]:http://7568.github.io/images/2021-11-03-seq2seqModel/input-numericalize.png
[lstm-struct]:http://7568.github.io/images/2021-11-03-seq2seqModel/lstm-struct.png
[seq2seq-lstm]:http://7568.github.io/images/2021-11-03-seq2seqModel/seq2seq-lstm.png
[seq2seq2-encoder]:http://7568.github.io/images/2021-11-03-seq2seqModel/seq2seq-encoder.png
[seq2seq2-decoder]:http://7568.github.io/images/2021-11-03-seq2seqModel/seq2seq-decoder.png
[Seq2Seq-model]:http://7568.github.io/images/2021-11-03-seq2seqModel/Seq2Seq-model.png
[seq2seq-with-gru]:http://7568.github.io/images/2021-11-03-seq2seqModel/seq2seq-with-gru.png
[bidirectional-rnn]:http://7568.github.io/images/2021-11-03-seq2seqModel/bidirectional-rnn.png
[rnn-attention-encoder]:http://7568.github.io/images/2021-11-03-seq2seqModel/rnn-attention-encoder.png
[rnn-attention-arcitecture]:http://7568.github.io/images/2021-11-03-seq2seqModel/rnn-attention-arcitecture.png

# ç®€ä»‹

åœ¨æ–‡æœ¬å¤„ç†ä¸­ä¸¤ä¸ªç»å…¸çš„ç½‘ç»œæ¨¡å‹ï¼Œä¸€ä¸ªæ˜¯åŸºäºå¾ªç¯ç¥ç»ç½‘ç»œåŠ ä¸Š attention çš„ Seq2Seq å’Œå®Œå…¨åŸºäº attention çš„ Transformerã€‚è¿™ä¸¤ä¸ªæ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä¸­éƒ½å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚
æœ¬æ–‡ä¸­å¾ˆå¤§ä¸€éƒ¨åˆ†å†…å®¹æ¥è‡ªç¿»è¯‘
[Seq2seq Models With Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
å’Œ
[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) ã€‚
ä»£ç å‚è€ƒäº [https://github.com/bentrevett/pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq)

æœ¬ç¯‡å°†ä¸»è¦è®²è¿°å’Œç¿»è¯‘åœ¨[Seq2seq Models With Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
ä¸­çš„å†…å®¹ï¼ŒSeq2seqçš„ç›¸å…³è®ºæ–‡åœ°å€åœ¨[Sutskever et al.](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) , [Cho et al., 2014](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)
æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç¯‡[æ–‡ç« ](https://7568.github.io/2021/11/03/transformer.html) ä¸­è®²è¿°Transformerï¼Œä¹Ÿå°±æ˜¯[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) ä¸­çš„å†…å®¹

# æ¨¡å‹ä»‹ç»

Sequence-to-sequenceæ¨¡å‹æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œåœ¨å¾ˆå¤šåƒæœºå™¨ç¿»è¯‘ï¼ŒçŸ­æ–‡æ€»ç»“ï¼Œå’Œå›¾åƒæè¿°ç­‰ä»»åŠ¡ä¸­éƒ½å–å¾—äº†å¾ˆå¥½çš„æˆç»©ã€‚æ¥ä¸‹æ¥æˆ‘å°†é€šè¿‡æœ¬blogæ¥ä»‹ç» Seq2Seq æ¨¡å‹çš„ç›¸å…³å†…å®¹å’Œä»£ç ã€‚å¸Œæœ›å¯¹äºåˆå­¦è€…æœ‰æ‰€å¸®åŠ©ã€‚
æœ¬æ–‡ä¸»è¦è®²è¿°çš„ä»£ç æ˜¯ Seq2Seq æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä¸Šè‹±æ–‡å¯¹ä¸­æ–‡çš„ç¿»è¯‘ã€‚
<br/>
Seq2Seq æ¨¡å‹æ˜¯å…¸å‹çš„ encoder-decoder æ¨¡å‹ï¼Œä¸‹é¢çš„åŠ¨ç”»å°†ä»‹ç» Seq2Seq è¿›è¡Œæœºå™¨ç¿»è¯‘æ—¶å€™çš„åŸºæœ¬å·¥ä½œæµç¨‹ã€‚å·¦è¾¹æ˜¯è¾“å…¥ï¼Œå³è¾¹æ˜¯è¾“å‡ºã€‚

<video width="100%" height="auto" loop autoplay controls>
  <source src="http://7568.github.io/images/2021-11-03-seq2seqModel/2021-11-04_seq2seq_1.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

ä¸‹é¢è¿™ä¸ªè§†é¢‘æ¥è‡ªäº [https://github.com/google/seq2seq](https://github.com/google/seq2seq) ä¸è¿‡ `https://github.com/google/seq2seq`ä¸­çš„å†…å®¹å¯¹æœ¬æ–‡å…³ç³»ä¸å¤§

<video width="100%" height="auto" loop autoplay controls>
  <source src="http://7568.github.io/images/2021-11-03-seq2seqModel/2021-11-04_seq2seq_2.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

## encoder

åœ¨ Seq2Seq æ¨¡å‹çš„ encoder ä¸­ï¼Œè¦è¿›è¡Œçš„å·¥ä½œæœ‰ï¼š

1. å°†è¾“å…¥ X1 å­—ç¬¦ç¼–ç ï¼Œå˜æˆæ•°å­—ç±»å‹ï¼Œå³ Word2Vecï¼Œå¾—åˆ° X1_Vecï¼Œå¦‚æœæˆ‘ä»¬çš„è¾“å…¥æ˜¯ "æ—©ä¸Šå¥½"ï¼Œåœ¨ Word2Vec ä¸­ï¼Œå…ˆä¼šåŠ ä¸Šå¼€å§‹æ ‡å¿— `<sos>` å’Œç»“æŸæ ‡å¿— `<eos>` ï¼Œ
   è¿™æ ·è¾“å…¥å°±å˜æˆäº†5ä¸ªå­—ç¬¦ï¼Œç„¶åæ¯ä¸ªå­—ç¬¦ç”¨ä¸€ä¸²0å’Œ1è¡¨ç¤ºï¼Œäºæ˜¯å¾—åˆ°5ä¸ªVectorï¼Œå°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„ X1_Vec ã€‚
2. å°† X1_Vec ä¸­çš„5ä¸ª Vector ä¾æ¬¡æŒ‰é¡ºåºæ”¾å…¥åˆ°RNNä¸­ï¼Œå¾—åˆ°ä¸€ä¸ªè¾“å‡º Z
   æ¯”å¦‚è¿™æ · ![2021-11-04_seq2seq_3]

## decoder

åœ¨ decoder ä¸­ï¼Œé¦–å…ˆæˆ‘ä»¬è¦å¯¹æ ‡ç­¾è¿›è¡Œç¼–ç ï¼Œç„¶åï¼Œå°†ç¼–ç åçš„ç»“æœæ”¾å…¥åˆ°ä¸€ä¸ªç¥ç»ç½‘ç»œä¸­ï¼Œç”¨æ¥æå–ç‰¹å¾ï¼Œ

## ä»£ç å®ç°

### æ•°æ®å‡†å¤‡

é¦–å…ˆæˆ‘ä»¬è¦å®‰è£…pytorch(1.0ä»¥ä¸Š)ï¼Œtorchtextï¼Œspacy

```python
import torch
import torch.nn as nn
import torch.optim as optim

from torchtext.legacy.datasets import Multi30k
from torchtext.legacy.data import Field, BucketIterator

import spacy
import numpy as np

import random
import math
import time

SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
```

æˆ‘ä»¬é€šè¿‡æ‰§è¡Œä¸€ä¸‹è„šæœ¬æ¥å®‰è£…æ•°æ®é›†

```shell
$ python -m spacy download en_core_web_sm
$ python -m spacy download de_core_news_sm
```

ç„¶ååŠ è½½æ•°æ®é›†

```python
spacy_de = spacy.load('de_core_news_sm')
spacy_en = spacy.load('en_core_web_sm')
```

æ¥ä¸‹æ¥å°±æ˜¯æˆ‘ä»¬çš„ç¼–ç é˜¶æ®µ
é¦–å…ˆæˆ‘ä»¬å°†è¾“å…¥çš„ä¸€ä¸²è¿ç»­çš„å­—ç¬¦è½¬æ¢æˆlistï¼Œå¦‚ 'æ—©ä¸Šå¥½' è½¬æ¢æˆ '[æ—©,ä¸Š,å¥½]' ï¼Œç„¶åå†å°†ä»–ä»¬å˜æˆ 0ï¼Œ1 ç¼–ç ï¼Œä»£ç å¦‚ä¸‹

```python
def tokenize_cn(text):
    """
    Tokenizes German text from a string into a list of strings (tokens) and reverses it
    å°†ä¸­æ–‡çš„ä¸€æ®µè¯è¿›è¡Œç¼–ç ï¼Œå°†æ¯ä¸ªæ±‰å­—éƒ½ç¼–ç æˆä¸€ä¸ªå­—ç¬¦ä¸²å¼çš„tokensã€‚ç„¶åå°†ä»–ä»¬åè½¬ï¼Œåè½¬æ˜¯ä¸ºäº†æ”¾å…¥RNNçš„æ—¶å€™ä¿è¯æœ€å…ˆæ”¾å…¥çš„æ˜¯ä¸€å¼€å§‹çš„å­—ç¬¦ï¼Œè€Œä¸æ˜¯æœ€åçš„å­—ç¬¦
    """
    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]

def tokenize_en(text):
    """
    Tokenizes English text from a string into a list of strings (tokens)
    è‹±æ–‡åŒä¸­æ–‡ä¸€æ ·
    """
    return [tok.text for tok in spacy_en.tokenizer(text)]
```

åœ¨ torchtext ä¸­å·²ç»æœ‰æ–¹æ³•å¸®æˆ‘ä»¬å®ç°ç¼–ç æ–¹æ³•ï¼Œæˆ‘ä»¬åªéœ€è¦è°ƒç”¨å¦‚ä¸‹æ–¹æ³•ï¼Œåˆ†åˆ«è¿›è¡Œä¸­æ–‡å’Œè‹±æ–‡çš„ç¼–ç 

```python
SRC = Field(tokenize = tokenize_de, 
            init_token = '<sos>', 
            eos_token = '<eos>', 
            lower = True)

TRG = Field(tokenize = tokenize_en, 
            init_token = '<sos>', 
            eos_token = '<eos>', 
            lower = True)
```

æ¥ä¸‹æ¥æˆ‘ä»¬åŠ è½½æ•°æ®é›†ï¼Œç„¶åè‡ªåŠ¨åˆ†ä¸ºè®­ç»ƒæ•°æ®ï¼ŒéªŒè¯æ•°æ®ï¼Œå’Œæµ‹è¯•æ•°æ®ï¼Œæœ¬æ•°æ®é›†ä½¿ç”¨çš„æ˜¯ [Multi30k dataset](https://github.com/multi30k/dataset) ï¼Œ
é‡Œé¢åŒ…å«æœ‰ 30000 æ¡è‹±æ–‡å¯¹æ³•æ–‡å’Œå¾·æ–‡çš„å¥å­ã€‚

```python
train_data, valid_data, test_data = Multi30k.splits(exts = ('.cn', '.en'),  fields = (SRC, TRG))
```

æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ¯ä¸ªæ•°æ®é›†çš„å¤§å°

```python
print(f"Number of training examples: {len(train_data.examples)}")
print(f"Number of validation examples: {len(valid_data.examples)}")
print(f"Number of testing examples: {len(test_data.examples)}")
```

æŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬æ•°æ®ï¼Œçœ‹çœ‹æ•°æ®é›†çš„æ ¼å¼æ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚

```python
print(vars(train_data.examples[0]))
```

æˆ‘ä»¬å¾—åˆ°çš„è¾“å‡ºæ˜¯è¿™æ ·å­çš„ï¼Œå‰é¢çš„ `src`ä¸­æ˜¯å¾·è¯­ï¼Œåé¢çš„ `trg`ä¸­æ˜¯è‹±è¯­ã€‚

```json5
{'src': ['.', 'bÃ¼sche', 'vieler', 'nÃ¤he', 'der', 'in', 'freien', 'im', 'sind', 'mÃ¤nner', 'weiÃŸe', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}
```

æ¥ä¸‹æ¥æˆ‘ä»¬å¯¹è®­ç»ƒé›†çš„è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œ vocabulary å¤„ç†ï¼Œ vocabulary å¤„ç†å…¶å®å°±æ˜¯æ‰¾å‡ºè¾“å…¥è¾“å‡ºä¸­æ‰€æœ‰çš„å•è¯ï¼Œç„¶åå»é‡ï¼Œç„¶åç»™å»é‡åçš„æ¯ä¸ªå•è¯æ’åºï¼Œå¾—åˆ°æ¯ä¸ªå•è¯çš„indexï¼Œè¿™ä¸ªindexå°±æ˜¯æ¯ä¸ªå•è¯çš„ç¼–ç ã€‚

æ‰§è¡Œ vocabulary å¤„ç†ä»£ç å¦‚ä¸‹

```python
# æ„å»ºè¯æ±‡ï¼Œæ„å»ºä¹‹åï¼ŒSRC å°±å¤šäº†ä¸ª vocab å±æ€§ï¼Œvocab ä¸­åŒ…å«æœ‰ freqsã€itosã€stoi ä¸‰ä¸ªå±æ€§ï¼Œå…¶ä¸­freqs è¡¨ç¤ºçš„æ˜¯ SRC ä¸­æ¯ä¸ªå•è¯å’Œè¯¥å•è¯çš„é¢‘æ•°ï¼Œä¹Ÿå°±æ˜¯ä¸ªæ•°ã€‚
# itos æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«çš„çš„æ˜¯é¢‘æ•° >= 2 çš„å•è¯ï¼Œstoi ç”¨æ¥æ ‡è®° itos ä¸­æ¯ä¸ªå•è¯çš„ç´¢å¼•ï¼Œä»0å¼€å§‹ã€‚
# ä¾‹å¦‚ è¾“å…¥æ˜¯['two', 'two', ',', 'two', 'two', 'are', 'outside', 'near', 'many', 'bushes', 'two', 'young', ',', 'white', 'white', 'near', 'outside', 'near', 'many', 'bushes', '.']
# åˆ™ freqs æ˜¯({'two': 5, 'near': 3, ',': 2, 'outside': 2, 'many': 2, 'bushes': 2, 'white': 2, 'are': 1, 'young': 1, '.': 1})
# itos æ˜¯ ['<unk>', '<pad>', '<sos>', '<eos>', 'two', 'near', ',', 'bushes', 'many', 'outside', 'white']
# å…¶ä¸­ <sos>ï¼šä¸€ä¸ªå¥å­çš„å¼€å§‹ï¼Œ<eos>ï¼šä¸€å¥è¯çš„ç»“æŸï¼Œ<UNK>: ä½é¢‘è¯æˆ–æœªåœ¨è¯è¡¨ä¸­çš„è¯ï¼Œæ¯”å¦‚æˆ‘ä»¬è®¾ç½® min_freq = 2 ï¼Œé‚£ä¹ˆé‚£äº›åªå‡ºç°äº† 1 æ¬¡çš„å•è¯ï¼Œå°†æ¥åœ¨æ”¾å…¥åˆ°ç¥ç»ç½‘ç»œä¹‹å‰éƒ½ä¼šè¢« <UNK> æ›¿æ¢æ‰
# <PAD>: è¡¥å…¨å­—ç¬¦ï¼Œç”±äºæˆ‘ä»¬åœ¨è¿›è¡Œæ‰¹é‡è®¡ç®—çš„æ—¶å€™ï¼Œæ¯ä¸ªæ ·æœ¬çš„é•¿åº¦ä¸ä¸€æ ·ï¼Œ<PAD>å°±æ˜¯ç”¨äºä¿è¯æ ·æœ¬é•¿åº¦ä¸€æ ·çš„ã€‚å‚è€ƒäº https://github.com/nicolas-ivanov/tf_seq2seq_chatbot/issues/15
# ç”±äºæˆ‘ä»¬çš„ min_freq = 2 ï¼Œæ‰€ä»¥å¯ä»¥çœ‹åˆ°é¢‘æ•°ä¸º1çš„ 'are'ï¼Œ'young'ï¼Œ '.' éƒ½æ²¡æœ‰åœ¨ itos ä¸­ã€‚
# stoi æ˜¯ {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3, 'two': 4, 'near': 5, ',': 6, 'bushes': 7, 'many': 8, 'outside': 9, 'white': 10})
SRC.build_vocab(train_data, min_freq = 2)
TRG.build_vocab(train_data, min_freq = 2)
# è¾“å‡ºä¸€ä¸‹ç»“æœ
print(f"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}")
print(f"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}")
```

æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹æˆ‘ä»¬çš„æ•°æ®åœ¨è¿›å…¥åˆ°ç¥ç»ç½‘ç»œä¹‹å‰ï¼Œéƒ½ç»å†äº†æ€æ ·çš„å¤„ç† ï¼š

- é¦–å…ˆæ˜¯æˆ‘ä»¬çš„åŸå§‹æ•°æ®ï¼Œå¦‚ä¸‹å›¾å±•ç¤ºã€‚
  
  ![input-batch]


- æ¥ä¸‹æ¥æ˜¯æˆ‘ä»¬çš„å¡«å……ï¼Œé¦–å…ˆåœ¨æ¯å¥è¯çš„å¼€å§‹å’Œç»“æŸåˆ†åˆ«åŠ ä¸Š'\<sos\>'å’Œ '\<eos\>' ï¼Œ ç„¶åå°†æ•´ä¸ª batch ä¸­çš„æ•°æ®å¯¹é½ï¼ŒæŒ‰ç…§æœ€é•¿çš„å¥å­å¯¹é½ï¼Œ
  ä¸å¤Ÿçš„ç”¨ '\<pad\>' æ¥å¡«å……ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
  
  ![padded-input-batch]
  

- æœ€åå°±æ˜¯å°†è¾“å…¥æ•°æ®è¿›è¡Œæ•°å­—åŒ–å¤„ç†ï¼Œå°†æ¯ä¸ªå•è¯åˆ†åˆ«è½¬æ¢æˆå®ƒæ‰€å¯¹åº”çš„ç´¢å¼•ï¼Œè¯¥ç´¢å¼•å°±æ˜¯ SRC.vocab stoi ä¸­çš„å€¼ ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
  
  ![input-numericalize]

åˆ°æ­¤ï¼Œæˆ‘ä»¬çš„æ•°æ®é¢„å¤„ç†å°±å®Œæˆäº†ã€‚

### Encoder

æ¥ä¸‹æ¥æˆ‘ä»¬æ„é€ æˆ‘ä»¬çš„ encoder æ¨¡å‹

åœ¨RNNç³»åˆ—ä¸­ï¼Œä¼ ç»Ÿçš„RNNå­˜åœ¨æ¯”è¾ƒå¤§çš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ï¼Œæ‰€ä»¥ç°åœ¨å¤§å®¶å¸¸å¸¸ç”¨LSTMæ¥ä»£æ›¿RNNï¼Œæœ¬æ–‡ä¹Ÿå°†ä½¿ç”¨ LSTM æ¥è¿›è¡Œç¼–ç ï¼Œåœ¨ [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) ä¸­æœ‰å¯¹ LSTM çš„è¯¦ç»†ä»‹ç»ã€‚
æˆ‘ä»¬å…ˆçœ‹çœ‹LSTMçš„ç»“æ„ï¼Œè¯¥ç»“æ„å›¾æ¥è‡ªäº[dive into deep learning](https://d2l.ai/chapter_recurrent-modern/lstm.html)
![lstm-struct]

æœ€ç»ˆçš„è¾“å‡ºå°±æ˜¯æŠŠ $$ H_t $$ åšä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œç›´æ¥å°† $$ H_t $$ å½“ä½œè¾“å‡ºä¹Ÿæ˜¯å¯ä»¥çš„ã€‚åœ¨ [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) è¿™ç¯‡æ–‡ç« ä¸­æœ‰å…³äº LSTM æ›´åŠ è¯¦ç»†çš„ä»‹ç»ã€‚

äºæ˜¯æˆ‘ä»¬çš„seq2seqæ¨¡å‹å°±å˜æˆäº†å¦‚ä¸‹ç»“æ„ï¼Œè¯¥å›¾æ¥è‡ªäº [nicolas-ivanov](https://github.com/nicolas-ivanov/tf_seq2seq_chatbot/issues/15)

![seq2seq-lstm]

åœ¨åŸè®ºæ–‡ä¸­ä½œè€…ä½¿ç”¨çš„æ˜¯4å±‚LSTMï¼Œæœ¬æ–‡æˆ‘ä»¬åªä½¿ç”¨2å±‚LSTMè¿›è¡Œè®­ç»ƒã€‚å…¶ä¸­æ¯å±‚ä¸­åˆåŒ…å«æœ‰å¤šä¸ªLSTMå•å…ƒï¼Œå…·ä½“æœ‰å¤šå°‘ä¸ªæ˜¯æ ¹æ®è¾“å…¥çš„é•¿åº¦å†³å®šçš„ã€‚LSTMæˆ‘ä»¬å¯ä»¥è¡¨ç¤ºæˆå¦‚ä¸‹è¡¨è¾¾å¼ï¼š

$$(h_t,c_t) = LSTM(e(x_t),h_{t-1},c_{t-1})$$

å…¶ä¸­ $$h_t$$ å’Œ $$c_t$$ åˆ†åˆ«è¡¨ç¤ºç¬¬ t ä¸ªLSTMçš„è¾“å‡ºä¸­çš„éšè—å•å…ƒå’Œè®°å¿†å•å…ƒï¼Œ$$x_t$$ è¡¨ç¤ºç¬¬ t ä¸ªè¾“å…¥ï¼Œ$$e(x_t)$$ è¡¨ç¤ºå°†ç¬¬ t ä¸ªè¾“å…¥è¿›è¡Œ one-hot å¤„ç†ã€‚$$h_(t-1),c_(t-1))$$ åˆ†åˆ«è¡¨ç¤ºä¸Šä¸€å±‚çš„è¾“å‡ºä¸­çš„éšè—å•å…ƒå’Œè®°å¿†å•å…ƒã€‚
åœ¨ç†è§£ä¸Šæˆ‘ä»¬å¯ä»¥æŠŠ $$h_t$$ å’Œ $$c_t$$ éƒ½å½“æˆéšè—å•å…ƒï¼Œåªä¸è¿‡è®¡ç®—æ–¹å¼ä¸ä¸€æ ·ã€‚**å…¶ä¸­ $$h_0$$ å’Œ $$c_0$$ ï¼Œæ˜¯åˆå§‹åŒ–éšæœºç”Ÿæˆçš„** ã€‚

$$z^i = (h_l^i,c_l^i)$$

æˆ‘ä»¬ä»¤ $$z^1$$ , $$z^2$$ åˆ†åˆ«ä¸ºæ¯ä¸ªéšè—å•å…ƒçš„è¾“å‡ºã€‚$$z^i$$ è¡¨ç¤ºç¬¬ i å±‚çš„è¾“å‡ºã€‚$$h_l^i$$ å’Œ $$c_l^i$$ è¡¨ç¤ºç¬¬ i å±‚çš„æœ€åä¸€ä¸ªLSTMå•å…ƒçš„éšè—å•å…ƒçš„è¾“å‡ºå’Œè®°å¿†å•å…ƒçš„è¾“å‡ºã€‚

ä¸‹å›¾æ˜¯ä¸€ä¸ªLSTMç¼–ç çš„ä¾‹å­ã€‚å…¶ä¸­é»„è‰²æ–¹å—è¡¨ç¤ºå¯¹è¾“å…¥è¿›è¡Œ one-hot å¤„ç†ï¼Œæœ‰2å±‚ç»¿è‰²æ–¹å—ï¼Œè¡¨ç¤ºæœ‰ä¸¤å±‚LSTMç½‘ç»œï¼Œæ¯ä¸ªç»¿è‰²æ–¹å—éƒ½è¡¨ç¤ºä¸€ä¸ªLSTMå•å…ƒï¼Œçº¢è‰²æ–¹å—è¡¨ç¤ºæ¯å±‚çš„è¾“å‡ºã€‚

![seq2seq2-encoder]

åœ¨ PyTorch ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout) æ¥åˆ›å»ºä¸€ä¸ªLSTMç½‘ç»œï¼Œå…¶ä¸­ ï¼š

* emb_dimï¼šè¾“å…¥çš„ç»´åº¦ï¼Œ ä¸æ˜¯æŒ‡ä¸€å¥è¯çš„é•¿åº¦ï¼Œè€Œæ˜¯æ¯ä¸ªå•è¯ one-hot ä¹‹åçš„å‘é‡çš„é•¿åº¦ã€‚
* hid_dimï¼šéšè—å•å…ƒçš„ç»´åº¦ã€‚
* n_layersï¼šç½‘ç»œçš„å±‚æ•°ï¼Œä¹Ÿæ˜¯æ·±åº¦ã€‚
* dropoutï¼šæ¯ä¸€å±‚çš„ dropoutã€‚


**Note:** éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨LSTMä¸­ï¼Œå¦‚æœæˆ‘ä»¬çš„è¾“å…¥çš„ç»´åº¦åªæœ‰1ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ä¸èƒ½ç›´æ¥ä½¿ç”¨ nn.LSTMï¼Œè€Œæ˜¯ä½¿ç”¨ nn.LSTMCellï¼Œå› ä¸ºå¦‚æœç›´æ¥ä½¿ç”¨ nn.LSTM ä¼šæœ‰ç»´åº¦è½¬æ¢çš„é—®é¢˜ã€‚

ä»£ç å¦‚ä¸‹ï¼š

````python

class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(input_dim, emb_dim)
        
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src):
        
        #src = [src len, batch size]
        
        embedded = self.dropout(self.embedding(src))
        
        #embedded = [src len, batch size, emb dim]
        
        outputs, (hidden, cell) = self.rnn(embedded)
        
        #outputs = [src len, batch size, hid dim * n directions]
        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]
        
        #outputs are always from the top hidden layer
        
        return hidden, cell

````

### Decoder

æˆ‘ä»¬å†æ¥æ„é€ æˆ‘ä»¬çš„ decoder æ¨¡å‹

decoder æ¨¡å‹æˆ‘ä»¬ä¹Ÿæ˜¯ä½¿ç”¨2å±‚ LSTM ï¼ŒåŸè®ºæ–‡æ˜¯4å±‚ã€‚ ç½‘ç»œç»“æ„è·Ÿ encoder éå¸¸ç›¸ä¼¼ï¼Œæ˜¯ä¸è¿‡è¿™é‡Œçš„ $$h_0$$ å’Œ $$c_0$$ å˜æˆäº† encoder ä¸­çš„ $$z^1$$ , $$z^2$$ ã€‚

ä¸‹å›¾å±•ç¤ºçš„æ˜¯æˆ‘ä»¬çš„decoderçš„ç»“æ„å›¾

![seq2seq2-decoder]

åœ¨æœ€åæˆ‘ä»¬å°†è¾“å‡ºä¼ å…¥åˆ°ä¸€ä¸ªå…¨è¿æ¥ç½‘ç»œä¸­ï¼Œå¾—åˆ°æˆ‘ä»¬çš„è¾“å‡ºã€‚


æ¥ä¸‹æ¥æ˜¯ decoder çš„ä»£ç å®ç°ï¼š

````python

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        
        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)
        
        self.fc_out = nn.Linear(hid_dim, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden, cell):
        
        #input = [batch size]
        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]
        
        #n directions in the decoder will both always be 1, therefore:
        #hidden = [n layers, batch size, hid dim]
        #context = [n layers, batch size, hid dim]
        
        input = input.unsqueeze(0)
        
        #input = [1, batch size]
        
        embedded = self.dropout(self.embedding(input))
        
        #embedded = [1, batch size, emb dim]
                
        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        
        #output = [seq len, batch size, hid dim * n directions]
        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]
        
        #seq len and n directions will always be 1 in the decoder, therefore:
        #output = [1, batch size, hid dim]
        #hidden = [n layers, batch size, hid dim]
        #cell = [n layers, batch size, hid dim]
        
        prediction = self.fc_out(output.squeeze(0))
        
        #prediction = [batch size, output dim]
        
        return prediction, hidden, cell

````

### Seq2Seq

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„ Seq2Seq æ•´ä½“çš„ç»“æ„ï¼Œåœ¨æˆ‘ä»¬çš„ Seq2Seq æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å°† Seq2Seq åˆ†æˆä¸‰éƒ¨åˆ†ï¼Œåˆ†åˆ«æ˜¯ï¼š
1. æ¥æ”¶è¾“å…¥æ•°æ®å’Œç›®æ ‡æ•°æ®ï¼Œå¹¶å°†ä»–ä»¬è¿›è¡Œé¢„å¤„ç†
- å°†è¾“å…¥æ•°æ®è¿›è¡Œ encoder ï¼Œå¾—åˆ°è¾“å…¥çš„ç‰¹å¾ã€‚
- å°†ç›®æ ‡æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒæ—¶å°† encoder ä¸­å¾—åˆ°çš„è¾“å…¥ç‰¹å¾ä½œä¸º LSTM çš„åˆè¯•å€¼ï¼Œä¸€èµ·æ”¾å…¥åˆ° decoder ç½‘ç»œä¸­ï¼Œå¾—åˆ°è¾“å‡ºã€‚

æ•´ä½“ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![Seq2Seq-model]

åœ¨ Seq2Seq æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å°† encoder çš„å±‚æ•°ä¸ decoder çš„å±‚æ•°è®¾ç½®ä¸ºä¸€æ ·ï¼Œè¿™ä¸æ˜¯å¿…é¡»çš„ï¼Œä½†æ˜¯è¿™æ ·åšèƒ½æ–¹ä¾¿æˆ‘ä»¬å¤„ç†æ¨¡å‹ã€‚

ä»£ç å¦‚ä¸‹ï¼š

```python
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must be equal!"
        assert encoder.n_layers == decoder.n_layers, \
            "Encoder and decoder must have equal number of layers!"
        
    def forward(self, src, trg, teacher_forcing_ratio = 0.5):
        
        #src = [src len, batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing
        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time
        
        batch_size = trg.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        #tensor to store decoder outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        #last hidden state of the encoder is used as the initial hidden state of the decoder
        hidden, cell = self.encoder(src)
        
        #first input to the decoder is the <sos> tokens
        input = trg[0,:]
        
        for t in range(1, trg_len):
            
            #insert input token embedding, previous hidden and previous cell states
            #receive output tensor (predictions) and new hidden and cell states
            output, hidden, cell = self.decoder(input, hidden, cell)
            
            #place predictions in a tensor holding predictions for each token
            outputs[t] = output
            
            #decide if we are going to use teacher forcing or not
            teacher_force = random.random() < teacher_forcing_ratio
            
            #get the highest predicted token from our predictions
            top1 = output.argmax(1) 
            
            #if teacher forcing, use actual next token as next input
            #if not, use predicted token
            # åˆ¤æ–­ä¸‹ä¸€ä¸ªçš„è¾“å…¥ï¼Œæ˜¯ä½¿ç”¨è®­ç»ƒé›†ä¸­çš„è¿˜æ˜¯ä½¿ç”¨ä» decoder ä¸­é¢„æµ‹çš„
            input = trg[t] if teacher_force else top1
        
        return outputs
```
ä»ä»£ç ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬å…ˆæ˜¯å°†æ•´ä¸ªæºæ•°æ®æ”¾å…¥ encoder ä¸­ï¼ˆæºæ•°æ®æŒ‡çš„æ˜¯è®­ç»ƒæ•°æ®ä¸­çš„ src æ•°æ®ï¼‰ï¼Œç„¶åæˆ‘ä»¬ä¸€ä¸ªä¸€ä¸ªçš„éå†ç›®æ ‡æ•°æ®ï¼Œ
é¦–å…ˆæˆ‘ä»¬å°† '\<sos\>' æ”¾å…¥åˆ° decoder ä¸­ï¼Œå¾—åˆ°ä¸€ä¸ªè¾“å‡ºï¼Œä¿å­˜åˆ°è¾“å‡ºçš„ç»“æœä¸­ã€‚ç„¶åæˆ‘ä»¬ä»¥ä¸€å®šçš„æ¦‚ç‡æ¥åˆ¤æ–­æ˜¯å¦è¦ä½¿ç”¨ç›®æ ‡æ•°æ®ä¸­çš„ä¸‹ä¸€ä¸ªå½“ä½œè¾“å…¥ï¼Œ
ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬çš„decoderçš„è¾“å…¥ä¸ä¸€å®šå…¨æ˜¯ç›®æ ‡æ•°æ®ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„è®­ç»ƒä»£ç 

```python

INPUT_DIM = len(SRC.vocab)
OUTPUT_DIM = len(TRG.vocab)
ENC_EMB_DIM = 256
DEC_EMB_DIM = 256
HID_DIM = 512
N_LAYERS = 2
ENC_DROPOUT = 0.5
DEC_DROPOUT = 0.5

enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)

model = Seq2Seq(enc, dec, device).to(device)


def init_weights(m):
    for name, param in m.named_parameters():
        nn.init.uniform_(param.data, -0.08, 0.08)


model.apply(init_weights)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


print(f'The model has {count_parameters(model):,} trainable parameters')

optimizer = optim.Adam(model.parameters())

TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)


def train(model, iterator, optimizer, criterion, clip):
    model.train()

    epoch_loss = 0

    for i, batch in enumerate(iterator):
        src = batch.src
        trg = batch.trg

        optimizer.zero_grad()

        output = model(src, trg)

        # trg = [trg len, batch size]
        # output = [trg len, batch size, output dim]

        output_dim = output.shape[-1]

        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)

        # trg = [(trg len - 1) * batch size]
        # output = [(trg len - 1) * batch size, output dim]

        loss = criterion(output, trg)

        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

        optimizer.step()

        epoch_loss += loss.item()

    return epoch_loss / len(iterator)


```
 
æˆ‘ä»¬ä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼ŒCrossEntropyLoss æŸå¤±å‡½æ•°ï¼Œåœ¨ CrossEntropyLoss ä¸­æˆ‘ä»¬ä¸è®¡ç®—ä¸ºäº†ä¿æŒbatchä¸­æ ·æœ¬é•¿åº¦ä¸€è‡´è€Œå¡«å……çš„éƒ¨åˆ†ã€‚
è®­ç»ƒä¸­è¿˜ä½¿ç”¨äº† torch.nn.utils.clip_grad_norm_ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æŒ‡çš„æ˜¯å°†æ¯ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œåå‘ä¼ æ’­æ—¶å€™çš„æ¢¯åº¦è¿›è¡Œå½’ä¸€åŒ–å¹¶é™åˆ¶ä½gradï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±ã€‚

æ¥ä¸‹æ¥å°±æ˜¯å¼€å§‹æˆ‘ä»¬çš„è®­ç»ƒäº†ã€‚

è®­ç»ƒäº†20ä¸ªepocheä¹‹åï¼Œæˆ‘ä»¬æ¥è¿›è¡Œæµ‹è¯•ä»¥ä¸‹

### æµ‹è¯•

æµ‹è¯•è¿‡ç¨‹ä¸ºï¼š
- æˆ‘ä»¬å°†æˆ‘ä»¬çš„è¦ç¿»è¯‘çš„æºæ•°æ®å…ˆè¿›è¡Œencoderè®¡ç®—ï¼Œå¾—åˆ° hidden, cell ï¼Œ
- ç„¶åæˆ‘ä»¬å°†å¼€å§‹æè¿°ç¬¦ `\<sos\>` å½“ä½œè¾“å…¥ï¼Œè·Ÿ hidden, cell ä¸€èµ·æ”¾å…¥åˆ°decoderä¸­å»ï¼Œè¿™ä¸ªæ—¶å€™å¾—åˆ°ä¸€ä¸ªè¾“å‡ºå’Œæ–°çš„ hidden, cell ã€‚ 
- æˆ‘ä»¬å°†è¾“å‡ºä¿å­˜ä¸‹æ¥ï¼Œç„¶åå°†è¯¥è¾“å‡ºä¸æ–°çš„ hidden, cell ä¸€èµ·å½“ä½œè¾“å…¥æ”¾å…¥åˆ°decoderä¸­å»ï¼Œå¦‚æ­¤å¾ªç¯ï¼Œå°±å¯ä»¥å¾—åˆ°æˆ‘ä»¬çš„ç¿»è¯‘è¯­å¥äº†ã€‚

æœ€ç»ˆç»“æœï¼š`| Test Loss: 3.943 | Test PPL:  51.571 |`

### GRU çš„ç®€å•ä»‹ç»

åœ¨ [2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb) ä¸­
ä½œè€…è¿˜åšäº†ä½¿ç”¨ GRU (Gated Recurrent Unit) æ¥æ›¿ä»£ LSTM è¿›è¡Œæœºå™¨ç¿»è¯‘çš„è®­ç»ƒï¼Œä½¿ç”¨äº†æ›´å¤šçš„å‚æ•°ï¼Œæœ‰æ›´å¥½çš„æ•ˆæœã€‚ä¸è¿‡å…¶å®æœ‰äººåšè¿‡å®éªŒï¼Œå‘ç°å…¶å® GRU ä¸ LSTM æ€§èƒ½å‡ ä¹æ˜¯å·®ä¸å¤šçš„ [è®ºæ–‡é“¾æ¥](https://arxiv.org/abs/1412.3555) åœ¨æ­¤ã€‚

ä¸‹å›¾æ˜¯ GRU çš„encoderç»“æ„å›¾ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯ä¸€å±‚ç½‘ç»œ

![gru-encoder]

ä»å›¾ä¸Šæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå‡ ä¹ä¸LSTMä¸€æ ·ï¼Œéœ€è¦æé†’çš„æ˜¯æ¯ä¸€ä¸ªç»¿è‰²çš„æ–¹å—éƒ½ä»£è¡¨ä¸€æ¬¡GRUæ“ä½œï¼Œæ¯æ¬¡GRUéƒ½æ˜¯ä¸€æ ·çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸Šå›¾æ˜¯ä¸€ä¸ªå•å±‚ï¼Œå•ä¸ªGRUçš„æ¨¡å‹ï¼Œ
å³ '\<sos\>' å…ˆè¿›å…¥ GRUï¼Œè¿ç®—åå¾—åˆ°è¾“å‡ºï¼Œå† guten è¿›å…¥ GRUï¼Œè¿˜æ˜¯ä¹‹å‰çš„é‚£ä¸ª GRUï¼Œåªæ˜¯è¾“å…¥å‚æ•°ä¸ä¸€æ ·äº†ï¼ŒGRU é‡Œé¢çš„å‚æ•°æ­¤æ—¶æ˜¯ä¸€æ ·çš„ã€‚

ä¸‹å›¾æ˜¯ GRU çš„decoderç»“æ„å›¾ï¼Œå…¶å®è·Ÿ LSTM å¾ˆç›¸ä¼¼

![gru-decoder]

å…¶ä¸­ç´«è‰²çš„æ–¹å—è¡¨ç¤ºå…¨è¿æ¥ã€‚decoderä¸ä¹‹å‰çš„LSTMç»“æ„çš„decoderçš„è¿ç»“æ–¹å¼ä¸ä¸€æ ·ï¼Œåœ¨ GRU ä¸­ encoder çš„è¾“å‡ºä¼šè¢«ä½¿ç”¨åˆ°æ¯ä¸€ä¸ª decoder çš„èŠ‚ç‚¹ä¸­æ¥ã€‚

ä¸‹å›¾æ˜¯ä½¿ç”¨ GRU æ¨¡å‹çš„seq2seqæ¨¡å‹

![seq2seq-with-gru]

æ•´ä¸ªç»“æ„ä¸ä½¿ç”¨ LSTM ç»“æ„çš„seq2seq æ¨¡å‹å¹¶æ— å¤ªå¤§çš„å·®åˆ«ï¼Œæ­¤å¤„å°±ä¸è¿‡å¤šä»‹ç»ã€‚åœ¨ [2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb) ä¸­æœ‰è¯¦ç»†çš„ä»£ç å®ç°ã€‚


ğŸ‘‰ï¸ ğŸ‘‰ï¸ ğŸ‘‰ï¸ ç‚¹å‡»[ ğŸ’ ğŸ’ ğŸ’ å¯ä»¥ç›´æ¥ä¸‹è½½ä½¿ç”¨ LSTM ç»“æ„çš„seq2seq æ¨¡å‹çš„ä»£ç ](https://7568.github.io/codes/text-process/2021-11-03-seq2seqModel-lstm.py)ã€‚å°†ä»£ç ä¸­ `is_train = False` æ”¹æˆ `is_train = True` å°±å¯ä»¥è®­ç»ƒäº†ï¼Œæµ‹è¯•çš„æ—¶å€™å†æ”¹å›æ¥å³å¯ã€‚
ğŸ‘‰ï¸ ğŸ‘‰ï¸ ğŸ‘‰ï¸ ç‚¹å‡»[ ğŸ’ ğŸ’ ğŸ’ å¯ä»¥ç›´æ¥ä¸‹è½½ä½¿ç”¨ GRU ç»“æ„çš„seq2seq æ¨¡å‹çš„ä»£ç ](https://7568.github.io/codes/text-process/2021-11-03-seq2seqModel-gru.py)ã€‚å°†ä»£ç ä¸­ `is_train = False` æ”¹æˆ `is_train = True` å°±å¯ä»¥è®­ç»ƒäº†ï¼Œæµ‹è¯•çš„æ—¶å€™å†æ”¹å›æ¥å³å¯ã€‚
ğŸ‘‰ï¸ ğŸ‘‰ï¸ ğŸ‘‰ï¸ ç‚¹å‡»[ ğŸ’ ğŸ’ ğŸ’ å¯ä»¥ç›´æ¥ä¸‹è½½ä½¿ç”¨ GRU ç»“æ„çš„seq2seq æ¨¡å‹çš„ä»£ç ](https://7568.github.io/codes/text-process/2021-11-03-seq2seqModel-gru.py)ã€‚å°†ä»£ç ä¸­ `is_train = False` æ”¹æˆ `is_train = True` å°±å¯ä»¥è®­ç»ƒäº†ï¼Œæµ‹è¯•çš„æ—¶å€™å†æ”¹å›æ¥å³å¯ã€‚

# Align ä»‹ç»

åœ¨å‰é¢æˆ‘ä»¬ä»‹ç»äº† LSTM å’Œ GRU æ¨¡å‹ï¼Œä»–ä»¬åœ¨å¤„ç†è¾“å…¥çš„æ—¶å€™ï¼Œéƒ½æ˜¯å°†ä¸€å¥è¯ä»å¤´åˆ°å°¾éƒ½ç»è¿‡ä¸€æ¬¡ç¥ç»ç½‘ç»œï¼Œåœ¨ [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) è®ºæ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†åŒå‘çš„ç½‘ç»œæ¨¡å‹ï¼Œå°±æ˜¯è¯´è®©æˆ‘ä»¬çš„è¾“å…¥å…ˆä»å¤´åˆ°å°¾è¿›å…¥ä¸€ä¸ªç½‘ç»œï¼Œç„¶åå†ä»å°¾åˆ°å¤´ç»è¿‡å¦ä¸€ä¸ªç½‘ç»œï¼Œå³åŒå‘ RNN ï¼Œè¿™æ ·æˆ‘ä»¬å°±æœ‰äº†ä¸¤ä¸ªè¾“å‡ºã€‚
å…·ä½“ç»“æ„å¦‚ä¸‹ï¼š

![bidirectional-rnn]

å¯ä»¥ç”¨æ•°å­¦è¡¨è¾¾ä¸ºï¼š

$$h_t^{\to} = EncoderGRU^{\to}(e(x_t^{\to}) , h_{t-1}^{\to})$$

$$h_t^{\gets} = EncoderGRU^{\gets}(e(x_t^{\gets}) , h_{t-1}^{\gets})$$

å…¶ä¸­ $$x_0^{\to}=<sos> , x_1^{\to}=guten$$ ï¼Œ$$x_0{^\gets}=<eos> , x_1^{\gets}=morgen$$ 

# Attention ä»‹ç»

Attention ä¹Ÿå«æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŸç†å°±æ˜¯æ¥å—è¾“å…¥ï¼Œç„¶åè¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œè¯¥å‘é‡æ‰€æœ‰çš„å€¼éƒ½æ˜¯[0ï¼Œ1]ä¹‹é—´çš„æ•°ï¼Œå‘é‡çš„å’Œä¸º1ï¼Œé€šå¸¸çš„åšæ³•æ˜¯é€šè¿‡æœ€åä¸€å±‚ç½‘ç»œåï¼ŒåŠ ä¸Šä¸€ä¸ªSoftMaxæ¿€æ´»å‡½æ•°ã€‚

ä¸‹å›¾æ˜¯ä¸€ä¸ªRNNä¸­è®¡ç®— Attention çš„ä¸€ç§æ–¹å¼ï¼š

![rnn-attention-encoder]

ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œzä¸º rnn çš„è¾“å‡ºï¼Œ$$h_1 $$ ~ $$ h_4$$ä¸ºæ¯ä¸ªè¾“å…¥çš„éšè—å•å…ƒï¼Œæˆ‘ä»¬å°† z ä¸ $$h_1 $$ ~ $$ h_4$$ ä¸€èµ·æ”¾è¿›ä¸€ä¸ªç¥ç»ç½‘ç»œä¸­ï¼Œå¾—åˆ° Attention aï¼Œè¯¥ç¥ç»ç½‘ç»œé€šå¸¸é€‰æ‹©ä¸ºå…¨è¿æ¥ã€‚

ä¸‹å›¾æ˜¯ decoderéƒ¨åˆ†ï¼Œ

![rnn-attention-arcitecture]

åœ¨æˆ‘ä»¬æ²¡æœ‰ Attention æœºåˆ¶çš„ RNN ä¸­ï¼Œencoder çš„è¾“å‡º z æ˜¯è¦å‚ä¸åˆ° decoder çš„æ‰€æœ‰æ“ä½œä¸­çš„ã€‚è€Œåœ¨å¸¦æœ‰ Attention æœºåˆ¶çš„ RNN ä¸­ï¼Œz æ­¤æ—¶åªæ˜¯å‚ä¸åˆ° GRU ä¸­å½“ä½œè¾“å…¥ï¼Œ
æ­¤æ—¶è¡¨ç¤º Attention çš„ a å‚ä¸åˆ° decoder çš„æ‰€æœ‰æ“ä½œä¸­çš„ï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„ GRUï¼Œå¸¦ Attention æœºåˆ¶çš„ RNN èƒ½å¤Ÿæºå¸¦ä¸Šåœ¨ encoder ä¸­çš„æ¯ä¸€æ¬¡è®¡ç®—çš„éšè—å•å…ƒ hï¼Œèƒ½å¤ŸæŠŠèƒ½å¤šçš„ä¿¡æ¯ä¼ é€’åˆ° decoder ä¸­ã€‚

åœ¨ decoder ä¸­æˆ‘ä»¬éœ€è¦å…³æ³¨ä¸¤ä¸ªåœ°æ–¹ï¼Œ1ï¼šæ˜¯ w æ˜¯å¦‚ä½•å‚ä¸è“è‰²æ–¹æ¡†çš„è¿ç®—ï¼Œ2ï¼šw æ˜¯å¦‚ä½•å‚ä¸åˆ°ç´«è‰²æ–¹æ¡†çš„è¿ç®—ã€‚ä»ä»£ç ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ

åœ¨ [Neural Machine Translation by Jointly Learning to Align and Translate](https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb) ä¸­æœ‰å®Œæ•´çš„ Align å’Œ Attention çš„å®ç°

æš‚æ—¶å®Œç»“ âœ¨â­ âœ¨â­ âœ¨â­ ã€‚

æ›´å¤šå‚è€ƒèµ„æ–™æ¥è‡ªäº
- [Towards Data Science - Attention â€” Seq2Seq Models](https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263)
- [Sequence to Sequence Learning with Neural Networks](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)
-[Jay Alammar Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)



