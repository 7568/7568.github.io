---
layout: blog
text-process: true
mathjax: true
background-image: http://7568.github.io/images/2021-11-03-transformer/img.png
category: æ–‡æœ¬å¤„ç†
title: æœºå™¨ç¿»è¯‘ - Transformer
tags:
- Transformer
- æ–‡æœ¬å¤„ç†
---

[transformer-architecture]:http://7568.github.io/images/2021-11-03-transformer/transformer-architecture.png
[a-high-level-look]:http://7568.github.io/images/2021-11-03-transformer/a-high-level-look.png
[a-high-level-look-1]:http://7568.github.io/images/2021-11-03-transformer/a-high-level-look-1.png
[a-high-level-look-2]:http://7568.github.io/images/2021-11-03-transformer/a-high-level-look-2.png
[a-high-level-look-3]:http://7568.github.io/images/2021-11-03-transformer/a-high-level-look-3.png
[a-high-level-look-4]:http://7568.github.io/images/2021-11-03-transformer/a-high-level-look-4.png
[word-embedding]:http://7568.github.io/images/2021-11-03-transformer/word-embedding.png
[encoder-process]:http://7568.github.io/images/2021-11-03-transformer/encoder-process.png
[self-attention-process]:http://7568.github.io/images/2021-11-03-transformer/self-attention-process.png
[self-attention-process-2]:http://7568.github.io/images/2021-11-03-transformer/self-attention-process-2.png
[matrix-calculation-of-self-attention]:http://7568.github.io/images/2021-11-03-transformer/matrix-calculation-of-self-attention.png
[matrix-calculation-of-self-attention-2]:http://7568.github.io/images/2021-11-03-transformer/matrix-calculation-of-self-attention-2.png
[multi-headed-attention]:http://7568.github.io/images/2021-11-03-transformer/multi-headed-attention.png
[multi-headed-attention-2]:http://7568.github.io/images/2021-11-03-transformer/multi-headed-attention-2.png
[multi-headed-attention-3]:http://7568.github.io/images/2021-11-03-transformer/multi-headed-attention-3.png
[self-attention-result]:http://7568.github.io/images/2021-11-03-transformer/self-attention-result.png
[self-attention-result-2]:http://7568.github.io/images/2021-11-03-transformer/self-attention-result-2.png
[position-vector]:http://7568.github.io/images/2021-11-03-transformer/position-vector.png
[position-vector-2]:http://7568.github.io/images/2021-11-03-transformer/position-vector-2.png
[position-vector-3]:http://7568.github.io/images/2021-11-03-transformer/position-vector-3.png
[position-vector-4]:http://7568.github.io/images/2021-11-03-transformer/position-vector-4.png
[encoder-architecture]:http://7568.github.io/images/2021-11-03-transformer/encoder-architecture.png
[encoder-architecture-2]:http://7568.github.io/images/2021-11-03-transformer/encoder-architecture-2.png

# ç®€ä»‹
åœ¨æ–‡æœ¬å¤„ç†ä¸­æœ‰ä¸¤ä¸ªç»å…¸çš„ç½‘ç»œæ¨¡å‹ï¼Œä¸€ä¸ªæ˜¯åŸºäºå¾ªç¯ç¥ç»ç½‘ç»œåŠ ä¸Š attention çš„ Seq2Seq å’Œå®Œå…¨åŸºäº attention çš„ Transformerã€‚è¿™ä¸¤ä¸ªæ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä¸­éƒ½å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚
æœ¬æ–‡ä¸­å¾ˆå¤§ä¸€éƒ¨åˆ†å†…å®¹æ¥è‡ªç¿»è¯‘
[jalammar ï¼š The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
ï¼Œ
[harvard ï¼š The Illustrated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) 
å’Œ
[bentrevett ï¼š Attention is All You Need](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb) ã€‚

æˆ‘ä»¬åœ¨ä¸Šä¸€ç¯‡[æ–‡ç« ](https://7568.github.io/2021/11/03/seq2seqModel.html) ä¸­è®²è¿°äº† Seq2Seq with attentionï¼Œä¹Ÿå°±æ˜¯ [Seq2seq Models With Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) ä¸­çš„å†…å®¹ã€‚

Transformer è®ºæ–‡åœ°å€åœ¨ [Attention is All You Need.](https://arxiv.org/abs/1706.03762) ã€‚

# æ¨¡å‹ç»“æ„

é¦–å…ˆ Transformer è¿˜æ˜¯ç»å…¸çš„ encoder ï¼Œdecoder æ¨¡å‹ï¼Œä¸ä¸€æ ·çš„åœ°æ–¹åœ¨äº Transformer æ²¡æœ‰ä½¿ç”¨ rnn å’Œ cnn è€Œæ˜¯ä½¿ç”¨ä¸€ç§å« self-attention çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç›¸å¯¹äº
rnnçš„ä¼˜åŠ¿æ˜¯ self-attention å¯ä»¥å¹¶è¡Œè¿ç®—ï¼Œä»è€Œä½¿å¾—å¤§è§„æ¨¡è®¡ç®—å¾—ä»¥è¿›è¡Œã€‚ä¸å†æ˜¯åé¢çš„å•è¯éœ€è¦ç­‰å‰é¢çš„å•è¯è¿è¡Œå®Œï¼Œå¾—åˆ°å‰ä¸€ä¸ªå•è¯çš„ hidden ä¹‹åï¼Œå†è¿›è¡Œåé¢çš„è¿ç®—ã€‚ç›¸å¯¹äº cnn çš„ä¼˜åŠ¿æ˜¯å®ƒæ˜¯å¯è§£é‡Šçš„ï¼Œ
èƒ½å¤Ÿç›´è§‚çš„çœ‹åˆ°ç¿»è¯‘ç»“æœæ˜¯ç”±å“ªäº›å› ç´ å†³å®šçš„ã€‚

Transformer æ•´ä½“ç»“æ„å¦‚ä¸‹ï¼š

![transformer-architecture]

æˆ‘ä»¬ä»é«˜å±‚æ¬¡æ¥çœ‹è¯¥æ¨¡å‹çš„åŒ–å°±æ˜¯è¿™æ ·ï¼Œä¸€ä¸ªè¾“å…¥ï¼Œä¸€ä¸ªé»‘ç›’ï¼Œä¸€ä¸ªè¾“å‡º

![a-high-level-look]

å½“æˆ‘ä»¬æ‹†å¼€é»‘ç›’ï¼Œå°±ä¼šå‘ç°é‡Œé¢åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼Œåˆ†åˆ«æ˜¯ encoders å’Œ decoders

![a-high-level-look-1]

å½“æˆ‘ä»¬ç»§ç»­æ¢ç©¶é»‘ç›’ï¼Œé‡Œé¢çš„ encoders å’Œ decoders ï¼Œæˆ‘ä»¬å°±ä¼šå‘ç°ï¼Œæ¯ä¸€ä¸ª encoders é‡Œé¢åˆåŒ…å«æœ‰8ä¸ª encoderï¼Œdecoders é‡Œé¢ä¹ŸåŒ…å«æœ‰8ä¸ª decoderã€‚

![a-high-level-look-2]

å½“æˆ‘ä»¬ç»§ç»­æ¢ç©¶æ¯ä¸€ä¸ª encoder ï¼Œå°±å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸€ä¸ª encoder éƒ½æœ‰ç›¸åŒçš„ç»“æ„ï¼Œéƒ½æ˜¯ç”±ä¸¤éƒ¨åˆ†æ„æˆï¼Œåˆ†åˆ«æ˜¯ feed-forward neural network å’Œ self-attention ã€‚

![a-high-level-look-3]

ç„¶åæˆ‘ä»¬æŸ¥çœ‹ decoderï¼Œå¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸€ä¸ª decoder ä¹Ÿæ˜¯éƒ½åŒ…å«ç›¸åŒçš„ç»“æ„ï¼Œéƒ½æ˜¯ç”±ä¸‰éƒ¨åˆ†æ„æˆï¼Œåˆ†åˆ«æ˜¯ feed-forward neural networkï¼ŒEncoder-Decoder Attentionï¼Œå’Œself-attention ã€‚

![a-high-level-look-4]

# è¿è¡Œè¿‡ç¨‹

ç°åœ¨æˆ‘ä»¬å¤§æ¦‚äº†è§£äº† transformer çš„æ•´ä½“ç»“æ„ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹ä¸€çœ‹ä¸€ä¸ªå¥å­æ˜¯å¦‚ä½•ä»è¾“å…¥ä¸€æ­¥ä¸€æ­¥åˆ°è¾“å‡ºï¼Œä»è€Œç ”ç©¶ ecnoder å’Œdecoderä¸­çš„å„ä¸ªæ¨¡å—æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

é¦–å…ˆä¸å¸¸è§„çš„NPLå¤„ç†ä¸€æ ·ï¼Œæˆ‘ä»¬çš„è¾“å…¥éƒ½è¦ç»è¿‡ embedding å¤„ç†ï¼Œå°†è¾“å…¥çš„æ¯ä¸ªå•è¯å˜æˆå‘é‡ã€‚å¦‚ä¸‹å›¾æ‰€ç¤º

![word-embedding]

ç„¶åå†æ”¾å…¥åˆ° encoder é‡Œé¢ï¼Œåœ¨ä¸€ä¸ª encoder é‡Œé¢å¤„ç†çš„æµç¨‹å¦‚ä¸‹ï¼š

![encoder-process]

æ¥ä¸‹æ¥æˆ‘ä»¬å°±æ¥è§£é‡Šä¸åŒçš„å•è¯æ˜¯å¦‚ä½•åœ¨ self-attention ä¸­è¢«å¤„ç†ï¼Œå¾—åˆ°è¾“å‡ºçš„ã€‚

## self-attention ä»‹ç»

é¦–å…ˆå‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªå•è¯ï¼Œåˆ†åˆ«æ˜¯ Thinkingï¼Œå’Œ Machinesã€‚åœ¨è®¡ç®— self-attention ä¹‹å‰é¦–å…ˆè¦è¿›è¡Œ embedding è¿ç®—ï¼Œå¾—åˆ° <span style='color:#07d015;'> $$X_1 , X_2$$ </span>  ï¼Œ
ç„¶åæˆ‘ä»¬é€šè¿‡<span style='color:#07d015;'> $$X_1 , X_2$$ </span> åˆ†åˆ«ä¹˜ä»¥çŸ©é˜µ<span style='color:#d436eb'>$$W^Q $$</span>,<span style='color:#ff8b00'>$$ W^K $$</span>,<span style='color:#30abff'>$$ W^V$$</span>ï¼Œ
å¾—åˆ°<span style='color:#d436eb'>$$q_1 , q_2 $$</span>,<span style='color:#ff8b00'>$$ k_1 , k_2 $$</span>,<span style='color:#30abff'>$$ v_1 , v_2$$</span> ï¼Œä»–ä»¬åˆ†åˆ«è¡¨ç¤ºä¸ºQuerysï¼Œ
keysï¼Œå’ŒValuesã€‚å…¶ä¸­çŸ©é˜µ<span style='color:#d436eb'>$$W^Q$$</span> , <span style='color:#ff8b00'>$$W^K$$</span> , <span style='color:#30abff'>$$W^V$$</span>ä½¿ç”¨é»˜è®¤åˆå§‹åŒ–æ•°æ®ï¼Œç„¶ååœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­å­¦ä¹ ä¼˜åŒ–ã€‚æ•´ä¸ªè¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤º

![self-attention-process]

å½“æˆ‘ä»¬å¾—åˆ°äº†ä¸åŒå•è¯çš„<span style='color:#d436eb'>$$q$$</span> , <span style='color:#ff8b00'>$$k$$</span> , <span style='color:#30abff'>$$v$$</span>ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œ self-attention è®¡ç®—äº†ã€‚æ¯”å¦‚æˆ‘ä»¬è¦è®¡ç®—<span style='color:07d015'>$$X_1$$</span>çš„self-attentionç»“æœï¼Œæˆ‘ä»¬çš„æ“ä½œæµç¨‹å¦‚ä¸‹ï¼š

![self-attention-process-2]

- é¦–å…ˆç¬¬ä¸€æ­¥å°±æ˜¯è®¡ç®—å¾—åˆ†ï¼Œä¹Ÿå°±æ˜¯å›¾ä¸­çš„Scoreï¼ŒThinking å¯¹è‡ªå·±çš„å¾—åˆ†ä¸º<span style='color:#d436eb'>$$q_1$$</span> $$\times$$ <span style='color:#ff8b00'>$$k_1^T$$</span>ï¼ŒThinking å¯¹ Machines çš„å¾—åˆ†ä¸º<span style='color:#d436eb'>$$q_1$$</span> $$\times$$ <span style='color:#ff8b00'>$$k_2^T$$</span>ï¼Œå¦‚æœåé¢è¿˜æœ‰å•è¯çš„åŒ–ï¼Œè®¡ç®—å¾—åˆ†ä¸º<span style='color:#d436eb'>$$q_1$$</span> $$\times$$ <span style='color:#ff8b00'>$$k_i^T$$</span>ã€‚
- ç¬¬äºŒæ­¥å°†å¾—åˆ†Score  é™¤ä»¥ $$\sqrt{d_k}$$ï¼Œ$$d_k$$ä¸º$$k$$çš„ç»´åº¦ï¼Œæ­¤å¤„å‡è®¾ä¸º8ã€‚
- ç¬¬ä¸‰æ­¥ä¸ºå°†ç¬¬äºŒæ­¥çš„ç»“æœè¿›è¡Œ softmax æ“ä½œã€‚
- ç¬¬å››æ­¥å°† softmax çš„ç»“æœä¹˜ä»¥å„è‡ªçš„ Valuesï¼Œå¾—åˆ°æ–°çš„å‘é‡ã€‚
- ç¬¬äº”æ­¥å°†ç¬¬å››æ­¥çš„ç»“æœå…¨éƒ¨è¿›è¡Œå‘é‡ç›¸åŠ ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„å‘é‡<span style='color:#ff5ab2'>$$z_1$$</span>ï¼Œè¿™ä¸ª<span style='color:#ff5ab2'>$$z_1$$</span>å°±æ˜¯ Thinking ç»è¿‡ self-attention è¿ç®—çš„ç»“æœã€‚
- å½“æˆ‘ä»¬è®¡ç®— Machines çš„ self-attention è¿ç®—ç»“æœçš„æ—¶å€™ï¼Œä¸ Thinking æµç¨‹æ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯åœ¨è®¡ç®— Score çš„æ—¶å€™ï¼Œä½¿ç”¨çš„æ˜¯<span style='color:#d436eb'>$$q_2$$</span>åˆ†åˆ«ä¹˜ä»¥<span style='color:#ff8b00'>$$k_1 , k_2 , ... , k_i$$</span>ï¼Œæ¥è®¡ç®—Thinkingç›¸å¯¹äºå„ä¸ªå•è¯çš„Scoreã€‚å‰©ä¸‹çš„æµç¨‹å…¶å®æ˜¯ä¸€æ ·çš„ã€‚

æˆ‘ä»¬å¯ä»¥æŠŠä¸Šé¢çš„æ­¥éª¤è½¬æ¢æˆçŸ©é˜µä¹˜æ³•è¿ç®—ï¼Œæ¯”å¦‚æˆ‘ä»¬é€šè¿‡<span style='color:#07d015;'> $$X$$ </span>è®¡ç®—<span style='color:#d436eb'>$$W^Q$$</span> , <span style='color:#ff8b00'>$$W^K$$</span> , <span style='color:#30abff'>$$W^V$$</span>ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡ä¸‹é¢çš„æ–¹å¼å¾—åˆ°ã€‚
å› ä¸ºå·ç§¯è¿ç®—å…¶å®ä¹Ÿæ˜¯çŸ©é˜µä¹˜æ³•ï¼Œ æ‰€ä»¥å…¶å®è¿™ä¸€æ­¥ä¹Ÿå¯ä»¥ç†è§£æˆä¸€ä¸ªç‰¹æ®Šå·ç§¯æ ¸çš„å·ç§¯æ“ä½œã€‚
![matrix-calculation-of-self-attention]

äºæ˜¯æˆ‘ä»¬çš„æ•´ä¸ªself-attentionå°±å¯ä»¥æè¿°æˆå¦‚ä¸‹çš„è¿ç®—ã€‚
![matrix-calculation-of-self-attention-2]

## å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

åœ¨è®ºæ–‡[Attention is All You Need.](https://arxiv.org/abs/1706.03762) ä¸­ä½œè€…ä½¿ç”¨çš„æ˜¯å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚å½“æˆ‘ä»¬çŸ¥é“äº†self-attentionæœºåˆ¶ä¹‹åï¼Œå†æ¥ç†è§£å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å°±å¾ˆç®€å•äº†ã€‚
åœ¨self-attentionä¸­æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ä¸Šå›¾çš„æ–¹æ³•å¾—åˆ°æŸä¸€ä¸ªå•è¯çš„self-attentionçš„ç»“æœï¼Œé‚£å¤šå¤´self-attentionå°±æ˜¯å¯¹æŸä¸€ä¸ªå•è¯çš„embeddingç»“æœ<span style='color:#07d015;'> $$X$$ </span>ä½¿ç”¨å¤šä¸ªä¸åŒçš„<span style='color:#d436eb'>$$W^Q$$</span> , <span style='color:#ff8b00'>$$W^K$$</span> , <span style='color:#30abff'>$$W^V$$</span>æ¥
åˆ†åˆ«è®¡ç®—å¾—åˆ°<span style='color:#07d015;'> $$X$$ </span>çš„å¤šä¸ªself-attentionç»“æœï¼Œç„¶åå°†è¿™äº›ç»“æœæ‹¼æ¥èµ·æ¥ï¼Œå†è¿›è¡Œä¸€æ¬¡å·ç§¯æ“ä½œï¼Œå¾—åˆ°çš„ç»“æœä½œä¸ºæœ€ç»ˆçš„<span style='color:#07d015;'> $$X$$ </span>å¯¹åº”çš„<span style='color:#ff5ab2'>$$Z$$</span>
å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå¯¹<span style='color:#07d015;'> $$X$$ </span>è®¡ç®—8ä¸ªself-attention
![multi-headed-attention]

ç„¶åæˆ‘ä»¬æŠŠ8ä¸ªself-attentionç»“æœæ‹¼æ¥èµ·æ¥ï¼Œç„¶åä¸€èµ·ä¹˜ä»¥ä¸€ä¸ªçŸ©é˜µ<span style='color:#f29fc6;'> $$W_O$$ </span>æœ€ç»ˆå¾—åˆ°è¾“å‡º<span style='color:#ff5ab2'>$$Z$$</span>ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º
![multi-headed-attention-2]

äºæ˜¯ä¹æ•´ä¸ªè¿‡ç¨‹å¯ä»¥æè¿°æˆå¦‚ä¸‹æ‰€ç¤ºã€‚åœ¨ä¸‹å›¾ä¸­å¤šäº†ä¸€ä¸ª<span style='color:#30abff'>$$R$$</span>ï¼Œä½ å¯ä»¥æŠŠå®ƒæƒ³åƒæˆä¸Šä¸€ä¸ªencoderçš„è¾“å‡ºï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¸‹ä¸€ä¸ªencoderçš„è¾“å…¥ã€‚
åŒæ—¶åœ¨æ•´ä¸ªencodersä¸­åªæœ‰ç¬¬ä¸€ä¸ªencoderçš„è¾“å…¥æ˜¯éœ€è¦è¿›è¡Œembeddingæ“ä½œçš„ï¼Œåé¢çš„éƒ½ä¸éœ€è¦ã€‚
![multi-headed-attention-3]

## æ³¨æ„åŠ›ç†è§£

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹æ³¨æ„åŠ›æœºåˆ¶æœ€ç»ˆè¾¾åˆ°çš„ç›®çš„æ˜¯å“ˆã€‚æ¯”å¦‚æˆ‘ä»¬çš„è¾“å…¥æ˜¯ <span style='color:red'>The animal didn't cross the street because it was too tired </span>ï¼Œæˆ‘ä»¬æƒ³è¦çŸ¥é“å…¶ä¸­çš„<span style='color:red'>it</span>è¡¨ç¤ºçš„æ˜¯å•¥ã€‚
å¯¹äºæˆ‘ä»¬äººç±»æ¥è¯´è¿™ä¸ªé—®é¢˜å¤ªç®€å•äº†ï¼Œä½†æ˜¯å¯¹äºæœºå™¨æ¥è¯´ç¡®æ˜¯å¾ˆéš¾ã€‚å½“æˆ‘ä»¬æœ‰äº†self-attentionä¹‹åï¼Œæœºå™¨è¿™ä¸ªé—®é¢˜çœ‹ä¸Šå»å°±æœ‰äº†ä¸€äº›è½¬æœºã€‚ä¸‹å›¾æ˜¯æˆ‘ä»¬çš„self-attentionçš„æ•ˆæœå›¾ã€‚
![self-attention-result]

å›¾ä¸­è¿çº¿è¶Šç²—è¡¨ç¤ºç›¸å…³æ€§è¶Šå¼ºï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ<span style='color:red'>it</span>ä¸<span style='color:red'>The animal</span>çš„ç›¸å…³æ€§æ˜¯æœ€å¼ºçš„ã€‚

åœ¨å¤šå¤´æ³¨æ„åŠ›multi-headed-attentionä¸­ï¼Œæˆ‘ä»¬çš„ <span style='color:red'>it</span> ç›¸å¯¹ä¸æ¯ä¸€ä¸ªä¸åŒçš„self-attentionï¼Œå®ƒè¦è¡¨è¾¾çš„ä¸œè¥¿ä¸ä¸€æ ·ï¼Œæ¯”å¦‚ä¸‹å›¾æ‰€ç¤ºæœ‰ä¸¤ä¸ªself-attentionï¼Œå¯¹äº<span style='color:red'>it</span>è¿çº¿æœ€ç²—çš„åˆ†åˆ«æ˜¯<span style='color:red'>The animal</span>å’Œ<span style='color:red'>tired</span>ï¼Œ
è™½ç„¶è™½ç„¶æ¯ä¸€ä¸ªself-attentionæœ€ç»ˆçš„ä¾§é‡ç‚¹ä¸ä¸€æ ·ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥ç†è§£æˆä¸åŒçš„self-attentionè¡¨ç¤ºçš„è”ç³»ä¸ä¸€æ ·ï¼Œæ¯”å¦‚<span style='color:red'>it</span>è™½ç„¶è¡¨ç¤ºçš„ä¸æ˜¯<span style='color:red'>tired</span>ï¼Œä½†æ˜¯<span style='color:red'>it</span>æ˜¾ç„¶æ˜¯ä¸<span style='color:red'>tired</span>æœ‰å…³çš„ã€‚
æˆ–è€…æˆ‘ä»¬å¯ä»¥å°†å®ƒç†è§£æˆé«˜ç»´çš„è”ç³»ã€‚å¦‚ä¸‹å›¾æ‰€ç¤º
![self-attention-result-2]

## ä½ç½®ç¼–ç 

åœ¨self-attentionä¸­ï¼Œæ‰€æœ‰çš„æ“ä½œéƒ½æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°†self-attentionç†è§£æˆç‰¹æ®Šçš„å·ç§¯æ“ä½œï¼Œè€Œä¸”æ˜¯ä¸åˆ†é¡ºåºçš„ï¼Œè¿™ä¸æˆ‘ä»¬åœ¨ä½¿ç”¨ä¹‹å‰ä»‹ç»çš„[å·ç§¯ç½‘ç»œè¿›è¡Œæ–‡æœ¬ç¿»è¯‘](https://7568.github.io/2021/11/15/cnn-seq2seqModel.html) æ¥æ“ä½œæ–‡æœ¬ç¿»è¯‘ä¸€æ ·ï¼Œéƒ½éœ€è¦æœ‰ä¸ªä½ç½®ç¼–ç æ¥åŒºåˆ†ç›¸åŒçš„å•è¯ç”±äºå‡ºç°çš„ä½ç½®ä¸ä¸€æ ·ï¼Œå¯¼è‡´å¥å­çš„æ„æ€ä¸ä¸€æ ·çš„æƒ…å†µã€‚
åœ¨[å·ç§¯ç½‘ç»œè¿›è¡Œæ–‡æœ¬ç¿»è¯‘](https://7568.github.io/2021/11/15/cnn-seq2seqModel.html) ä¸­ï¼Œæˆ‘ä»¬çš„ä½ç½®ç¼–ç ç›´æ¥ä½¿ç”¨çš„æ˜¯é¡ºåºç¼–ç ï¼Œä¹Ÿå°±æ˜¯è¯´æŒ‰ç…§å¥å­çš„é•¿åº¦ï¼Œä»0å¼€å§‹ï¼Œä¾æ¬¡ç¼–ç ï¼Œä½ç½®åœ¨ç¬¬0ä½ä½ç½®ç¼–ç å°±æ˜¯0ï¼Œåœ¨ç¬¬1ä½ï¼Œä½ç½®ç¼–ç å°±æ˜¯1ï¼Œä¾æ¬¡å¾€åã€‚
ä½†æ˜¯åœ¨æˆ‘ä»¬çš„è®ºæ–‡[Attention is All You Need.](https://arxiv.org/abs/1706.03762) ä¸­ä½œè€…æä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„ä½ç½®ç¼–ç çš„æ–¹å¼ï¼Œå…·ä½“å°±æ˜¯ä½¿ç”¨æŸä¸ªæ–¹æ³•ï¼Œç”Ÿæˆä¸€ç»„å‘é‡ï¼Œæ¯ä¸ªå‘é‡å°±è¡¨ç¤ºä¸€ä¸ªä½ç½®ã€‚æ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
![position-vector]

æ¯”å¦‚æˆ‘ä»¬çš„ä½ç½®ç¼–ç å‘é‡çš„é•¿åº¦ä¸º4ï¼Œå¯¹äºä¸åŒçš„ä½ç½®ï¼Œæ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
![position-vector-2]

æ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹å‘é‡é•¿åº¦ä¸º512ï¼Œå¥å­é•¿åº¦ä¸º20çš„ä¸€ä¸ªä½ç½®ç¼–ç çš„æ•ˆæœå›¾ã€‚æ¯ä¸€è¡Œå°±è¡¨ç¤ºä¸€ä¸ªå•è¯çš„ä½ç½®å‘é‡ã€‚
![position-vector-3]

åœ¨å¾ˆå¤šåœ°æ–¹æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°å¦‚ä¸‹çš„ä½ç½®ç¼–ç æ•ˆæœå›¾ï¼Œå…¶å®è¯¥æ•ˆæœå›¾ä¸ä¸Šé¢çš„æ•ˆæœå›¾å·®åˆ«ä¸æ˜¯å¾ˆå¤§ï¼Œåªæ˜¯ä¸‹é¢çš„æ•ˆæœå›¾åŠ ä¸Šäº† interweaves æ“ä½œï¼Œå…¶å®å°±æ˜¯å¯¹å¥‡æ•°ä½ç½®å’Œå¶æ•°ä½ç½®åˆ†åˆ«å†ç”¨sinå’Œcosè¿›è¡Œè¿ç®—ä¸€æ¬¡ã€‚
![position-vector-4]

## æ®‹å·®å—

æ¥ä¸‹æ¥æˆ‘ä»¬å†æ¥çœ‹çœ‹æ¯ä¸€ä¸ªencoderå†…éƒ¨çš„ç»“æ„ï¼Œå¦‚ä¸‹ï¼š
![encoder-architecture]

å½“æˆ‘ä»¬çš„Xç»è¿‡äº†self-attentionä¹‹åï¼Œä¼šè¿›å…¥åˆ°ä¸€ä¸ªAdd&Normalizeçš„å±‚ï¼ŒAddæ“ä½œæ˜¯æŒ‡å°†self-attentionçš„è¾“å‡º<span style='color:#ff5ab2'>$$z_1,z_2, ... , z_i$$</span>æ‹¼æ¥èµ·æ¥ï¼Œç„¶åå†
ä¸è¾“å…¥<span style='color:#07d015'>$$x_1,x_2,...,x_i$$</span>è¿›è¡Œç›¸åŠ ï¼Œè¿™ä¸€å¥—æ“ä½œä¹Ÿç§°ä¸ºæ®‹å·®æ“ä½œã€‚ç„¶åå°†æ®‹å·®çš„ç»“æœè¿›è¡ŒLayerNormè¿ç®—ï¼Œå¾—åˆ°æ–°çš„<span style='color:#ff5ab2'>$$z_1,z_2, ... , z_i$$</span>ã€‚ä¹‹åæˆ‘ä»¬å°†<span style='color:#ff5ab2'>$$z_1,z_2, ... , z_i$$</span>å½“ä½œæ–°çš„è¾“å…¥ï¼Œ
ä¼ å…¥åˆ°ä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œä¸­ï¼Œç„¶åå†è¿›è¡Œä¸€æ¬¡æ®‹å·®æ“ä½œï¼Œä¸LayerNormæ“ä½œã€‚æˆ‘ä»¬å°†è¿™ä¸¤æ¬¡æ®‹å·®æ“ä½œåˆç§°ä¸ºä¸€ä¸ªæ®‹å·®å—ã€‚äºæ˜¯ä¸€ä¸ªencoderå°±æ˜¯ä¸€ä¸ªæ®‹å·®å—ï¼Œæ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤º
![encoder-architecture-2]

æ¥ä¸‹æ¥æˆ‘ä»¬å†æ¥çœ‹çœ‹decodersä¸­çš„å†…å®¹ï¼Œå‘ç°å…¶å®ä¸encodersä¸­çš„å†…å®¹æ˜¯ä¸€æ ·çš„ï¼Œæ•´ä½“ç»“æ„å¦‚ä¸‹
![encoder-architecture-3]

è·Ÿåœ¨[å·ç§¯ç½‘ç»œè¿›è¡Œæ–‡æœ¬ç¿»è¯‘](https://7568.github.io/2021/11/15/cnn-seq2seqModel.html) ä¸­ä¸€æ ·ï¼Œæˆ‘ä»¬æœ€å…³å¿ƒçš„è¿˜æ˜¯åœ¨å®é™…ä¸­ï¼Œdecoderæ˜¯å¦‚ä½•ä¸€æ­¥æ­¥ç”Ÿæˆç¿»è¯‘ç»“æœçš„ã€‚
ä¸‹é¢çš„åŠ¨ç”»å°±å¾ˆå¥½çš„è§£é‡Šäº†ç¿»è¯‘çš„decoderçš„å…¨è¿‡ç¨‹
<video width="100%" height="auto" loop autoplay controls>
  <source src="http://7568.github.io/images/2021-11-03-transformer/transformer_decoding.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

ä¹Ÿå°±æ˜¯è¯´åœ¨æˆ‘ä»¬è®­ç»ƒçš„æ—¶å€™ï¼Œæˆ‘ä»¬çš„decodersåªæ˜¯ä»ä¸‹åˆ°ä¸Šè¿è¡Œä¸€æ¬¡ï¼Œä¸€æ¬¡åŠ è½½æ‰€æœ‰çš„è¾“å…¥ï¼Œç„¶åä¸€æ¬¡æ€§çš„å¾—åˆ°ç»“æœã€‚è€Œåœ¨æˆ‘ä»¬å®é™…ç¿»è¯‘çš„æ—¶å€™ï¼Œç”±äºæˆ‘ä»¬å®é™…çš„ç­”æ¡ˆæ˜¯ä¸èƒ½å†å½“ä½œè¾“å…¥çš„ï¼Œæˆ–è€…æˆ‘ä»¬æ­¤æ—¶æ ¹æœ¬å°±æ²¡æœ‰ç¿»è¯‘çš„ç­”æ¡ˆï¼Œè¿™ä¸ªæ—¶å€™decoderså°±ä¼šè¿è¡Œå¤šæ¬¡ï¼Œæ¯”å¦‚ç¿»è¯‘çš„ç»“æœå¥å­é•¿åº¦ä¸º10ï¼Œé‚£ä¹ˆdecoderså°±ä¼šè¿è¡Œ12æ¬¡ï¼Œå› ä¸ºä»å¼€å§‹ç¬¦å·ä¸€ç›´è¦è¿è¡Œåˆ°ç»“æŸç¬¦ä¸ºæ­¢ã€‚

æ¯”å¦‚æˆ‘ä»¬çš„ç¿»è¯‘çš„ç»“æœä¸º<span style='color:red'>"I am a student"</span>ï¼Œé‚£ä¹ˆæˆ‘ä»¬åœ¨decodersä¸­ï¼Œç¬¬ä¸€æ¬¡çš„è¾“å…¥æ˜¯ <span style='color:red'>"\<sos\>"</span>ï¼Œç»è¿‡decodersä¹‹åå¾—åˆ° <span style='color:red'>"I"</span>ï¼Œç„¶åå°† <span style='color:red'>"\<sos\> I"</span>ä¸€èµ·å½“ä½œè¾“å…¥ï¼Œæ”¾å…¥åˆ°decodersä¸­ï¼Œå¾—åˆ° <span style='color:red'>"am"</span>
ç„¶åå†å°† <span style='color:red'>"\<sos\> I am"</span> å½“ä½œè¾“å…¥æ”¾å…¥åˆ°decodersä¸­ï¼Œå¾—åˆ°ç»“æœ <span style='color:red'>"a"</span> ä¾æ¬¡ç±»æ¨ä¸‹å»ï¼Œä¸€ç›´åˆ°å¾—åˆ°ç»“æŸç¬¦ <span style='color:red'>"\<eos\>"</span> ä¸ºæ­¢ã€‚
æœ€ç»ˆå¾—åˆ°çš„ç¿»è¯‘ç»“æœä¸º<span style='color:red'>"\<sos\> I am a student \<eos\>"</span>

# ä»£ç åˆ†æ

ç”±äºæœ¬ç« å†…å®¹è¿‡å¤šï¼Œå·²å°†ä»£ç åˆ†ææ”¾äº[ğŸ’ æ­¤å¤„ ğŸ’ ](https://7568.github.io/2021/11/24/transformer-code-comments.html)

æ›´å¤šå‚è€ƒæ¥è‡ªäº
- [graykode / nlp-tutorial](https://github.com/graykode/nlp-tutorial/blob/d05e31ec81d56d70c1db89b99ab07e948f7ebc11/5-1.Transformer/Transformer(Greedy_decoder).py)
- [Transformers: Attention in Disguise](https://www.mihaileric.com/posts/transformers-attention-in-disguise/)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)