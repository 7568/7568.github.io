---
layout: blog
text-process: true
background-image: http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/img.png
category: æ–‡æœ¬å¤„ç†
title: æœºå™¨ç¿»è¯‘ - Seq2Seq with Attention
mathjax: true
tags:
- Seq2Seq
- Attention
- æ–‡æœ¬å¤„ç†
---

[2021-11-04_seq2seq_3]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/2021-11-04_seq2seq_3.png
[input-batch]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/input-batch.png
[padded-input-batch]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/padded-input-batch.png
[input-numericalize]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/input-numericalize.png
[lstm-struct]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/lstm-struct.png
[seq2seq-lstm]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/seq2seq-lstm.png
[seq2seq2-encoder]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/seq2seq-encoder.png
[seq2seq2-decoder]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/seq2seq-decoder.png
[gru-encoder]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/gru-encoder.png
[gru-decoder]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/gru-decoder.png
[Seq2Seq-model]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/Seq2Seq-model.png
[seq2seq-with-gru]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/seq2seq-with-gru.png
[bidirectional-rnn]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/bidirectional-rnn.png
[rnn-attention-encoder]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/rnn-attention-encoder.png
[rnn-attention-arcitecture]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/rnn-attention-arcitecture.png
[seq2seq2-Embedding]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/Embedding.png
[pack_padded_sequence]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/pack_padded_sequence.png
[align]:http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/align.png

# ç®€ä»‹

åœ¨æ–‡æœ¬å¤„ç†ä¸­æœ‰ä¸¤ä¸ªç»å…¸çš„ç½‘ç»œæ¨¡å‹ï¼Œä¸€ä¸ªæ˜¯åŸºäºå¾ªç¯ç¥ç»ç½‘ç»œåŠ ä¸Š attention çš„ Seq2Seq å’Œå®Œå…¨åŸºäº attention çš„ Transformerã€‚è¿™ä¸¤ä¸ªæ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä¸­éƒ½å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚
æœ¬æ–‡ä¸­å¾ˆå¤§ä¸€éƒ¨åˆ†å†…å®¹æ¥è‡ªç¿»è¯‘
[Seq2seq Models With Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
å’Œ
[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) ã€‚
ä»£ç å‚è€ƒäº [https://github.com/bentrevett/pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq)

æœ¬ç¯‡å°†ä¸»è¦è®²è¿°å’Œç¿»è¯‘åœ¨[Seq2seq Models With Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
ä¸­çš„å†…å®¹ï¼ŒSeq2seqçš„ç›¸å…³è®ºæ–‡åœ°å€åœ¨[Sutskever et al.](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) , [Cho et al., 2014](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)
æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç¯‡[æ–‡ç« ](https://7568.github.io/2021/11/03/transformer.html) ä¸­è®²è¿°Transformerï¼Œä¹Ÿå°±æ˜¯[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) ä¸­çš„å†…å®¹

æ‰€æœ‰ä»£ç éƒ½å¯åœ¨æ–‡æœ«ä¸‹è½½ï¼Œè¿™äº›ä»£ç éƒ½æ˜¯æˆ‘ä»[bentrevett / pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq) ä¸­æå–å‡ºçš„

# æ¨¡å‹ä»‹ç»

Sequence-to-sequenceæ¨¡å‹æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œåœ¨å¾ˆå¤šåƒæœºå™¨ç¿»è¯‘ï¼ŒçŸ­æ–‡æ€»ç»“ï¼Œå’Œå›¾åƒæè¿°ç­‰ä»»åŠ¡ä¸­éƒ½å–å¾—äº†å¾ˆå¥½çš„æˆç»©ã€‚æ¥ä¸‹æ¥æˆ‘å°†é€šè¿‡æœ¬blogæ¥ä»‹ç» Seq2Seq æ¨¡å‹çš„ç›¸å…³å†…å®¹å’Œä»£ç ã€‚å¸Œæœ›å¯¹äºåˆå­¦è€…æœ‰æ‰€å¸®åŠ©ã€‚
æœ¬æ–‡ä¸»è¦è®²è¿°çš„ä»£ç æ˜¯ Seq2Seq æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä¸Šè‹±æ–‡å¯¹å¾·æ–‡çš„ç¿»è¯‘ã€‚
<br/>
Seq2Seq æ¨¡å‹æ˜¯å…¸å‹çš„ encoder-decoder æ¨¡å‹ï¼Œä¸‹é¢çš„åŠ¨ç”»å°†ä»‹ç» Seq2Seq è¿›è¡Œæœºå™¨ç¿»è¯‘æ—¶å€™çš„åŸºæœ¬å·¥ä½œæµç¨‹ã€‚å·¦è¾¹æ˜¯è¾“å…¥ï¼Œå³è¾¹æ˜¯è¾“å‡ºã€‚

<video width="100%" height="auto" loop autoplay controls>
  <source src="http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/2021-11-04_seq2seq_1.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

ä¸‹é¢è¿™ä¸ªè§†é¢‘æ¥è‡ªäº [https://github.com/google/seq2seq](https://github.com/google/seq2seq) ä¸è¿‡ `https://github.com/google/seq2seq`ä¸­çš„å†…å®¹å¯¹æœ¬æ–‡å…³ç³»ä¸å¤§

<video width="100%" height="auto" loop autoplay controls>
  <source src="http://7568.github.io/images/2021-11-03-rnn-seq2seqModel/2021-11-04_seq2seq_2.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

## encoder

åœ¨ Seq2Seq æ¨¡å‹çš„ encoder ä¸­ï¼Œè¦è¿›è¡Œçš„å·¥ä½œæœ‰ï¼š

1. å°†è¾“å…¥ X1 å­—ç¬¦ç¼–ç ï¼Œå˜æˆæ•°å­—ç±»å‹ï¼Œå³ Word2Vecï¼Œå¾—åˆ° X1_Vecï¼Œå¦‚æœæˆ‘ä»¬çš„è¾“å…¥æ˜¯ "æ—©ä¸Šå¥½"ï¼Œåœ¨ Word2Vec ä¸­ï¼Œå…ˆä¼šåŠ ä¸Šå¼€å§‹æ ‡å¿— `<sos>` å’Œç»“æŸæ ‡å¿— `<eos>` ï¼Œ
   è¿™æ ·è¾“å…¥å°±å˜æˆäº†5ä¸ªå­—ç¬¦ï¼Œç„¶åæ¯ä¸ªå­—ç¬¦ç”¨ä¸€ä¸²0å’Œ1è¡¨ç¤ºï¼Œäºæ˜¯å¾—åˆ°5ä¸ªVectorï¼Œå°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„ X1_Vec ã€‚
2. å°† X1_Vec ä¸­çš„5ä¸ª Vector ä¾æ¬¡æŒ‰é¡ºåºæ”¾å…¥åˆ°RNNä¸­ï¼Œå¾—åˆ°ä¸€ä¸ªè¾“å‡º Z
   æ¯”å¦‚è¿™æ · ![2021-11-04_seq2seq_3]

## decoder

åœ¨ decoder ä¸­ï¼Œé¦–å…ˆæˆ‘ä»¬è¦å¯¹æ ‡ç­¾è¿›è¡Œç¼–ç ï¼Œç„¶åï¼Œå°†ç¼–ç åçš„ç»“æœæ”¾å…¥åˆ°ä¸€ä¸ªç¥ç»ç½‘ç»œä¸­ï¼Œç”¨æ¥æå–ç‰¹å¾ï¼Œ

## ä»£ç å®ç°

### æ•°æ®å‡†å¤‡

é¦–å…ˆæˆ‘ä»¬è¦å®‰è£…pytorch(1.0ä»¥ä¸Š)ï¼Œtorchtextï¼Œspacy

```python
import torch
import torch.nn as nn
import torch.optim as optim

from torchtext.legacy.datasets import Multi30k
from torchtext.legacy.data import Field, BucketIterator

import spacy
import numpy as np

import random
import math
import time

SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
```

æˆ‘ä»¬é€šè¿‡æ‰§è¡Œä¸€ä¸‹è„šæœ¬æ¥å®‰è£…æ•°æ®é›†

```shell
$ python -m spacy download en_core_web_sm
$ python -m spacy download de_core_news_sm
```

ç„¶ååŠ è½½æ•°æ®é›†

```python
spacy_de = spacy.load('de_core_news_sm')
spacy_en = spacy.load('en_core_web_sm')
```

æ¥ä¸‹æ¥å°±æ˜¯æˆ‘ä»¬çš„ç¼–ç é˜¶æ®µ
é¦–å…ˆæˆ‘ä»¬å°†è¾“å…¥çš„ä¸€ä¸²è¿ç»­çš„å­—ç¬¦è½¬æ¢æˆlistï¼Œå¦‚ 'æ—©ä¸Šå¥½' è½¬æ¢æˆ '[æ—©,ä¸Š,å¥½]' ï¼Œç„¶åå†å°†ä»–ä»¬å˜æˆ 0ï¼Œ1 ç¼–ç ï¼Œä»£ç å¦‚ä¸‹

```python
def tokenize_cn(text):
    """
    Tokenizes German text from a string into a list of strings (tokens) and reverses it
    å°†å¾·æ–‡çš„ä¸€æ®µè¯è¿›è¡Œç¼–ç ï¼Œå°†æ¯ä¸ªæ±‰å­—éƒ½ç¼–ç æˆä¸€ä¸ªå­—ç¬¦ä¸²å¼çš„tokensã€‚ç„¶åå°†ä»–ä»¬åè½¬ï¼Œåè½¬æ˜¯ä¸ºäº†æ”¾å…¥RNNçš„æ—¶å€™ä¿è¯æœ€å…ˆæ”¾å…¥çš„æ˜¯ä¸€å¼€å§‹çš„å­—ç¬¦ï¼Œè€Œä¸æ˜¯æœ€åçš„å­—ç¬¦
    """
    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]

def tokenize_en(text):
    """
    Tokenizes English text from a string into a list of strings (tokens)
    è‹±æ–‡åŒå¾·æ–‡ä¸€æ ·
    """
    return [tok.text for tok in spacy_en.tokenizer(text)]
```

åœ¨ torchtext ä¸­å·²ç»æœ‰æ–¹æ³•å¸®æˆ‘ä»¬å®ç°ç¼–ç æ–¹æ³•ï¼Œæˆ‘ä»¬åªéœ€è¦è°ƒç”¨å¦‚ä¸‹æ–¹æ³•ï¼Œåˆ†åˆ«è¿›è¡Œå¾·æ–‡å’Œè‹±æ–‡çš„ç¼–ç 

```python
SRC = Field(tokenize = tokenize_de, 
            init_token = '<sos>', 
            eos_token = '<eos>', 
            lower = True)

TRG = Field(tokenize = tokenize_en, 
            init_token = '<sos>', 
            eos_token = '<eos>', 
            lower = True)
```

æ¥ä¸‹æ¥æˆ‘ä»¬åŠ è½½æ•°æ®é›†ï¼Œç„¶åè‡ªåŠ¨åˆ†ä¸ºè®­ç»ƒæ•°æ®ï¼ŒéªŒè¯æ•°æ®ï¼Œå’Œæµ‹è¯•æ•°æ®ï¼Œæœ¬æ•°æ®é›†ä½¿ç”¨çš„æ˜¯ [Multi30k dataset](https://github.com/multi30k/dataset) ï¼Œ
é‡Œé¢åŒ…å«æœ‰ 30000 æ¡è‹±æ–‡å¯¹æ³•æ–‡å’Œå¾·æ–‡çš„å¥å­ã€‚

```python
train_data, valid_data, test_data = Multi30k.splits(exts = ('.cn', '.en'),  fields = (SRC, TRG))
```

æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ¯ä¸ªæ•°æ®é›†çš„å¤§å°

```python
print(f"Number of training examples: {len(train_data.examples)}")
print(f"Number of validation examples: {len(valid_data.examples)}")
print(f"Number of testing examples: {len(test_data.examples)}")
```

æŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬æ•°æ®ï¼Œçœ‹çœ‹æ•°æ®é›†çš„æ ¼å¼æ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚

```python
print(vars(train_data.examples[0]))
```

æˆ‘ä»¬å¾—åˆ°çš„è¾“å‡ºæ˜¯è¿™æ ·å­çš„ï¼Œå‰é¢çš„ `src`ä¸­æ˜¯å¾·è¯­ï¼Œåé¢çš„ `trg`ä¸­æ˜¯è‹±è¯­ã€‚

```json5
{'src': ['.', 'bÃ¼sche', 'vieler', 'nÃ¤he', 'der', 'in', 'freien', 'im', 'sind', 'mÃ¤nner', 'weiÃŸe', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}
```

æ¥ä¸‹æ¥æˆ‘ä»¬å¯¹è®­ç»ƒé›†çš„è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œ vocabulary å¤„ç†ï¼Œ vocabulary å¤„ç†å…¶å®å°±æ˜¯æ‰¾å‡ºè¾“å…¥è¾“å‡ºä¸­æ‰€æœ‰çš„å•è¯ï¼Œç„¶åå»é‡ï¼Œç„¶åç»™å»é‡åçš„æ¯ä¸ªå•è¯æ’åºï¼Œå¾—åˆ°æ¯ä¸ªå•è¯çš„indexï¼Œè¿™ä¸ªindexå°±æ˜¯æ¯ä¸ªå•è¯çš„ç¼–ç ã€‚

æ‰§è¡Œ vocabulary å¤„ç†ä»£ç å¦‚ä¸‹

```python
# æ„å»ºè¯æ±‡ï¼Œæ„å»ºä¹‹åï¼ŒSRC å°±å¤šäº†ä¸ª vocab å±æ€§ï¼Œvocab ä¸­åŒ…å«æœ‰ freqsã€itosã€stoi ä¸‰ä¸ªå±æ€§ï¼Œå…¶ä¸­freqs è¡¨ç¤ºçš„æ˜¯ SRC ä¸­æ¯ä¸ªå•è¯å’Œè¯¥å•è¯çš„é¢‘æ•°ï¼Œä¹Ÿå°±æ˜¯ä¸ªæ•°ã€‚
# itos æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«çš„çš„æ˜¯é¢‘æ•° >= 2 çš„å•è¯ï¼Œstoi ç”¨æ¥æ ‡è®° itos ä¸­æ¯ä¸ªå•è¯çš„ç´¢å¼•ï¼Œä»0å¼€å§‹ã€‚
# ä¾‹å¦‚ è¾“å…¥æ˜¯['two', 'two', ',', 'two', 'two', 'are', 'outside', 'near', 'many', 'bushes', 'two', 'young', ',', 'white', 'white', 'near', 'outside', 'near', 'many', 'bushes', '.']
# åˆ™ freqs æ˜¯({'two': 5, 'near': 3, ',': 2, 'outside': 2, 'many': 2, 'bushes': 2, 'white': 2, 'are': 1, 'young': 1, '.': 1})
# itos æ˜¯ ['<unk>', '<pad>', '<sos>', '<eos>', 'two', 'near', ',', 'bushes', 'many', 'outside', 'white']
# å…¶ä¸­ <sos>ï¼šä¸€ä¸ªå¥å­çš„å¼€å§‹ï¼Œ<eos>ï¼šä¸€å¥è¯çš„ç»“æŸï¼Œ<UNK>: ä½é¢‘è¯æˆ–æœªåœ¨è¯è¡¨ä¸­çš„è¯ï¼Œæ¯”å¦‚æˆ‘ä»¬è®¾ç½® min_freq = 2 ï¼Œé‚£ä¹ˆé‚£äº›åªå‡ºç°äº† 1 æ¬¡çš„å•è¯ï¼Œå°†æ¥åœ¨æ”¾å…¥åˆ°ç¥ç»ç½‘ç»œä¹‹å‰éƒ½ä¼šè¢« <UNK> æ›¿æ¢æ‰
# <PAD>: è¡¥å…¨å­—ç¬¦ï¼Œç”±äºæˆ‘ä»¬åœ¨è¿›è¡Œæ‰¹é‡è®¡ç®—çš„æ—¶å€™ï¼Œæ¯ä¸ªæ ·æœ¬çš„é•¿åº¦ä¸ä¸€æ ·ï¼Œ<PAD>å°±æ˜¯ç”¨äºä¿è¯æ ·æœ¬é•¿åº¦ä¸€æ ·çš„ã€‚å‚è€ƒäº https://github.com/nicolas-ivanov/tf_seq2seq_chatbot/issues/15
# ç”±äºæˆ‘ä»¬çš„ min_freq = 2 ï¼Œæ‰€ä»¥å¯ä»¥çœ‹åˆ°é¢‘æ•°ä¸º1çš„ 'are'ï¼Œ'young'ï¼Œ '.' éƒ½æ²¡æœ‰åœ¨ itos ä¸­ã€‚
# stoi æ˜¯ {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3, 'two': 4, 'near': 5, ',': 6, 'bushes': 7, 'many': 8, 'outside': 9, 'white': 10})
SRC.build_vocab(train_data, min_freq = 2)
TRG.build_vocab(train_data, min_freq = 2)
# è¾“å‡ºä¸€ä¸‹ç»“æœ
print(f"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}")
print(f"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}")
```

æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹æˆ‘ä»¬çš„æ•°æ®åœ¨è¿›å…¥åˆ°ç¥ç»ç½‘ç»œä¹‹å‰ï¼Œéƒ½ç»å†äº†æ€æ ·çš„å¤„ç† ï¼š

- é¦–å…ˆæ˜¯æˆ‘ä»¬çš„åŸå§‹æ•°æ®ï¼Œå¦‚ä¸‹å›¾å±•ç¤ºã€‚
  
  ![input-batch]


- æ¥ä¸‹æ¥æ˜¯æˆ‘ä»¬çš„å¡«å……ï¼Œé¦–å…ˆåœ¨æ¯å¥è¯çš„å¼€å§‹å’Œç»“æŸåˆ†åˆ«åŠ ä¸Š'\<sos\>'å’Œ '\<eos\>' ï¼Œ ç„¶åå°†æ•´ä¸ª batch ä¸­çš„æ•°æ®å¯¹é½ï¼ŒæŒ‰ç…§æœ€é•¿çš„å¥å­å¯¹é½ï¼Œ
  ä¸å¤Ÿçš„ç”¨ '\<pad\>' æ¥å¡«å……ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
  
  ![padded-input-batch]
  

- æœ€åå°±æ˜¯å°†è¾“å…¥æ•°æ®è¿›è¡Œæ•°å­—åŒ–å¤„ç†ï¼Œå°†æ¯ä¸ªå•è¯åˆ†åˆ«è½¬æ¢æˆå®ƒæ‰€å¯¹åº”çš„ç´¢å¼•ï¼Œè¯¥ç´¢å¼•å°±æ˜¯ SRC.vocab stoi ä¸­çš„å€¼ ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
  
  ![input-numericalize]

åˆ°æ­¤ï¼Œæˆ‘ä»¬çš„æ•°æ®é¢„å¤„ç†å°±å®Œæˆäº†ã€‚

### Encoder

æ¥ä¸‹æ¥æˆ‘ä»¬æ„é€ æˆ‘ä»¬çš„ encoder æ¨¡å‹


åœ¨RNNç³»åˆ—ä¸­ï¼Œä¼ ç»Ÿçš„RNNå­˜åœ¨æ¯”è¾ƒå¤§çš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ï¼Œæ‰€ä»¥ç°åœ¨å¤§å®¶å¸¸å¸¸ç”¨LSTMæ¥ä»£æ›¿RNNï¼Œæœ¬æ–‡ä¹Ÿå°†ä½¿ç”¨ LSTM æ¥è¿›è¡Œç¼–ç ï¼Œåœ¨ [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) ä¸­æœ‰å¯¹ LSTM çš„è¯¦ç»†ä»‹ç»ã€‚
æˆ‘ä»¬å…ˆçœ‹çœ‹LSTMçš„ç»“æ„ï¼Œè¯¥ç»“æ„å›¾æ¥è‡ªäº[dive into deep learning](https://d2l.ai/chapter_recurrent-modern/lstm.html)
![lstm-struct]

æœ€ç»ˆçš„è¾“å‡ºå°±æ˜¯æŠŠ $$ H_t $$ åšä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œç›´æ¥å°† $$ H_t $$ å½“ä½œè¾“å‡ºä¹Ÿæ˜¯å¯ä»¥çš„ã€‚åœ¨ [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) è¿™ç¯‡æ–‡ç« ä¸­æœ‰å…³äº LSTM æ›´åŠ è¯¦ç»†çš„ä»‹ç»ã€‚

äºæ˜¯æˆ‘ä»¬çš„seq2seqæ¨¡å‹å°±å˜æˆäº†å¦‚ä¸‹ç»“æ„ï¼Œè¯¥å›¾æ¥è‡ªäº [nicolas-ivanov](https://github.com/nicolas-ivanov/tf_seq2seq_chatbot/issues/15)

![seq2seq-lstm]

åœ¨åŸè®ºæ–‡ä¸­ä½œè€…ä½¿ç”¨çš„æ˜¯4å±‚LSTMï¼Œæœ¬æ–‡æˆ‘ä»¬åªä½¿ç”¨2å±‚LSTMè¿›è¡Œè®­ç»ƒã€‚å…¶ä¸­æ¯å±‚ä¸­åˆåŒ…å«æœ‰å¤šä¸ªLSTMå•å…ƒï¼Œå…·ä½“æœ‰å¤šå°‘ä¸ªæ˜¯æ ¹æ®è¾“å…¥çš„é•¿åº¦å†³å®šçš„ã€‚LSTMæˆ‘ä»¬å¯ä»¥è¡¨ç¤ºæˆå¦‚ä¸‹è¡¨è¾¾å¼ï¼š

$$(h_t,c_t) = LSTM(e(x_t),h_{t-1},c_{t-1})$$

å…¶ä¸­ $$h_t$$ å’Œ $$c_t$$ åˆ†åˆ«è¡¨ç¤ºç¬¬ t ä¸ªLSTMçš„è¾“å‡ºä¸­çš„éšè—å•å…ƒå’Œè®°å¿†å•å…ƒï¼Œ$$x_t$$ è¡¨ç¤ºç¬¬ t ä¸ªè¾“å…¥ï¼Œ$$e(x_t)$$ è¡¨ç¤ºå°†ç¬¬ t ä¸ªè¾“å…¥è¿›è¡Œ [embedding](#embedding) å¤„ç†ã€‚$$h_(t-1),c_(t-1))$$ åˆ†åˆ«è¡¨ç¤ºä¸Šä¸€å±‚çš„è¾“å‡ºä¸­çš„éšè—å•å…ƒå’Œè®°å¿†å•å…ƒã€‚
åœ¨ç†è§£ä¸Šæˆ‘ä»¬å¯ä»¥æŠŠ $$h_t$$ å’Œ $$c_t$$ éƒ½å½“æˆéšè—å•å…ƒï¼Œåªä¸è¿‡è®¡ç®—æ–¹å¼ä¸ä¸€æ ·ã€‚**å…¶ä¸­ $$h_0$$ å’Œ $$c_0$$ ï¼Œæ˜¯åˆå§‹åŒ–éšæœºç”Ÿæˆçš„** ã€‚

$$z^i = (h_l^i,c_l^i)$$

æˆ‘ä»¬ä»¤ $$z^1$$ , $$z^2$$ åˆ†åˆ«ä¸ºæ¯ä¸ªéšè—å•å…ƒçš„è¾“å‡ºã€‚$$z^i$$ è¡¨ç¤ºç¬¬ i å±‚çš„è¾“å‡ºã€‚$$h_l^i$$ å’Œ $$c_l^i$$ è¡¨ç¤ºç¬¬ i å±‚çš„æœ€åä¸€ä¸ªLSTMå•å…ƒçš„éšè—å•å…ƒçš„è¾“å‡ºå’Œè®°å¿†å•å…ƒçš„è¾“å‡ºã€‚

ä¸‹å›¾æ˜¯ä¸€ä¸ªLSTMç¼–ç çš„ä¾‹å­ã€‚å…¶ä¸­é»„è‰²æ–¹å—è¡¨ç¤ºå¯¹è¾“å…¥è¿›è¡Œ  [embedding](#embedding)  å¤„ç†ï¼Œæœ‰2å±‚ç»¿è‰²æ–¹å—ï¼Œè¡¨ç¤ºæœ‰ä¸¤å±‚LSTMç½‘ç»œï¼Œæ¯ä¸ªç»¿è‰²æ–¹å—éƒ½è¡¨ç¤ºä¸€ä¸ªLSTMå•å…ƒï¼Œçº¢è‰²æ–¹å—è¡¨ç¤ºæ¯å±‚çš„è¾“å‡ºã€‚

![seq2seq2-encoder]

åœ¨ PyTorch ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout) æ¥åˆ›å»ºä¸€ä¸ªLSTMç½‘ç»œï¼Œå…¶ä¸­ ï¼š

* emb_dimï¼šè¾“å…¥çš„ç»´åº¦ï¼Œ ä¸æ˜¯æŒ‡ä¸€å¥è¯çš„é•¿åº¦ï¼Œè€Œæ˜¯æ¯ä¸ªå•è¯  [embedding](#embedding)  ä¹‹åçš„å‘é‡çš„é•¿åº¦ã€‚
* hid_dimï¼šéšè—å•å…ƒçš„ç»´åº¦ã€‚
* n_layersï¼šç½‘ç»œçš„å±‚æ•°ï¼Œä¹Ÿæ˜¯æ·±åº¦ã€‚
* dropoutï¼šæ¯ä¸€å±‚çš„ dropoutã€‚


**Note:** éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨LSTMä¸­ï¼Œå¦‚æœæˆ‘ä»¬çš„è¾“å…¥çš„ç»´åº¦åªæœ‰1ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ä¸èƒ½ç›´æ¥ä½¿ç”¨ nn.LSTMï¼Œè€Œæ˜¯ä½¿ç”¨ nn.LSTMCellï¼Œå› ä¸ºå¦‚æœç›´æ¥ä½¿ç”¨ nn.LSTM ä¼šæœ‰ç»´åº¦è½¬æ¢çš„é—®é¢˜ã€‚

ä»£ç å¦‚ä¸‹ï¼š

````python

class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(input_dim, emb_dim)
        
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src):
        
        #src = [src len, batch size]
        
        embedded = self.dropout(self.embedding(src))
        
        #embedded = [src len, batch size, emb dim]
        
        outputs, (hidden, cell) = self.rnn(embedded)
        
        #outputs = [src len, batch size, hid dim * n directions]
        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]
        
        #outputs are always from the top hidden layer
        
        return hidden, cell

````

#### embedding

ä¸‹é¢è¿™å¼ å›¾å¾ˆå¥½çš„ä»‹ç»äº† embedding çš„è¿‡ç¨‹

![seq2seq2-Embedding]

ä¸‹é¢æ˜¯pytorchçš„embeddingæ–‡æ¡£ä¸­çš„ä¾‹å­ã€‚`nn.Embedding(10, 3)` ï¼Œå°±æ˜¯éšæœºç”Ÿæˆä¸€ä¸ª 10x3 çš„è¡¨ï¼Œç„¶åå½“è¿›è¡Œembeddingçš„æ—¶å€™ï¼Œæ¯ä¸€ä¸ªè¾“å…¥éƒ½å¯¹åº”ç€ä¸€è¡Œæ•°æ®ã€‚
```python
>>> # an Embedding module containing 10 tensors of size 3
>>> embedding = nn.Embedding(10, 3)
>>> # a batch of 2 samples of 4 indices each
>>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
>>> embedding(input)
tensor([[[-0.0251, -1.6902,  0.7172],
         [-0.6431,  0.0748,  0.6969],
         [ 1.4970,  1.3448, -0.9685],
         [-0.3677, -2.7265, -0.1685]],

        [[ 1.4970,  1.3448, -0.9685],
         [ 0.4362, -0.4004,  0.9400],
         [-0.6431,  0.0748,  0.6969],
         [ 0.9124, -2.3616,  1.1151]]])
```


### Decoder

æˆ‘ä»¬å†æ¥æ„é€ æˆ‘ä»¬çš„ decoder æ¨¡å‹

decoder æ¨¡å‹æˆ‘ä»¬ä¹Ÿæ˜¯ä½¿ç”¨2å±‚ LSTM ï¼ŒåŸè®ºæ–‡æ˜¯4å±‚ã€‚ ç½‘ç»œç»“æ„è·Ÿ encoder éå¸¸ç›¸ä¼¼ï¼Œæ˜¯ä¸è¿‡è¿™é‡Œçš„ $$h_0$$ å’Œ $$c_0$$ å˜æˆäº† encoder ä¸­çš„ $$z^1$$ , $$z^2$$ ã€‚

ä¸‹å›¾å±•ç¤ºçš„æ˜¯æˆ‘ä»¬çš„decoderçš„ç»“æ„å›¾

![seq2seq2-decoder]

åœ¨æœ€åæˆ‘ä»¬å°†è¾“å‡ºä¼ å…¥åˆ°ä¸€ä¸ªå…¨è¿æ¥ç½‘ç»œä¸­ï¼Œå¾—åˆ°æˆ‘ä»¬çš„è¾“å‡ºã€‚


æ¥ä¸‹æ¥æ˜¯ decoder çš„ä»£ç å®ç°ï¼š

````python

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        
        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)
        
        self.fc_out = nn.Linear(hid_dim, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden, cell):
        
        #input = [batch size]
        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]
        
        #n directions in the decoder will both always be 1, therefore:
        #hidden = [n layers, batch size, hid dim]
        #context = [n layers, batch size, hid dim]
        
        input = input.unsqueeze(0)
        
        #input = [1, batch size]
        
        embedded = self.dropout(self.embedding(input))
        
        #embedded = [1, batch size, emb dim]
                
        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        
        #output = [seq len, batch size, hid dim * n directions]
        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]
        
        #seq len and n directions will always be 1 in the decoder, therefore:
        #output = [1, batch size, hid dim]
        #hidden = [n layers, batch size, hid dim]
        #cell = [n layers, batch size, hid dim]
        
        prediction = self.fc_out(output.squeeze(0))
        
        #prediction = [batch size, output dim]
        
        return prediction, hidden, cell

````

### Seq2Seq

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„ Seq2Seq æ•´ä½“çš„ç»“æ„ï¼Œåœ¨æˆ‘ä»¬çš„ Seq2Seq æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å°† Seq2Seq åˆ†æˆä¸‰éƒ¨åˆ†ï¼Œåˆ†åˆ«æ˜¯ï¼š
1. æ¥æ”¶è¾“å…¥æ•°æ®å’Œç›®æ ‡æ•°æ®ï¼Œå¹¶å°†ä»–ä»¬è¿›è¡Œé¢„å¤„ç†
- å°†è¾“å…¥æ•°æ®è¿›è¡Œ encoder ï¼Œå¾—åˆ°è¾“å…¥çš„ç‰¹å¾ã€‚
- å°†ç›®æ ‡æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒæ—¶å°† encoder ä¸­å¾—åˆ°çš„è¾“å…¥ç‰¹å¾ä½œä¸º LSTM çš„åˆè¯•å€¼ï¼Œä¸€èµ·æ”¾å…¥åˆ° decoder ç½‘ç»œä¸­ï¼Œå¾—åˆ°è¾“å‡ºã€‚

æ•´ä½“ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![Seq2Seq-model]

åœ¨ Seq2Seq æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å°† encoder çš„å±‚æ•°ä¸ decoder çš„å±‚æ•°è®¾ç½®ä¸ºä¸€æ ·ï¼Œè¿™ä¸æ˜¯å¿…é¡»çš„ï¼Œä½†æ˜¯è¿™æ ·åšèƒ½æ–¹ä¾¿æˆ‘ä»¬å¤„ç†æ¨¡å‹ã€‚

ä»£ç å¦‚ä¸‹ï¼š

```python
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must be equal!"
        assert encoder.n_layers == decoder.n_layers, \
            "Encoder and decoder must have equal number of layers!"
        
    def forward(self, src, trg, teacher_forcing_ratio = 0.5):
        
        #src = [src len, batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing
        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time
        
        batch_size = trg.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        #tensor to store decoder outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        #last hidden state of the encoder is used as the initial hidden state of the decoder
        hidden, cell = self.encoder(src)
        
        #first input to the decoder is the <sos> tokens
        input = trg[0,:]
        
        for t in range(1, trg_len):
            
            #insert input token embedding, previous hidden and previous cell states
            #receive output tensor (predictions) and new hidden and cell states
            output, hidden, cell = self.decoder(input, hidden, cell)
            
            #place predictions in a tensor holding predictions for each token
            outputs[t] = output
            
            #decide if we are going to use teacher forcing or not
            teacher_force = random.random() < teacher_forcing_ratio
            
            #get the highest predicted token from our predictions
            top1 = output.argmax(1) 
            
            #if teacher forcing, use actual next token as next input
            #if not, use predicted token
            # åˆ¤æ–­ä¸‹ä¸€ä¸ªçš„è¾“å…¥ï¼Œæ˜¯ä½¿ç”¨è®­ç»ƒé›†ä¸­çš„è¿˜æ˜¯ä½¿ç”¨ä» decoder ä¸­é¢„æµ‹çš„
            input = trg[t] if teacher_force else top1
        
        return outputs
```
ä»ä»£ç ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬å…ˆæ˜¯å°†æ•´ä¸ªæºæ•°æ®æ”¾å…¥ encoder ä¸­ï¼ˆæºæ•°æ®æŒ‡çš„æ˜¯è®­ç»ƒæ•°æ®ä¸­çš„ src æ•°æ®ï¼‰ï¼Œç„¶åæˆ‘ä»¬ä¸€ä¸ªä¸€ä¸ªçš„éå†ç›®æ ‡æ•°æ®ï¼Œ
é¦–å…ˆæˆ‘ä»¬å°† '\<sos\>' æ”¾å…¥åˆ° decoder ä¸­ï¼Œå¾—åˆ°ä¸€ä¸ªè¾“å‡ºï¼Œä¿å­˜åˆ°è¾“å‡ºçš„ç»“æœä¸­ã€‚ç„¶åæˆ‘ä»¬ä»¥ä¸€å®šçš„æ¦‚ç‡æ¥åˆ¤æ–­æ˜¯å¦è¦ä½¿ç”¨ç›®æ ‡æ•°æ®ä¸­çš„ä¸‹ä¸€ä¸ªå½“ä½œè¾“å…¥ï¼Œ
ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬çš„decoderçš„è¾“å…¥ä¸ä¸€å®šå…¨æ˜¯ç›®æ ‡æ•°æ®ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„è®­ç»ƒä»£ç 

```python

INPUT_DIM = len(SRC.vocab)
OUTPUT_DIM = len(TRG.vocab)
ENC_EMB_DIM = 256
DEC_EMB_DIM = 256
HID_DIM = 512
N_LAYERS = 2
ENC_DROPOUT = 0.5
DEC_DROPOUT = 0.5

enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)

model = Seq2Seq(enc, dec, device).to(device)


def init_weights(m):
    for name, param in m.named_parameters():
        nn.init.uniform_(param.data, -0.08, 0.08)


model.apply(init_weights)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


print(f'The model has {count_parameters(model):,} trainable parameters')

optimizer = optim.Adam(model.parameters())

TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)


def train(model, iterator, optimizer, criterion, clip):
    model.train()

    epoch_loss = 0

    for i, batch in enumerate(iterator):
        src = batch.src
        trg = batch.trg

        optimizer.zero_grad()

        output = model(src, trg)

        # trg = [trg len, batch size]
        # output = [trg len, batch size, output dim]

        output_dim = output.shape[-1]

        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)

        # trg = [(trg len - 1) * batch size]
        # output = [(trg len - 1) * batch size, output dim]

        loss = criterion(output, trg)

        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

        optimizer.step()

        epoch_loss += loss.item()

    return epoch_loss / len(iterator)


```
 
æˆ‘ä»¬ä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼ŒCrossEntropyLoss æŸå¤±å‡½æ•°ï¼Œåœ¨ CrossEntropyLoss ä¸­æˆ‘ä»¬ä¸è®¡ç®—ä¸ºäº†ä¿æŒbatchä¸­æ ·æœ¬é•¿åº¦ä¸€è‡´è€Œå¡«å……çš„éƒ¨åˆ†ã€‚
è®­ç»ƒä¸­è¿˜ä½¿ç”¨äº† torch.nn.utils.clip_grad_norm_ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æŒ‡çš„æ˜¯å°†æ¯ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œåå‘ä¼ æ’­æ—¶å€™çš„æ¢¯åº¦è¿›è¡Œå½’ä¸€åŒ–å¹¶é™åˆ¶ä½gradï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±ã€‚

æ¥ä¸‹æ¥å°±æ˜¯å¼€å§‹æˆ‘ä»¬çš„è®­ç»ƒäº†ã€‚

è®­ç»ƒäº†20ä¸ªepocheä¹‹åï¼Œæˆ‘ä»¬æ¥è¿›è¡Œæµ‹è¯•ä»¥ä¸‹

### æµ‹è¯•

æµ‹è¯•è¿‡ç¨‹ä¸ºï¼š
- æˆ‘ä»¬å°†æˆ‘ä»¬çš„è¦ç¿»è¯‘çš„æºæ•°æ®å…ˆè¿›è¡Œencoderè®¡ç®—ï¼Œå¾—åˆ° hidden, cell ï¼Œ
- ç„¶åæˆ‘ä»¬å°†å¼€å§‹æè¿°ç¬¦ `\<sos\>` å½“ä½œè¾“å…¥ï¼Œè·Ÿ hidden, cell ä¸€èµ·æ”¾å…¥åˆ°decoderä¸­å»ï¼Œè¿™ä¸ªæ—¶å€™å¾—åˆ°ä¸€ä¸ªè¾“å‡ºå’Œæ–°çš„ hidden, cell ã€‚ 
- æˆ‘ä»¬å°†è¾“å‡ºä¿å­˜ä¸‹æ¥ï¼Œç„¶åå°†è¯¥è¾“å‡ºä¸æ–°çš„ hidden, cell ä¸€èµ·å½“ä½œè¾“å…¥æ”¾å…¥åˆ°decoderä¸­å»ï¼Œå¦‚æ­¤å¾ªç¯ï¼Œå°±å¯ä»¥å¾—åˆ°æˆ‘ä»¬çš„ç¿»è¯‘è¯­å¥äº†ã€‚

åœ¨æµ‹è¯•ä¸­ï¼Œé€šè¿‡è®¾ç½®max-lenï¼Œä¹Ÿå°±æ˜¯è¯´è¾“å‡ºçš„æœ€å¤§é•¿åº¦ï¼Œä»è€Œæ¥å†³å®šè¾“å‡ºçš„å¥å­çš„é•¿åº¦ã€‚å½“decoderä¸­å¾—åˆ°çš„ç»“æŸç¬¦å°±åœæ­¢ï¼Œå¦åˆ™ç›´åˆ°max-lenç»“æŸã€‚

æœ€ç»ˆç»“æœï¼š`| Test Loss: 3.943 | Test PPL:  51.571 |`

### GRU çš„ç®€å•ä»‹ç»

åœ¨ [2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb) ä¸­
ä½œè€…è¿˜åšäº†ä½¿ç”¨ GRU (Gated Recurrent Unit) æ¥æ›¿ä»£ LSTM è¿›è¡Œæœºå™¨ç¿»è¯‘çš„è®­ç»ƒï¼Œä½¿ç”¨äº†æ›´å¤šçš„å‚æ•°ï¼Œæœ‰æ›´å¥½çš„æ•ˆæœã€‚ä¸è¿‡å…¶å®æœ‰äººåšè¿‡å®éªŒï¼Œå‘ç°å…¶å® GRU ä¸ LSTM æ€§èƒ½å‡ ä¹æ˜¯å·®ä¸å¤šçš„ [è®ºæ–‡é“¾æ¥](https://arxiv.org/abs/1412.3555) åœ¨æ­¤ã€‚

ä¸‹å›¾æ˜¯ GRU çš„encoderç»“æ„å›¾ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯ä¸€å±‚ç½‘ç»œ

![gru-encoder]

ä»å›¾ä¸Šæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå‡ ä¹ä¸LSTMä¸€æ ·ï¼Œéœ€è¦æé†’çš„æ˜¯æ¯ä¸€ä¸ªç»¿è‰²çš„æ–¹å—éƒ½ä»£è¡¨ä¸€æ¬¡GRUæ“ä½œï¼Œæ¯æ¬¡GRUéƒ½æ˜¯ä¸€æ ·çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸Šå›¾æ˜¯ä¸€ä¸ªå•å±‚ï¼Œå•ä¸ªGRUçš„æ¨¡å‹ï¼Œ
å³ '\<sos\>' å…ˆè¿›å…¥ GRUï¼Œè¿ç®—åå¾—åˆ°è¾“å‡ºï¼Œå† guten è¿›å…¥ GRUï¼Œè¿˜æ˜¯ä¹‹å‰çš„é‚£ä¸ª GRUï¼Œåªæ˜¯è¾“å…¥å‚æ•°ä¸ä¸€æ ·äº†ï¼ŒGRU é‡Œé¢çš„å‚æ•°æ­¤æ—¶æ˜¯ä¸€æ ·çš„ã€‚

ä¸‹å›¾æ˜¯ GRU çš„decoderç»“æ„å›¾ï¼Œæ­¤å¤„æˆ‘ä»¬å¯¹decoderç¨ä½œä¿®æ”¹ï¼Œèƒ½è®©ç½‘ç»œæœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œå…·ä½“åšæ³•æ˜¯å°† z æ‹¼æ¥åˆ°decoderçš„æ¯ä¸€æ¬¡è¿ç®—ä¸­ï¼Œå†å°† decoder ä¸­è¿
è¡Œçš„ç»“æœéƒ½æ‹¼æ¥åˆ°åé¢ï¼Œä¸€èµ·å½“ä½œè¾“å…¥ã€‚å…¶ä½™çš„è¿‡ç¨‹è·Ÿè·Ÿ LSTM å¾ˆç›¸ä¼¼ã€‚æ•ˆæœå…¥ä¸‹å›¾ã€‚

![gru-decoder]

å…¶ä¸­ç´«è‰²çš„æ–¹å—è¡¨ç¤ºå…¨è¿æ¥ã€‚decoderä¸ä¹‹å‰çš„LSTMç»“æ„çš„decoderçš„è¿ç»“æ–¹å¼ä¸ä¸€æ ·ï¼Œåœ¨ GRU ä¸­ encoder çš„è¾“å‡ºä¼šè¢«ä½¿ç”¨åˆ°æ¯ä¸€ä¸ª decoder çš„èŠ‚ç‚¹ä¸­æ¥ã€‚

ä¸‹å›¾æ˜¯ä½¿ç”¨ GRU æ¨¡å‹çš„seq2seqæ¨¡å‹

![seq2seq-with-gru]

æ•´ä¸ªç»“æ„ä¸ä½¿ç”¨ LSTM ç»“æ„çš„seq2seq æ¨¡å‹å¹¶æ— å¤ªå¤§çš„å·®åˆ«ï¼Œæ­¤å¤„å°±ä¸è¿‡å¤šä»‹ç»ã€‚åœ¨ [2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb) ä¸­æœ‰è¯¦ç»†çš„ä»£ç å®ç°ã€‚


# Align ä»‹ç»

åœ¨å‰é¢æˆ‘ä»¬ä»‹ç»äº† LSTM å’Œ GRU æ¨¡å‹ï¼Œä»–ä»¬åœ¨å¤„ç†è¾“å…¥çš„æ—¶å€™ï¼Œéƒ½æ˜¯å°†ä¸€å¥è¯ä»å¤´åˆ°å°¾éƒ½ç»è¿‡ä¸€æ¬¡ç¥ç»ç½‘ç»œï¼Œåœ¨ [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) è®ºæ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†åŒå‘çš„ç½‘ç»œæ¨¡å‹ï¼Œå°±æ˜¯è¯´è®©æˆ‘ä»¬çš„è¾“å…¥å…ˆä»å¤´åˆ°å°¾è¿›å…¥ä¸€ä¸ªç½‘ç»œï¼Œç„¶åå†ä»å°¾åˆ°å¤´ç»è¿‡å¦ä¸€ä¸ªç½‘ç»œï¼Œå³åŒå‘ RNN ï¼Œè¿™æ ·æˆ‘ä»¬å°±æœ‰äº†ä¸¤ä¸ªè¾“å‡ºã€‚
å…·ä½“ç»“æ„å¦‚ä¸‹ï¼š

![bidirectional-rnn]

å¯ä»¥ç”¨æ•°å­¦è¡¨è¾¾ä¸ºï¼š

$$h_t^{\to} = EncoderGRU^{\to}(e(x_t^{\to}) , h_{t-1}^{\to})$$

$$h_t^{\gets} = EncoderGRU^{\gets}(e(x_t^{\gets}) , h_{t-1}^{\gets})$$

å…¶ä¸­ $$x_0^{\to}=<sos> , x_1^{\to}=guten$$ ï¼Œ$$x_0{^\gets}=<eos> , x_1^{\gets}=morgen$$ 

æˆ‘ä»¬å†æ¥çœ‹çœ‹è®ºæ–‡ä¸­å…³äºalignçš„ä»‹ç»å¦‚ä¸‹å›¾

![align]

åœ¨ decoder ä¸­ï¼Œè¾“å…¥åŒ…å«äº† ä¸€ä¸ªåŒå‘çš„ encoder çš„æ‹¼æ¥ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€å¥è¯ä¸­çš„æŸä¸ªå•è¯çš„é¢„æµ‹ç¿»è¯‘ï¼Œä¼šåŠ ä¸Šæ‰€æœ‰çš„è¾“å…¥çš„ä¿¡æ¯ã€‚ä½†æ˜¯å®é™…ä¸­æˆ‘ä»¬å¾€å¾€ä¸éœ€è¦å…¨éƒ¨çš„ä¿¡æ¯ï¼Œ
æ‰€æœ‰è¿™é‡Œå°±å¼•å…¥äº†Attentionæœºåˆ¶ï¼Œä½¿å¾—åœ¨æ‹¼æ¥çš„æ—¶å€™æœ‰é€‰æ‹©æ€§çš„æ‹¼æ¥ã€‚ä¹Ÿå°±æ˜¯è¯´æŸä¸ªå•è¯çš„é¢„æµ‹ç¿»è¯‘ï¼Œä¸è¾“å…¥ä¸­çš„æŸå‡ ä¸ªå•è¯æœ‰å…³ï¼Œè‡³äºæ˜¯å“ªå‡ ä¸ªï¼Œåˆ™æ˜¯é€šè¿‡ç½‘ç»œæ¥å­¦ä¹ çš„ã€‚

# Attention ä»‹ç»

Attention ä¹Ÿå«æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŸç†å°±æ˜¯æ¥å—è¾“å…¥ï¼Œç„¶åè¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œè¯¥å‘é‡æ‰€æœ‰çš„å€¼éƒ½æ˜¯[0ï¼Œ1]ä¹‹é—´çš„æ•°ï¼Œå‘é‡çš„å’Œä¸º1ï¼Œé€šå¸¸çš„åšæ³•æ˜¯é€šè¿‡æœ€åä¸€å±‚ç½‘ç»œåï¼ŒåŠ ä¸Šä¸€ä¸ªSoftMaxæ¿€æ´»å‡½æ•°ã€‚

ä¸‹å›¾æ˜¯ä¸€ä¸ªRNNä¸­è®¡ç®— Attention çš„ä¸€ç§æ–¹å¼ï¼š

![rnn-attention-encoder]

ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œzä¸º rnn çš„è¾“å‡ºï¼Œ$$h_1 $$ ~ $$ h_4$$ä¸ºæ¯ä¸ªè¾“å…¥çš„éšè—å•å…ƒï¼Œæˆ‘ä»¬å°† z ä¸ $$h_1 $$ ~ $$ h_4$$ ä¸€èµ·æ”¾è¿›ä¸€ä¸ªç¥ç»ç½‘ç»œä¸­ï¼Œå¾—åˆ° Attention aï¼Œè¯¥ç¥ç»ç½‘ç»œé€šå¸¸é€‰æ‹©ä¸ºå…¨è¿æ¥ã€‚

ä¸‹å›¾æ˜¯ decoderéƒ¨åˆ†ï¼Œ

![rnn-attention-arcitecture]

åœ¨æˆ‘ä»¬æ²¡æœ‰ Attention æœºåˆ¶çš„ RNN ä¸­ï¼Œencoder çš„è¾“å‡º z æ˜¯è¦å‚ä¸åˆ° decoder çš„æ‰€æœ‰æ“ä½œä¸­çš„ã€‚è€Œåœ¨å¸¦æœ‰ Attention æœºåˆ¶çš„ RNN ä¸­ï¼Œz æ­¤æ—¶åªæ˜¯å‚ä¸åˆ° GRU ä¸­å½“ä½œè¾“å…¥ï¼Œ
æ­¤æ—¶è¡¨ç¤º Attention çš„ a å‚ä¸åˆ° decoder çš„æ‰€æœ‰æ“ä½œä¸­çš„ï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„ GRUï¼Œå¸¦ Attention æœºåˆ¶çš„ RNN èƒ½å¤Ÿæºå¸¦ä¸Šåœ¨ encoder ä¸­çš„æ¯ä¸€æ¬¡è®¡ç®—çš„éšè—å•å…ƒ hï¼Œèƒ½å¤ŸæŠŠèƒ½å¤šçš„ä¿¡æ¯ä¼ é€’åˆ° decoder ä¸­ã€‚

åœ¨ decoder ä¸­æˆ‘ä»¬éœ€è¦å…³æ³¨ä¸¤ä¸ªåœ°æ–¹ï¼Œ1ï¼šæ˜¯ w æ˜¯å¦‚ä½•å‚ä¸è“è‰²æ–¹æ¡†çš„è¿ç®—ï¼Œ2ï¼šw æ˜¯å¦‚ä½•å‚ä¸åˆ°ç´«è‰²æ–¹æ¡†çš„è¿ç®—ã€‚ä»ä»£ç ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ

```python
class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):
        super().__init__()

        self.output_dim = output_dim
        self.attention = attention
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        
        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)
        
        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden, encoder_outputs):
             
        #input = [batch size]
        #hidden = [batch size, dec hid dim]
        #encoder_outputs = [src len, batch size, enc hid dim * 2]
        
        input = input.unsqueeze(0)
        
        #input = [1, batch size]
        
        embedded = self.dropout(self.embedding(input))
        
        #embedded = [1, batch size, emb dim]
        
        a = self.attention(hidden, encoder_outputs)
                
        #a = [batch size, src len]
        
        a = a.unsqueeze(1)
        
        #a = [batch size, 1, src len]
        
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        
        #encoder_outputs = [batch size, src len, enc hid dim * 2]
        
        weighted = torch.bmm(a, encoder_outputs)
        
        #weighted = [batch size, 1, enc hid dim * 2]
        
        weighted = weighted.permute(1, 0, 2)
        
        #weighted = [1, batch size, enc hid dim * 2]
        
        rnn_input = torch.cat((embedded, weighted), dim = 2)
        
        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]
            
        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))
        
        #output = [seq len, batch size, dec hid dim * n directions]
        #hidden = [n layers * n directions, batch size, dec hid dim]
        
        #seq len, n layers and n directions will always be 1 in this decoder, therefore:
        #output = [1, batch size, dec hid dim]
        #hidden = [1, batch size, dec hid dim]
        #this also means that output == hidden
        assert (output == hidden).all()
        
        embedded = embedded.squeeze(0)
        output = output.squeeze(0)
        weighted = weighted.squeeze(0)
        
        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))
        
        #prediction = [batch size, output dim]
        
        return prediction, hidden.squeeze(0)
```

inputæ˜¯decoderçš„è¾“å…¥, hiddenæ˜¯encoderè¾“å‡ºçš„éšè—å•å…ƒ, encoder_outputsæ˜¯encoderçš„è¾“å‡ºï¼Œ`a = self.attention(hidden, encoder_outputs)` å°±æ˜¯è®¡ç®—Attentionï¼Œ
`weighted = torch.bmm(a, encoder_outputs)` å°†Attentionä¸encoder_outputsè¿›è¡ŒçŸ©é˜µç›¸ä¹˜å¾—åˆ°weightedï¼Œç„¶åå°†weightedæ‹¼è£…è¿›inputç¼–ç ä¹‹åçš„çŸ©é˜µembeddedä¸­ï¼Œ
äºæ˜¯çš„åˆ°äº†æ–°çš„ decoderçš„è¾“å…¥ï¼Œå°†è¯¥æ–°è¾“å…¥æ”¾å…¥åˆ°GRUä¸­å»è®¡ç®—å¾—åˆ°outputã€‚æœ€å`prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))` å°†
'output', 'weighted', 'embedded' ä¸€èµ·æ‹¼è£…æ”¾å…¥åˆ°ä¸€ä¸ªå…¨è¿æ¥ä¸­ï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹å€¼ã€‚

# æå‡å‡†ç¡®ç‡

## packed padded sequences

æˆ‘ä»¬ç”¨ Packed padded sequences æ¥å‘Šè¯‰æˆ‘ä»¬çš„RNNç½‘ç»œï¼Œå¿½ç•¥æ‰ encoder ä¸­ä¸ºäº†å¯¹é½batchè€Œæ·»åŠ  padding çš„éƒ¨åˆ†ã€‚
å…·ä½“çš„åšæ³•å¦‚ä¸‹ï¼š
```python
SRC = Field(tokenize = tokenize_de, 
            init_token = '<sos>', 
            eos_token = '<eos>', 
            lower = True, 
            include_lengths = True)

TRG = Field(tokenize = tokenize_en, 
            init_token = '<sos>', 
            eos_token = '<eos>', 
            lower = True)

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, valid_data, test_data), 
     batch_size = BATCH_SIZE,
     sort_within_batch = True,
     sort_key = lambda x : len(x.src),
     device = device)
```
å…¶ä¸­ `include_lengths = True` è¡¨ç¤ºå°†æ¥åœ¨æˆ‘ä»¬çš„ batch.src ä¸­å°†ä¸å†è¿›è¡Œ padding æ“ä½œï¼Œè¿˜è¦è®°å½•æ¯ä¸ªå¥å­çš„é•¿åº¦ã€‚ `sort_within_batch = Trueï¼Œsort_key = lambda x : len(x.src),` è¡¨ç¤ºå°† batch ä¸­çš„æ ·æœ¬æŒ‰ç…§ len(x.src) è¾“å…¥çš„é•¿åº¦æ’åºï¼Œç¬¬ä¸€ä¸ªæ˜¯æœ€é•¿çš„ã€‚
è¿™æ ·æˆ‘ä»¬çš„è®­ç»ƒæ ·æœ¬çš„ src å°±ä¸å†æ˜¯ä¸€ä¸ªtensoräº†ï¼Œè€Œæ˜¯ä¸€ä¸ª tuple ï¼Œé‡Œé¢çš„å…ƒç´ çš„é•¿åº¦éƒ½ä¸ä¸€æ ·ï¼Œç¬¬ä¸€ä¸ªæœ€é•¿ï¼Œç„¶åä¾æ¬¡é€’å‡ã€‚
æ—¢ç„¶æˆ‘ä»¬çš„æ¨¡å‹éœ€è¦æ¥æ”¶tensorï¼Œæ‰€ä»¥å°±éœ€è¦æ¥å¯¹encoderåšä¸€äº›ä¿®æ”¹ã€‚ä»¥ä¸‹æ˜¯encoderçš„ä¸»è¦ä»£ç ï¼š
```python
def forward(self, src, src_len):
    # src = [src len, batch size]
    # src_len = [batch size]

    embedded = self.dropout(self.embedding(src))

    # embedded = [src len, batch size, emb dim]

    # need to explicitly put lengths on cpu!
    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))

    packed_outputs, hidden = self.rnn(packed_embedded)

    # packed_outputs is a packed sequence containing all hidden states
    # hidden is now from the final non-padded element in the batch

    outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)

    # outputs is now a non-packed sequence, all hidden states obtained
    #  when the input is a pad token are all zeros

    # outputs = [src len, batch size, hid dim * num directions]
    # hidden = [n layers * num directions, batch size, hid dim]

    # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]
    # outputs are always from the last layer

    # hidden [-2, :, : ] is the last of the forwards RNN
    # hidden [-1, :, : ] is the last of the backwards RNN

    # initial decoder hidden is final hidden state of the forwards and backwards
    #  encoder RNNs fed through a linear layer
    hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)))

    # outputs = [src len, batch size, enc hid dim * 2]
    # hidden = [batch size, dec hid dim]

    return outputs, hidden
```
ä»£ç ä¸­çš„ `nn.utils.rnn.pack_padded_sequence` æ˜¯ç”¨æ¥æ‰“åŒ…æˆ‘ä»¬çš„å¥å­ï¼Œä»è€Œå¯ä»¥æ”¾å…¥åˆ°rnnä¸­å»ã€‚é¦–å…ˆæ¯”å¦‚ `nn.utils.rnn.pack_padded_sequence` çš„è¾“å…¥çš„ç»´åº¦æ˜¯ [19, 128, 256]ï¼Œ
19è¡¨ç¤ºæœ€é•¿çš„å¥å­çš„é•¿åº¦ï¼Œ128æ˜¯batchï¼Œ256è¡¨ç¤ºæ¯ä¸ªå•è¯çš„ç¼–ç ï¼Œæ­¤æ—¶çš„ embedded æ˜¯æœ‰ padding çš„ã€‚è¿˜æœ‰ä¸€ä¸ªè¾“å…¥æ˜¯ `src_len.to('cpu')` ï¼Œè¡¨ç¤ºæ¯ä¸€ä¸ªå¥å­çš„é•¿åº¦ï¼Œ `nn.utils.rnn.pack_padded_sequence` çš„è¾“å‡ºæ˜¯
å°† embedded ä¸­çš„å¥å­å–å‡ºæ¥ï¼Œå–å¥å­çš„æ–¹å¼å¦‚ä¸‹å›¾æ‰€ç¤ºï¼šå›¾åƒæ¥è‡ª[stackoverflow](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch) 
![pack_padded_sequence]
è¿™ä¸ªæ—¶å€™å¥å­ä¸­å°±æ²¡æœ‰äº† padding äº†ã€‚

å¯èƒ½æœ‰äººä¼šé—®ä¸ºä»€ä¹ˆ packed_embedded ä¸­çš„å†…å®¹æ˜¯äº¤æ›¿ç€ä» embedded çš„æ¯ä¸ª batch ä¸­å–ï¼Œè€Œä¸æ˜¯æ¯ä¸ªæ¯ä¸ªå¥å­çš„å–ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„ç²—æµ…çš„ç†è§£ï¼Œå¦‚æœé”™äº†ï¼Œæœ‰äººçœ‹åˆ°äº†è¯è¯·æäº¤issueï¼Œæˆ‘ä»¥åä¹Ÿä¼šæŒç»­å…³æ³¨è¿™ä¸ªé—®é¢˜ï¼ŒåŠ¨æ€æ›´æ–°ã€‚

æˆ‘ä»¬å¯ä»¥å°†ç¥ç»ç½‘ç»œçš„è¿ç®—æƒ³è±¡æˆä¸ºçŸ©é˜µçš„ä¹˜æ³•ã€‚

æ¯”å¦‚batchä¸º1çš„ä¸€ä¸ªæ ·æœ¬ï¼Œä¾‹å¦‚æ˜¯ä¸€ä¸ªå¥å­ï¼Œå¥å­é•¿åº¦ä¸º10ï¼Œåœ¨å…¨è¿æ¥ç½‘ç»œä¸­ï¼Œå¦‚æœè¾“å‡ºæ˜¯ä¸€ä¸ªæ•°çš„è¯ï¼Œæ•´ä¸ªç½‘ç»œæˆ‘ä»¬å¯ä»¥æè¿°å‡ºå¦‚ä¸‹ï¼š

$$
[i_1,i_2,i_3,\dots,i_{10}] \times \begin{bmatrix}*  \\* \\ \vdots  \\ * \end{bmatrix}_{10\times 1} = O_{1\times1}
$$

å…¶ä¸­ $$ [i_1,i_2,i_3,\dots,i_{10}] $$è¡¨ç¤ºè¾“å…¥ï¼Œ$$\begin{bmatrix}*  \\* \\ \vdots  \\ * \end{bmatrix}_{10\times 1} $$è¡¨ç¤ºç¥ç»ç½‘ç»œï¼Œ$$O_{1\times 1}$$ è¡¨ç¤ºè¾“å‡ºã€‚

å¦‚æœæ˜¯åœ¨å¾ªç¯ç¥ç»ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬å…ˆçœ‹çœ‹å¾ªç¯ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œï¼Œä¸‹å›¾æ˜¯ä¸€ä¸ªç®€å•çš„rnnçš„æµç¨‹å›¾è§£

![2021-11-04_seq2seq_3]

æ‰€ä»¥å¾ªç¯ç¥ç»ç½‘ç»œçš„è¿ç®—è¿‡ç¨‹å¯ä»¥è¡¨ç¤ºæˆ

$$
i_1     \times    rnn = o_1 \\
\phi(o_1,i_2) \times    rnn = o_2 \\
\phi(o_2,i_3) \times    rnn = o_3 \\
\vdots \\
\phi(o_9,i_10) \times   rnn = O \\
$$

æ­¤æ—¶å°† batch æ¢æˆ n çš„è¯ï¼Œå¦‚æœæ ·æœ¬æ˜¯å¯¹é½çš„ã€‚åˆ™æ‰€ä»¥å¾ªç¯ç¥ç»ç½‘ç»œçš„è¿ç®—è¿‡ç¨‹å¯ä»¥è¡¨ç¤ºæˆ

$$
(i_{11},i_{21},...,i_{n1})     \times    rnn = (o_{11},o_{21},o_{31},...,o_{n1}) = O_1 \\
\phi(O_1,(i_{12},i_{22},...,i_{n2})) \times    rnn = (o_{12},o_{22},o_{32},...,o_{n2}) = O_2 \\
\phi(O_2,(i_{13},i_{23},...,i_{n3})) \times    rnn = (o_{13},o_{23},o_{33},...,o_{n3}) = O_3 \\
\vdots \\
\phi(O_9,(i_{19},i_{29},...,i_{n9})) \times   rnn = (o_{19},o_{29},o_{39},...,o_{n9}) = O_9 \\
$$

æ¯ä¸€æ¬¡ä¸ rnn ç›¸ä¹˜çš„éƒ½æ˜¯batchä¸­ç›¸åº”ä½ç½®çš„å•ä½æ•°æ®ã€‚

æ‰€ä»¥åœ¨è¿›è¡Œ `nn.utils.rnn.pack_padded_sequence` ä¹‹åï¼Œè¾“å‡ºéƒ½æ˜¯æ¯ä¸ªæ ·æœ¬äº¤æ›¿æ‹¼æ¥è€Œæˆçš„å‘é‡ï¼Œç”±äºæ•°æ®ä¸æ˜¯å¯¹é½çš„ï¼Œæ‰€ä»¥è¾“å‡ºåªèƒ½æ˜¯å‘é‡ï¼Œè€Œä¸èƒ½æ˜¯çŸ©é˜µã€‚


## masking

Masking æ˜¯ç›´æ¥ä½œç”¨äºç½‘ç»œè®©å®ƒç›´æ¥å¿½ç•¥æ‰æŸäº›ç¡®å®šçš„å€¼ï¼Œä¾‹å¦‚è®©æ¨¡å‹ä¸å°† Attention ä½¿ç”¨åˆ° padding ä¸Šã€‚
ä¾‹å¦‚è¾“å…¥æ˜¯ ["hello", "how", "are", "you", "?", <pad>, <pad>] é‚£ä¹ˆ masking å°±æ˜¯ [1, 1, 1, 1, 1, 0, 0] ã€‚ä¸‹é¢æ˜¯å¸¦ masking çš„ Attention çš„ä»£ç ã€‚
```python
class Attention(nn.Module):
    def __init__(self, enc_hid_dim, dec_hid_dim):
        super().__init__()
        
        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)
        self.v = nn.Linear(dec_hid_dim, 1, bias = False)
        
    def forward(self, hidden, encoder_outputs, mask):
        
        #hidden = [batch size, dec hid dim]
        #encoder_outputs = [src len, batch size, enc hid dim * 2]
        
        src_len = encoder_outputs.shape[0]
        
        #repeat decoder hidden state src_len times
        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
  
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        
        #hidden = [batch size, src len, dec hid dim]
        #encoder_outputs = [batch size, src len, enc hid dim * 2]
        
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) 
        
        #energy = [batch size, src len, dec hid dim]

        attention = self.v(energy).squeeze(2)
        
        #attention = [batch size, src len]
        
        # attention.masked_fill(mask == 0, -1e10) æŒ‡çš„æ˜¯å°† attention ä¸ mask ç›¸ä¹˜ï¼Œç„¶åä¸º0çš„åœ°æ–¹æ¢æˆ-1e10ï¼Œ
        # ä¸º -1e10 çš„åœ°æ–¹ä¼šåœ¨ softmax ä¹‹åå˜æˆ0
        attention = attention.masked_fill(mask == 0, -1e10)
        
        return F.softmax(attention, dim = 1)
```
é€šè¿‡ä»£ç æˆ‘ä»¬å¯ä»¥çœ‹åˆ° mask é€šè¿‡å‚æ•°ä¼ å…¥åˆ° Attention ä¸­ï¼Œç„¶ååœ¨æœ€åè®¡ç®— softmax ä¹‹å‰å°†å¾—åˆ°çš„ attention ä¸ mask ç›¸ä¹˜ã€‚

# BLEU ä»‹ç»

åœ¨è‡ªç„¶è¯­è¨€ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œä¸€ç§å¸¸ç”¨çš„è¯„ä»·æ–¹æ³•å°±æ˜¯ä½¿ç”¨blueï¼Œè¯¥æ–¹æ³•ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯è®¡ç®—é¢„æµ‹çš„å¥å­çš„ n-grams ä¸å®é™…çš„å¥å­çš„ n-grams æœ‰å¤šå°‘å•è¯æ˜¯é‡åˆçš„ã€‚æ¯”å¦‚Aä¸ºæˆ‘ä»¬é¢„æµ‹çš„å¥å­çš„ n-gramsï¼ŒBä¸ºå®é™…ç¿»è¯‘å‡ºæ¥çš„å¥å­çš„ n-gramsï¼Œ
é‚£ä¹ˆ$$blue = \frac{A \cap B}{B}$$


æš‚æ—¶å®Œç»“ âœ¨â­ âœ¨â­ âœ¨â­ ã€‚

æœ¬æ–‡ä¸»è¦è®²è¿°çš„æ˜¯ä½¿ç”¨ rnn æ¥è¿›è¡Œè‡ªç„¶è¯­è¨€çš„ç¿»è¯‘ï¼Œæˆ‘ä»¬å°†ä¼šåœ¨[ä¸‹ä¸€ç¯‡ blog](https://7568.github.io/2021/11/03/cnn-seq2seqModel) ä¸­æ¥è®²è¿°ä½¿ç”¨ cnn æ¥è¿›è¡Œè‡ªç„¶è¯­è¨€çš„ç¹è‚²

# ä»£ç ä¸‹è½½

ä»[bentrevett / pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq) ä¸­æå–å‡ºçš„ä»£ç å¦‚ä¸‹ï¼š

ğŸ‘‰ï¸ ğŸ‘‰ï¸ ğŸ‘‰ï¸ ç‚¹å‡»[ ğŸ’ ğŸ’ ğŸ’ å¯ä»¥ç›´æ¥ä¸‹è½½ä½¿ç”¨ LSTM ç»“æ„çš„seq2seq æ¨¡å‹çš„ä»£ç ](https://7568.github.io/codes/text-process/2021-11-03-seq2seqModel-lstm.py)ã€‚å°†ä»£ç ä¸­ `is_train = False` æ”¹æˆ `is_train = True` å°±å¯ä»¥è®­ç»ƒäº†ï¼Œæµ‹è¯•çš„æ—¶å€™å†æ”¹å›æ¥å³å¯ã€‚

ğŸ‘‰ï¸ ğŸ‘‰ï¸ ğŸ‘‰ï¸ ç‚¹å‡»[ ğŸ’ ğŸ’ ğŸ’ å¯ä»¥ç›´æ¥ä¸‹è½½ä½¿ç”¨ GRU ç»“æ„çš„seq2seq æ¨¡å‹çš„ä»£ç ](https://7568.github.io/codes/text-process/2021-11-03-seq2seqModel-gru.py)ã€‚å°†ä»£ç ä¸­ `is_train = False` æ”¹æˆ `is_train = True` å°±å¯ä»¥è®­ç»ƒäº†ï¼Œæµ‹è¯•çš„æ—¶å€™å†æ”¹å›æ¥å³å¯ã€‚

ğŸ‘‰ï¸ ğŸ‘‰ï¸ ğŸ‘‰ï¸ ç‚¹å‡»[ ğŸ’ ğŸ’ ğŸ’ å¯ä»¥ç›´æ¥ä¸‹è½½ä½¿ç”¨ Attention ç»“æ„çš„seq2seq æ¨¡å‹çš„ä»£ç ](https://7568.github.io/codes/text-process/2021-11-03-seq2seqModel-attention.py)ã€‚å°†ä»£ç ä¸­ `is_train = False` æ”¹æˆ `is_train = True` å°±å¯ä»¥è®­ç»ƒäº†ï¼Œæµ‹è¯•çš„æ—¶å€™å†æ”¹å›æ¥å³å¯ã€‚

ğŸ‘‰ï¸ ğŸ‘‰ï¸ ğŸ‘‰ï¸ ç‚¹å‡»[ ğŸ’ ğŸ’ ğŸ’ å¯ä»¥ç›´æ¥ä¸‹è½½ä½¿ç”¨ Packed Padded Sequences ï¼ŒAttention ï¼ŒMasking ç»“æ„çš„seq2seqï¼Œå¹¶ç”¨ BLEU è¯„ä»·æ¨¡å‹çš„ä»£ç ](https://7568.github.io/codes/text-process/2021-11-13-seq2seqModel-paddedSequences-masking.py)ã€‚å°†ä»£ç ä¸­ `is_train = False` æ”¹æˆ `is_train = True` å°±å¯ä»¥è®­ç»ƒäº†ï¼Œæµ‹è¯•çš„æ—¶å€™å†æ”¹å›æ¥å³å¯ã€‚


æ›´å¤šå‚è€ƒèµ„æ–™æ¥è‡ªäº
- [Towards Data Science - Attention â€” Seq2Seq Models](https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263)
- [Sequence to Sequence Learning with Neural Networks](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)
-[Jay Alammar Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)



