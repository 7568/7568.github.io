---
layout: blog
text-process: true
background-image: http://7568.github.io/images/2021-11-15-cnn-seq2seqModel/img.png
category: æ–‡æœ¬å¤„ç†
title: æœºå™¨ç¿»è¯‘ - Seq2Seq with Attention
mathjax: true
tags:
- Seq2Seq
- Attention
- æ–‡æœ¬å¤„ç†
---

[convseq2seq0]:http://7568.github.io/images/2021-11-15-cnn-seq2seqModel/convseq2seq0.png
[convseq2seq1]:http://7568.github.io/images/2021-11-15-cnn-seq2seqModel/convseq2seq1.png
[gated-linear-unit]:http://7568.github.io/images/2021-11-15-cnn-seq2seqModel/gated-linear-unit.png
[train-decoder]:http://7568.github.io/images/2021-11-15-cnn-seq2seqModel/train-decoder.png
[train-decoder-cnn-struct]:http://7568.github.io/images/2021-11-15-cnn-seq2seqModel/train-decoder-cnn-struct.png

# ç®€ä»‹

åœ¨[ğŸ’ ä¸Šä¸€ç¯‡blog ğŸ’ ](https://7568.github.io/2021/11/03/rnn-seq2seqModel.html) ä¸­æˆ‘ä»¬è®²è¿°äº†ä½¿ç”¨rnnæ¥è¿›è¡Œè‡ªç„¶è¯­è¨€çš„ç¿»è¯‘å·¥ä½œï¼Œé™äºç¯‡å¹…çš„åŸå› ï¼Œæˆ‘ä»¬å°†ä¼šåœ¨æœ¬blogæ¥è®²è¿°ä½¿ç”¨ cnn è¿›è¡Œè‡ªç„¶è¯­è¨€çš„ç¿»è¯‘å·¥ä½œã€‚æˆ‘ä»¬å°†ä¼šåœ¨[ğŸ’ ä¸‹ä¸€ç¯‡ ğŸ’ ]() è¿›è¡Œ Transformer çš„è®²è§£ã€‚

æœ¬æ–‡æˆ‘ä»¬å°†ä¼šåœ¨æœ¬blogä¸­å®ç° [Convolutional Sequence to Sequence Learning ]() è®ºæ–‡ä¸­çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸æˆ‘ä»¬çš„ä¹‹å‰çš„æ–¹æ³•å®Œå…¨ä¸ä¸€æ ·ï¼Œåœ¨ä¹‹å‰çš„æ–¹æ³•ä¸­æˆ‘ä»¬
ä½¿ç”¨çš„éƒ½æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å¸¸ç”¨çš„å¾ªç¯ç¥ç»ç½‘ç»œrnnï¼Œè€Œæœ¬æ–‡ä½¿ç”¨çš„æ˜¯é€šå¸¸ä½¿ç”¨åœ¨å›¾åƒå¤„ç†ä¸­ä»»åŠ¡ä¸­çš„å·ç§¯ç¥ç»ç½‘ç»œcnnã€‚ä¸è¿‡ä¸é€šå¸¸åœ¨å›¾åƒä¸­ä½¿ç”¨çš„cnnä¸åŒçš„æ˜¯ï¼Œåœ¨å›¾åƒä¸­cnnçš„å·ç§¯æ ¸é€šå¸¸
æ˜¯å¸¦æœ‰å®½åº¦å’Œé«˜åº¦çš„ï¼Œä½†æ˜¯åœ¨æ–‡æœ¬å¤„ç†ä»»åŠ¡ä¸­çš„cnnå·ç§¯æ ¸åªæœ‰é•¿åº¦ï¼Œæ²¡æœ‰é«˜åº¦ã€‚åœ¨[ğŸ’ æ­¤å¤„ ğŸ’ ](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb) æœ‰å…³äºcnnçš„ä»‹ç»ã€‚

# å‡†å¤‡æ•°æ®

é¦–å…ˆæˆ‘ä»¬è¿˜æ˜¯å‡†å¤‡æ•°æ®ï¼Œè¯¥éƒ¨åˆ†ä¸[ ğŸ’ ä¹‹å‰ ğŸ’ ](https://7568.github.io/2021/11/03/rnn-seq2seqModel.html) çš„å†…å®¹ä¸€è‡´ï¼Œå°±ä¸åšè¿‡å¤šè®²è§£ã€‚

# æ¨¡å‹ä»‹ç»

ä½¿ç”¨cnnè¿›è¡Œæ–‡æœ¬ç¿»è¯‘å·¥ä½œï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜æ˜¯åˆ†æˆ encoder å’Œ decoder ä¸¤éƒ¨åˆ†ï¼Œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![convseq2seq0]

## encoder

æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹ encoder çš„ç»“æ„

![convseq2seq1]

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨ encoder ä¸­æœ‰ä¸€ä¸ªå¾ˆå¤§çš„ç‰¹ç‚¹å°±æ˜¯ä½ç½®çš„æ“ä½œï¼Œä¹‹å‰æˆ‘ä»¬çš„rnnä¸­éƒ½æ²¡æœ‰ä½ç½®ç¼–ç ï¼Œæ˜¯å› ä¸ºrnnå¤©ç„¶å°±æœ‰å…ˆåé¡ºåºï¼Œè€Œcnnæ²¡æœ‰ï¼Œ
è€Œæˆ‘ä»¬è‡ªç„¶è¯­è¨€æ˜¯æœ‰é¡ºåºçš„ï¼Œç›¸åŒçš„å•è¯å¯èƒ½ä¼šå› ä¸ºé¡ºåºçš„ä¸ä¸€æ ·ï¼Œç»„æˆçš„å¥å­çš„æ„æ€å¯èƒ½ä¼šå®Œå…¨ä¸ä¸€æ ·ã€‚æ‰€ä»¥åœ¨cnnä¸­éœ€è¦å¯¹è¾“å…¥è¿›è¡Œä½ç½®ç¼–ç ã€‚

åœ¨ encoder ä¸­æˆ‘ä»¬çš„è¾“å…¥åˆ†ä¸º6ä¸ªéƒ¨åˆ†:
1. å°†è¾“å…¥è¿›è¡ŒtokenåŒ–ï¼Œå°±æ˜¯å°†å­—ç¬¦è½¬æ¢æˆæ•°å­—ã€‚å†æ‹¼æ¥ä¸Šä½ç½®ç¼–ç 
- å°†ä½ç½®ç¼–ç ä¸tokenè¿›è¡Œé€ç‚¹ç›¸åŠ ï¼Œå¾—åˆ°å¸¦ä½ç½®å±æ€§çš„token
- å°†ä¸Šä¸€æ­¥çš„ç»“æœè¿›è¡Œå…¨è¿æ¥æ“ä½œ
- å°†ä¸Šä¸€æ­¥çš„ç»“æœè¿›è¡Œå·ç§¯æ“ä½œï¼Œå¾—åˆ°ç¬¬ä¸€ä¸ªç»“æœï¼Œä¸ºå·ç§¯å±‚çš„è¾“å‡º "conved output"
- å°†ä¸Šä¸€æ­¥çš„ç»“æœä¸ç¬¬äºŒæ­¥çš„ç»“æœè¿›è¡Œé€ç‚¹ç›¸åŠ ï¼Œå¾—åˆ°ç¬¬äºŒä¸ªç»“æœï¼Œç§°ä¸º "combined output"

åœ¨ rnn ä¸­æˆ‘ä»¬çš„ encoder åªä¼šæœ‰ä¸€ä¸ªç»“æœä¼ åˆ° decoder ä¸­ï¼Œè€Œåœ¨ cnn ä¸­æˆ‘ä»¬æœ‰ä¸¤ä¸ªç»“æœï¼Œåˆ†åˆ«æ˜¯"conved output"å’Œ"combined output"ï¼Œéƒ½ä¼šä½œä¸ºå‚æ•°ä¼ åˆ° decoder ä¸­å»ã€‚

åœ¨ä¸Šé¢æˆ‘ä»¬æè¿°çš„æ˜¯åªæœ‰ä¸€å±‚ cnn çš„ç½‘ç»œï¼Œå¦‚æœæƒ³æœ‰å¤šå±‚ï¼Œå…¶ä¸­ä¸€ä¸ªç®€å•çš„æ–¹æ³•æ˜¯ç›´æ¥åœ¨ç¬¬4æ­¥åŠ ä¸Šå¤šå±‚ç½‘ç»œï¼Œæœ¬æ–‡å°†ä»‹ç»ä¸€ä¸ªå¸¦æ®‹å·®å—çš„ cnn ç½‘ç»œæ¨¡å‹ï¼Œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![convseq2seq1]

åœ¨ä¸Šå›¾ä¸­ç»¿è‰²çš„æ–¹å—è¡¨ç¤º gated linear units (GLU) å¤–åŠ æ¿€æ´»å‡½æ•°ï¼Œè¯¥æ“ä½œä¹Ÿæ˜¯è·Ÿ GRU å’Œ LSTM ä¸€æ ·ï¼Œå¸¦æœ‰é—¨æ§å•å…ƒï¼Œæ˜¯ä¸€ç§å¸¦é—¨æ§çš„ç½‘ç»œç»“æ„ã€‚GLU å¯ç”¨æ•°å­¦è¡¨è¾¾å¼ä¸º

$$GLU(a,b) = a \otimes \sigma(b) $$

åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œçœ‹ä¸Šå»ä¸æ®‹å·®ç½‘ç»œç±»ä¼¼

![gated-linear-unit]

ä¸‹é¢æ˜¯ encoder çš„ä»£ç 
```python
class Encoder(nn.Module):
    def __init__(self, 
                 input_dim, 
                 emb_dim, 
                 hid_dim, 
                 n_layers, 
                 kernel_size, 
                 dropout, 
                 device,
                 max_length = 100):
        super().__init__()
        
        assert kernel_size % 2 == 1, "Kernel size must be odd!"
        
        self.device = device
        
        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)
        
        self.tok_embedding = nn.Embedding(input_dim, emb_dim)
        self.pos_embedding = nn.Embedding(max_length, emb_dim)
        
        self.emb2hid = nn.Linear(emb_dim, hid_dim)
        self.hid2emb = nn.Linear(hid_dim, emb_dim)
        
        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, 
                                              out_channels = 2 * hid_dim, 
                                              kernel_size = kernel_size, 
                                              padding = (kernel_size - 1) // 2)
                                    for _ in range(n_layers)])
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src):
        
        #src = [batch size, src len]
        
        batch_size = src.shape[0]
        src_len = src.shape[1]
        
        #create position tensor
        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)
        
        #pos = [0, 1, 2, 3, ..., src_len - 1]
        
        #pos = [batch_size, src_len]
        
        #embed tokens and positions
        tok_embedded = self.tok_embedding(src)
        pos_embedded = self.pos_embedding(pos)
        
        #tok_embedded = pos_embedded = [batch size, src len, emb dim]
        
        #combine embeddings by elementwise summing
        embedded = self.dropout(tok_embedded + pos_embedded)
        
        #embedded = [batch size, src len, emb dim]
        
        #pass embedded through linear layer to convert from emb dim to hid dim
        conv_input = self.emb2hid(embedded)
        
        #conv_input = [batch size, src len, hid dim]
        
        #permute for convolutional layer
        # å°†1ï¼Œ2ç»´åº¦å¯¹è°ƒï¼Œç›¸å½“äºå°†çŸ©é˜µè½¬ç½®
        # torch.equal(conv_input[0].permute(1,0), conv_input[0].t()) ä¸º Trueï¼Œæ•…ç”±æ­¤åˆ¤æ–­
        conv_input = conv_input.permute(0, 2, 1) 
        
        #conv_input = [batch size, hid dim, src len]
        
        #begin convolutional blocks...
        
        for i, conv in enumerate(self.convs):
        
            #pass through convolutional layer
            conved = conv(self.dropout(conv_input))

            #conved = [batch size, 2 * hid dim, src len]

            #pass through GLU activation function
            conved = F.glu(conved, dim = 1)

            #conved = [batch size, hid dim, src len]
            
            #apply residual connection
            conved = (conved + conv_input) * self.scale

            #conved = [batch size, hid dim, src len]
            
            #set conv_input to conved for next loop iteration
            conv_input = conved
        
        #...end convolutional blocks
        
        #permute and convert back to emb dim
        conved = self.hid2emb(conved.permute(0, 2, 1))
        
        #conved = [batch size, src len, emb dim]
        
        #elementwise sum output (conved) and input (embedded) to be used for attention
        combined = (conved + embedded) * self.scale
        
        #combined = [batch size, src len, emb dim]
        
        return conved, combined
```
æ•´ä¸ªè¿‡ç¨‹å¤§æ¦‚å°±æ˜¯embeddingåŠ æ®‹å·®ï¼Œè‡³äºä¸ºä»€ä¹ˆè¦æœ‰ self.scale æš‚æ—¶è¿˜ä¸æ¸…æ¥šï¼Œä¹Ÿä¸æ¸…æ¥šé€‰æ‹©ä¸åŒçš„ self.scale çš„å€¼å¯¹æ¨¡å‹æ€§èƒ½æ˜¯å¦æœ‰å½±å“ï¼Œç›®å‰é€‰æ‹©çš„æ˜¯æ ¹å·5ã€‚

## decoder

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ decoder ä¸ encoder å¾ˆç›¸ä¼¼ï¼Œä½†æ˜¯åœ¨ decoder ä¸­ï¼Œå·ç§¯çš„åœ°æ–¹ä¸ä»…è¦æ¥æ”¶ä¸Šä¸€å±‚ç½‘ç»œä¼ è¿‡æ¥çš„æ•°æ®ï¼Œè¿˜è¦æ¥æ”¶ä»ä¸Šä¸Šä¸€å±‚è·³è·ƒè¿æ¥è¿‡æ¥çš„æ•°æ®å’Œ encoder ä¸­è¾“å‡ºçš„ä¸¤ä¸ªå‚æ•°ã€‚

è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![train-decoder]

ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œcnnå±‚å…±æ¥æ”¶äº†4ä¸ªå‚æ•°ã€‚ä¸‹å›¾æ˜¯å·ç§¯å±‚æ‹†å¼€ä¹‹åçš„æ ·å­ã€‚

![train-decoder-cnn-struct]

ä¸‹é¢æ˜¯ä»£ç 
```python
class Decoder(nn.Module):
    def __init__(self, 
                 output_dim, 
                 emb_dim, 
                 hid_dim, 
                 n_layers, 
                 kernel_size, 
                 dropout, 
                 trg_pad_idx, 
                 device,
                 max_length = 100):
        super().__init__()
        
        self.kernel_size = kernel_size
        self.trg_pad_idx = trg_pad_idx
        self.device = device
        
        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)
        
        self.tok_embedding = nn.Embedding(output_dim, emb_dim)
        self.pos_embedding = nn.Embedding(max_length, emb_dim)
        
        self.emb2hid = nn.Linear(emb_dim, hid_dim)
        self.hid2emb = nn.Linear(hid_dim, emb_dim)
        
        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)
        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)
        
        self.fc_out = nn.Linear(emb_dim, output_dim)
        
        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, 
                                              out_channels = 2 * hid_dim, 
                                              kernel_size = kernel_size)
                                    for _ in range(n_layers)])
        
        self.dropout = nn.Dropout(dropout)
      
    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):
        
        #embedded = [batch size, trg len, emb dim]
        #conved = [batch size, hid dim, trg len]
        #encoder_conved = encoder_combined = [batch size, src len, emb dim]
        
        #permute and convert back to emb dim
        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))
        
        #conved_emb = [batch size, trg len, emb dim]
        
        combined = (conved_emb + embedded) * self.scale
        
        #combined = [batch size, trg len, emb dim]
                
        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))
        
        #energy = [batch size, trg len, src len]
        
        attention = F.softmax(energy, dim=2)
        
        #attention = [batch size, trg len, src len]
            
        attended_encoding = torch.matmul(attention, encoder_combined)
        
        #attended_encoding = [batch size, trg len, emd dim]
        
        #convert from emb dim -> hid dim
        attended_encoding = self.attn_emb2hid(attended_encoding)
        
        #attended_encoding = [batch size, trg len, hid dim]
        
        #apply residual connection
        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale
        
        #attended_combined = [batch size, hid dim, trg len]
        
        return attention, attended_combined
        
    def forward(self, trg, encoder_conved, encoder_combined):
        
        #trg = [batch size, trg len]
        #encoder_conved = encoder_combined = [batch size, src len, emb dim]
                
        batch_size = trg.shape[0]
        trg_len = trg.shape[1]
            
        #create position tensor
        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)
        
        #pos = [batch size, trg len]
        
        #embed tokens and positions
        tok_embedded = self.tok_embedding(trg)
        pos_embedded = self.pos_embedding(pos)
        
        #tok_embedded = [batch size, trg len, emb dim]
        #pos_embedded = [batch size, trg len, emb dim]
        
        #combine embeddings by elementwise summing
        embedded = self.dropout(tok_embedded + pos_embedded)
        
        #embedded = [batch size, trg len, emb dim]
        
        #pass embedded through linear layer to go through emb dim -> hid dim
        conv_input = self.emb2hid(embedded)
        
        #conv_input = [batch size, trg len, hid dim]
        
        #permute for convolutional layer
        conv_input = conv_input.permute(0, 2, 1) 
        
        #conv_input = [batch size, hid dim, trg len]
        
        batch_size = conv_input.shape[0]
        hid_dim = conv_input.shape[1]
        
        for i, conv in enumerate(self.convs):
        
            #apply dropout
            conv_input = self.dropout(conv_input)
        
            #need to pad so decoder can't "cheat"
            padding = torch.zeros(batch_size, 
                                  hid_dim, 
                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)
                
            padded_conv_input = torch.cat((padding, conv_input), dim = 2)
        
            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]
        
            #pass through convolutional layer
            conved = conv(padded_conv_input)

            #conved = [batch size, 2 * hid dim, trg len]
            
            #pass through GLU activation function
            conved = F.glu(conved, dim = 1)

            #conved = [batch size, hid dim, trg len]
            
            #calculate attention
            attention, conved = self.calculate_attention(embedded, 
                                                         conved, 
                                                         encoder_conved, 
                                                         encoder_combined)
            
            #attention = [batch size, trg len, src len]
            
            #apply residual connection
            conved = (conved + conv_input) * self.scale
            
            #conved = [batch size, hid dim, trg len]
            
            #set conv_input to conved for next loop iteration
            conv_input = conved
            
        conved = self.hid2emb(conved.permute(0, 2, 1))
         
        #conved = [batch size, trg len, emb dim]
            
        output = self.fc_out(self.dropout(conved))
        
        #output = [batch size, trg len, output dim]
            
        return output, attention
```
åœ¨ decoder ä¸­æ¯” encoder ä¸­å¤šäº†ä¸€ä¸ª calculate_attention æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¥æ”¶4ä¸ªå‚æ•°ï¼Œè¾“å‡ºä¸¤ç»“æœï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¯attentionã€‚

æ¥ä¸‹æ¥å°±æ˜¯è®­ç»ƒä»£ç ï¼Œä¸ä¹‹å‰çš„å¹¶æ— å·®åˆ«ã€‚

åœ¨æµ‹è¯•çš„æ—¶å€™ï¼Œæˆ‘ä»¬ decoder çš„è¾“å…¥éœ€è¦åšä¸€äº›ä¿®æ”¹ã€‚åœ¨ rnn ç»“æ„çš„ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬çš„ decoder çš„è¾“å…¥ï¼ˆå³è®­ç»ƒæ—¶å€™çš„ç›®æ ‡ç¿»è¯‘ç»“æœï¼‰å¯ä»¥æ ¹æ®èµ·å§‹æ ‡å¿—æ¥ä¾æ¬¡å¾€åï¼Œ
å¾—åˆ°ç¿»è¯‘çš„é¢„æµ‹ï¼Œä½†æ˜¯åœ¨ cnn ä¸­å´ä¸èƒ½è¿™æ ·åšï¼Œéœ€è¦ä¸€æ¬¡æ€§è¾“å…¥æ•´ä¸ªç›®æ ‡ç»“æœï¼Œä½†æ˜¯ç›®æ ‡ç»“æœæ˜¯ä¸çŸ¥é“çš„ã€‚

é€šè¿‡åˆ†æä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼Œcnn ç½‘ç»œçš„æ–‡æœ¬ç¿»è¯‘ä¸ rnn ç½‘ç»œçš„æ–‡æœ¬ç¿»è¯‘ï¼Œä»–ä»¬åœ¨ decoder çš„æ—¶å€™ï¼Œè¾“å…¥éƒ½æ˜¯ç¬¬ä¸€ä¸ªå¼€å§‹å­—ç¬¦ï¼Œé‚£ cnn æ˜¯å¦‚ä½•è¿›è¡Œå·ç§¯æ“ä½œçš„å‘¢ï¼Ÿ
ç­”æ¡ˆå°±æ˜¯é€šè¿‡ embedding æ“ä½œå®ç°çš„ã€‚è¿™ä¹Ÿè§£é‡Šäº†ä¸ºä»€ä¹ˆåœ¨ cnn ç½‘ç»œä¸­ä¸ºä»€ä¹ˆæœ‰é‚£ä¹ˆå¤šçš„ embedding æ“ä½œï¼Œembedding æ“ä½œèƒ½å°†ä»»æ„é•¿åº¦çš„è¾“å…¥å˜æˆç›¸åŒç»´åº¦çš„è¾“å‡ºï¼Œ
ä»è€Œä¿è¯äº†åé¢çš„è®¡ç®—ç»´åº¦åˆé€‚ã€‚å¦™å•Šï¼

# ä»£ç ä¸‹è½½

ä»[bentrevett / pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq) ä¸­æå–å‡ºçš„ä»£ç å¦‚ä¸‹ï¼š

ğŸ‘‰ï¸ ğŸ‘‰ï¸ ğŸ‘‰ï¸ ç‚¹å‡»[ ğŸ’ ğŸ’ ğŸ’ å¯ä»¥ç›´æ¥ä¸‹è½½ä½¿ç”¨ rnn ç»“æ„çš„ seq2seq æ¨¡å‹çš„ä»£ç ](https://7568.github.io/codes/text-process/2021-11-16-seq2seqModel-cnn.py) ã€‚å°†ä»£ç ä¸­ `is_train = False` æ”¹æˆ `is_train = True` å°±å¯ä»¥è®­ç»ƒäº†ï¼Œæµ‹è¯•çš„æ—¶å€™å†æ”¹å›æ¥å³å¯ã€‚



æ›´å¤šå‚è€ƒèµ„æ–™æ¥è‡ªäº
- [Towards Data Science - Attention â€” Seq2Seq Models](https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263)
- [Sequence to Sequence Learning with Neural Networks](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)
-[Jay Alammar Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)



