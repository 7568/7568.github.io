---
layout: blog
time-series-process: true
mathjax: true
date:   2021-11-26
background-image: https://7568.github.io/images/2021-11-26-tabnet/img.png
title:  TabNet - Attentive Interpretable Tabular Learning
category: time series 处理
tags:
- tabular data
- time series
---

[tabnet-architecture]:https://7568.github.io/images/2021-11-26-tabnet/tabnet-architecture.png
[sparsemax-compare-softmax]:https://7568.github.io/images/2021-11-26-tabnet/sparsemax-compare-softmax.png

# 简介

TabNet 是2020年 Google Cloud AI 团队发表的一篇用来处理表格数据的深度神经网络模型，它可解释性强而且使用到了自监督技术，本文将
通过[💝 💝 💝 论文 💝 💝 💝](https://arxiv.org/pdf/1908.07442.pdf) 和[💝 💝 💝 代码 💝 💝 💝(非官方)](https://github.com/dreamquark-ai/tabnet) 来对 TabNet 进行介绍。

# 论文介绍

对于处理 tabular 类型的数据，目前使用基于树结构的集成学习框架会有很好的效果。例如 [💝 LightGBM 💝](#LightGBM) 和 [💝 XDBoost 💝](#XDBoost) 在众多 tabular 类型的数据处理任务中都有精彩的表现。
但是 TabNet 有自己的优点，而且在很多任务重并不比它们差。

TabNet 优点如下：
1. 方便使用，对原始数据不需要做任何其他的操作，就能直接使用，而且 TabNet 是端对端的，训练起来非常方便。
2. TabNet 使用 attention 机制，使得模型的解释性强。
3. TabNet 效果好，并且有两种不同的可解释性，一个是局部可解释性，一个是全局可解释性。
4. 对于第一次遇见的 tabular 类型的数据，我们使用非监督填词游戏的方式对模型进行预训练，使得模型在该数据上有很好的表现。

所谓非监督填词游戏就是把样本数据随机的选择一些属性置为空，然后让网络来进行预测学习。当网络对该数据的填词游已经处理得
很好之后，然后再来进行对该数据的实际任务的微调训练，从而提高网络在实际任务上的性能。通常我们可以把这种预训练好的模型
当作整个网络的 embedding 层。

下图展示了 TabNet 的整体结构

![tabnet-architecture]

整体分为 encoder 和 decoder 两部分，在 encoder 中有两个特殊的结构，分别为 feature transformer，attentive transformer和 feature masking，其中 feature transformer 是
用来进行特征提取，attentive transformer 是用来进行特征选择，和提供对模型的可解释性，而 feature masking 是用来获取全局特征重要性的分布。
图中的（a）是 encoder 结构，（b）是 decoder 结构，（c）是 feature transformer，里面可以分成4层，每一层都是由 FC，BN，[💝 GLU 💝](https://7568.github.io/2021/11/15/cnn-seq2seqModel.html#glu) 构成。其中前2层为 Shared across decision steps，另外两层为 Decision step dependent，
（d）是 attentive transformer 结构，里面的 [💝 sparsemax 💝](https://arxiv.org/pdf/1602.02068) 用来做归一化处理，并且结果中只包含突出的特征信息。

## sparsemax

sparsemax 常用于多标签分类任务中，通常当作神经网络的激活函数放在最后一层取代 softmax。sparsemax 和 softmax 的区别如下：

![sparsemax-compare-softmax]

个人理解 sparsemax 的优势是在多分类任务中，使用 sparsemax 能够平衡一个样本属于多个分类的权重，而在 softmax 中
一个样本虽然可能属于多个类别，但是他们的权重却是不一样的。例如某一个样本属于1，2两个类别，如果我们的网络算得该样本在类别1，2上的
softmax 值分别为0.7和0.9，而且这两个值也是最大的两个值，虽然最终结果能得出该样本属于1，2两个类别，但是其实
网络在反向求导的过程中还是会惩罚这两个输出，因为他们不是1，而如果使用 sparsemax 就不会这样，结果也是能够得出该样本
属于1，2两个类别，而且反向的时候不会对它们进行惩罚。所以 sparsemax 在某些应用中似乎会更加合理一些。

## LightGBM

## XDBoost

## gradient-boosted DT

# 代码

在 [💝 dreamquark-ai 💝](https://github.com/dreamquark-ai/tabnet) 的 tabnet 实现代码中有7个 example ，他们分别是：
- tabnet 对美国人口数据分类预测的基本使用介绍 [💝 census_example.ipynb 💝](https://github.com/dreamquark-ai/tabnet/blob/develop/census_example.ipynb)
- 自定义规则的 tabnet 分类预测使用介绍，[💝 customizing_example.py 💝](https://github.com/dreamquark-ai/tabnet/blob/develop/customizing_example.py)
- tabnet 对森林覆盖率数据分类预测的基本使用介绍 [💝 forest_example.py 💝](https://github.com/dreamquark-ai/tabnet/blob/develop/forest_example.py)
- tabnet 对回归数据的基本使用介绍 [💝 regression_example.py 💝](https://github.com/dreamquark-ai/tabnet/blob/develop/regression_example.py)
- tabnet 对多分类任务的使用介绍 [💝 multi_task_example.py 💝](https://github.com/dreamquark-ai/tabnet/blob/develop/multi_task_example.py)
- tabnet 对多回归任务的使用介绍 [💝 multi_regression_example.py 💝](https://github.com/dreamquark-ai/tabnet/blob/develop/multi_regression_example.py)
- 带自监督的预训练模型的tabnet对分类任务的使用介绍 [💝 pretraining_example.py 💝](https://github.com/dreamquark-ai/tabnet/blob/develop/pretraining_example.py)



[💝 INVASE 💝](https://openreview.net/pdf?id=BJg_roAcK7) 论文中用实验说明了使用带选择性的feature比使用全部的feature效果要好。

[💝 强化学习RL 💝](https://openreview.net/pdf?id=B1gJOoRcYQ)


