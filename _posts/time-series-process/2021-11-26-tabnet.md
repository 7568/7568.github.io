---
layout: blog
time-series-process: true
mathjax: true
date:   2021-11-26
background-image: https://7568.github.io/images/2021-11-26-tabnet/img.png
title:  TabNet - Attentive Interpretable Tabular Learning
category: time series 处理
tags:
- tabular data
- time series
---

[tabnet-architecture]:https://7568.github.io/images/2021-11-26-tabnet/tabnet-architecture.png
[sparsemax-compare-softmax]:https://7568.github.io/images/2021-11-26-tabnet/sparsemax-compare-softmax.png

# 简介

TabNet 是2020年 Google Cloud AI 团队发表的一篇用来处理表格数据的深度神经网络模型，它可解释性强而且使用到了自监督技术，本文将
通过[💝 💝 💝 论文 💝 💝 💝](https://arxiv.org/pdf/1908.07442.pdf) 和[💝 💝 💝 代码 💝 💝 💝](https://github.com/dreamquark-ai/tabnet) 来对 TabNet 进行介绍。

# 论文介绍

TabNet 优点如下：
1. 方便使用，对原始数据不需要做任何其他的操作，就能直接使用，而且 TabNet 是端对端的，训练起来非常方便。
2. TabNet 使用 attention 机制，使得模型的解释性强。
3. TabNet 效果好，并且有两种不同的可解释性，一个是局部可解释性，一个是全局可解释性。
4. 我们的非监督预训练模型对第一次见到的表格数据进行填词游戏有相当好的效果。

下图展示了 TabNet 的整体结构

![tabnet-architecture]

整体分为 encoder 和 decoder 两部分，在 encoder 中有两个特殊的结构，分别为 feature transformer，attentive transformer和 feature masking，其中 feature transformer 是
用来进行特征提取， attentive transformer 是用来进行特征选择，和提供对模型的可解释性，而 feature masking 是用来获取全局特征重要性的分布。
图中的（a）是encoder结构，（b）是 decoder 结构，（c）是 feature transformer，里面可以分成4层，每一层都是由 FC，BN，[GLU](https://7568.github.io/2021/11/15/cnn-seq2seqModel.html#glu) 构成。其中前2层为 Shared across decision steps，另外两层为 Decision step dependent，
（d）是 attentive transformer 结构，里面的 [sparsemax](https://arxiv.org/pdf/1602.02068) 用来做归一化处理，并且结果中只包含突出的特征信息。

## sparsemax

sparsemax 常用于多标签分类任务中，通常当作神经网络的激活函数放在最后一层取代softmax。sparsemax和softmax的区别如下：

![sparsemax-compare-softmax]

个人理解 sparsemax 的优势是在多分类任务中，使用 sparsemax 能够平衡一个样本属于多个分类的权重，而在 softmax中
一个样本虽然可能属于多个类别，但是他们的权重却是不一样的。例如某一个样本属于1，2两个类别，如果我们的网络算得该样本在类别1，2上的
softmax值分别为0.7和0.9，而且这两个值也是最大的两个值，虽然最终结果能得出该样本属于1，2两个类别，但是其实
网络在反向求导的过程中还是会惩罚这两个输出，因为他们不是1，而如果使用 sparsemax 就不会这样，结果也是能够得出该样本
属于1，2两个类别，而且反向的时候不会对它们进行惩罚。所以 sparsemax 在某些应用中似乎会更加合理一些。


[INVASE](https://openreview.net/pdf?id=BJg_roAcK7) 论文中用实验说明了使用带选择性的feature比使用全部的feature效果要好。

[强化学习](https://openreview.net/pdf?id=B1gJOoRcYQ)


